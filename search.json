[{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to mvgam","title":"Contributing to mvgam","text":"document outlines propose change mvgam. detailed discussion contributing open source R packages, please see development contributing guide code review principles.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to mvgam","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to mvgam","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). See tidyverse guide create great issue advice.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to mvgam","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"nicholasjclark/mvgam\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to mvgam","text":"New code follow tidyverse style guide possible. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to mvgam","text":"Please note mvgam project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/CONTRIBUTING.html","id":"roadmap","dir":"","previous_headings":"","what":"Roadmap","title":"Contributing to mvgam","text":"mvgam package stable state development, degree active subsequent development envisioned primary authors.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 Nicholas Clark Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"required-tidy-data-format","dir":"Articles","previous_headings":"","what":"Required tidy data format","title":"Formatting data for use in mvgam","text":"Manipulating data ‘long’ format (.e. tidy format) necessary modelling mvgam. ‘long’ format, mean series x time observation needs entry dataframe list object wish pass data two primary modelling functions, mvgam() jsdgam(). simple example can viewed simulating data using sim_mvgam() function. See ?sim_mvgam details","code":"simdat <- sim_mvgam(   n_series = 4,    T = 24,    prop_missing = 0.2 ) head(simdat$data_train, 16) #>    y season year   series time #> 1  0      1    1 series_1    1 #> 2  3      1    1 series_2    1 #> 3  3      1    1 series_3    1 #> 4  1      1    1 series_4    1 #> 5  3      2    1 series_1    2 #> 6  1      2    1 series_2    2 #> 7  4      2    1 series_3    2 #> 8  0      2    1 series_4    2 #> 9  1      3    1 series_1    3 #> 10 0      3    1 series_2    3 #> 11 1      3    1 series_3    3 #> 12 1      3    1 series_4    3 #> 13 0      4    1 series_1    4 #> 14 0      4    1 series_2    4 #> 15 0      4    1 series_3    4 #> 16 2      4    1 series_4    4"},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"series-as-a-factor-variable","dir":"Articles","previous_headings":"Required tidy data format","what":"series as a factor variable","title":"Formatting data for use in mvgam","text":"Notice four different time series simulated data, identified series-level indicator factor variable. important number levels matches number unique series data ensure indexing across series works properly underlying modelling functions. Several main workhorse functions package (including mvgam() get_mvgam_priors()) give error case, may worth checking anyway: Note can technically supply data series indicator, package generally assume using single time series. exceptions , example grouped data like estimate hierarchical dependencies (see example hierarchical process error correlations ?AR documentation) like set Joint Species Distribution Model (JSDM) using Zero-Mean Multivariate Gaussian distribution latent residuals (see examples ?ZMVN documentation).","code":"class(simdat$data_train$series) #> [1] \"factor\" levels(simdat$data_train$series) #> [1] \"series_1\" \"series_2\" \"series_3\" \"series_4\" all(levels(simdat$data_train$series) %in%        unique(simdat$data_train$series)) #> [1] TRUE"},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"a-single-outcome-variable","dir":"Articles","previous_headings":"Required tidy data format","what":"A single outcome variable","title":"Formatting data for use in mvgam","text":"may also notices spread numeric / integer-classed outcome variable different columns. Rather, single column outcome variable, labelled y simulated data (though outcome labelled y). another important requirement mvgam, shouldn’t unfamiliar R users frequently use modelling packages lme4, mgcv, brms many regression modelling packages . advantage format now easy specify effects vary among time series: Depending observation families plan use building models, may restrictions need satisfied within outcome variable. example, Beta regression can handle proportional data, values >= 1 <= 0 allowed. Likewise, Poisson regression can handle non-negative integers. regression functions R assume user knows issue warnings errors choose wrong distribution, often ends leading unhelpful error optimizer difficult interpret diagnose. mvgam attempt provide errors something simply allowed. example, can simulate data zero-centred Gaussian distribution (ensuring values < 1) attempt Beta regression mvgam using betar family: call gam() using mgcv package leads model actually fits (though give unhelpful warning message): call mvgam() gives us something useful: Please see ?mvgam_families information types responses package can handle restrictions","code":"summary(glm(   y ~ series + time,   data = simdat$data_train,   family = poisson() )) #>  #> Call: #> glm(formula = y ~ series + time, family = poisson(), data = simdat$data_train) #>  #> Coefficients: #>                Estimate Std. Error z value Pr(>|z|)     #> (Intercept)    -0.63755    0.40343  -1.580 0.114037     #> seriesseries_2  0.52233    0.42213   1.237 0.215949     #> seriesseries_3  1.23795    0.37608   3.292 0.000996 *** #> seriesseries_4  0.45028    0.43429   1.037 0.299822     #> time            0.01284    0.02257   0.569 0.569442     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for poisson family taken to be 1) #>  #>     Null deviance: 101.652  on 59  degrees of freedom #> Residual deviance:  86.252  on 55  degrees of freedom #>   (12 observations deleted due to missingness) #> AIC: 182.44 #>  #> Number of Fisher Scoring iterations: 5 summary(mgcv::gam(   y ~ series + s(time, by = series),   data = simdat$data_train,   family = poisson() )) #>  #> Family: poisson  #> Link function: log  #>  #> Formula: #> y ~ series + s(time, by = series) #>  #> Parametric coefficients: #>                Estimate Std. Error z value Pr(>|z|)    #> (Intercept)     -0.5484     0.3456  -1.587  0.11258    #> seriesseries_2   0.5130     0.4357   1.177  0.23903    #> seriesseries_3   1.1818     0.3934   3.004  0.00266 ** #> seriesseries_4 -21.5347   110.5452  -0.195  0.84555    #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Approximate significance of smooth terms: #>                          edf Ref.df Chi.sq p-value   #> s(time):seriesseries_1 1.135  1.258  0.948  0.3480   #> s(time):seriesseries_2 2.186  2.715  2.786  0.4972   #> s(time):seriesseries_3 2.464  3.048  7.190  0.0676 . #> s(time):seriesseries_4 7.834  7.986  5.992  0.6466   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> R-sq.(adj) =  0.387   Deviance explained = 51.4% #> UBRE = 0.41091  Scale est. = 1         n = 60 gauss_dat <- data.frame(   outcome = rnorm(10),   series = factor(\"series1\",     levels = \"series1\"   ),   time = 1:10 ) gauss_dat #>         outcome  series time #> 1  -0.314772157 series1    1 #> 2  -0.005381841 series1    2 #> 3  -1.358302802 series1    3 #> 4   1.572927578 series1    4 #> 5  -1.199384161 series1    5 #> 6  -0.072655731 series1    6 #> 7  -0.024083976 series1    7 #> 8   1.584497498 series1    8 #> 9   0.005675627 series1    9 #> 10 -0.648204484 series1   10 mgcv::gam(outcome ~ time,   family = betar(),   data = gauss_dat ) #>  #> Family: Beta regression(0.084)  #> Link function: logit  #>  #> Formula: #> outcome ~ time #> Total model degrees of freedom 2  #>  #> REML score: -237.6676 mvgam(outcome ~ time,   family = betar(),   data = gauss_dat ) #> Error: Values <= 0 not allowed for beta responses"},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"a-time-variable","dir":"Articles","previous_headings":"Required tidy data format","what":"A time variable","title":"Formatting data for use in mvgam","text":"requirement models can fit mvgam numeric / integer-classed variable labelled time. ensures modelling software knows arrange time series building models. setup still allows us formulate multivariate time series models. plan use autoregressive dynamic trend functions available mvgam (see ?mvgam_trends details available dynamic processes), need ensure time series entered fixed sampling interval (.e. time timesteps 1 2 time timesteps 2 3, etc…). note can missing observations () series. mvgam() check , useful ensure missing timepoint x series combinations data. can generally simple dplyr call: Note models use dynamic components assume smaller values time older (.e. time = 1 came time = 2, etc…)","code":"# A function to ensure all timepoints within a sequence are identical all_times_avail <- function(time, min_time, max_time) {   identical(     as.numeric(sort(time)),     as.numeric(seq.int(from = min_time, to = max_time))   ) }  # Get min and max times from the data min_time <- min(simdat$data_train$time) max_time <- max(simdat$data_train$time)  # Check that all times are recorded for each series data.frame(   series = simdat$data_train$series,   time = simdat$data_train$time ) %>%   dplyr::group_by(series) %>%   dplyr::summarise(all_there = all_times_avail(     time,     min_time,     max_time   )) -> checked_times if (any(checked_times$all_there == FALSE)) {   warning(\"One or more series in is missing observations for one or more timepoints\") } else {   cat(\"All series have observations at all timepoints :)\") } #> All series have observations at all timepoints :)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"irregular-sampling-intervals","dir":"Articles","previous_headings":"Required tidy data format","what":"Irregular sampling intervals?","title":"Formatting data for use in mvgam","text":"mvgam dynamic trend models expect time measured discrete, evenly-spaced intervals (.e. one measurement per week, one per year, example; though missing values allowed). please note irregularly sampled time intervals allowed, case CAR() trend model (continuous time autoregressive) appropriate. can see example kind model Examples section ?CAR. can also use trend_model = 'None' (default mvgam()) instead use Gaussian Process model temporal variation irregularly-sampled time series. See ?brms::gp details. reiterate point , time series data (don’t want estimate latent temporal dynamics) like estimate correlated latent residuals among multivariate outcomes, can set models use trend_model = ZMVN(...) without need time variable (see ?ZMVN details).","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"checking-data-with-get_mvgam_priors","dir":"Articles","previous_headings":"","what":"Checking data with get_mvgam_priors()","title":"Formatting data for use in mvgam","text":"get_mvgam_priors() function designed return information parameters model whose prior distributions can modified user. , perform series checks ensure data formatted properly. can therefore useful new users ensuring isn’t anything strange going data setup. example, can replicate steps taken (check factor levels timepoint x series combinations) single call get_mvgam_priors(). first simulate data timepoints time variable included data: Next call get_mvgam_priors() simply specifying intercept-model, enough trigger checks: error useful tells us problem . many ways fill missing timepoints, correct way left user. don’t covariates, pretty easy using expand.grid(): Now call get_mvgam_priors(), using filled data, work: function also pick misaligned factor levels series variable. can check simulating, time adding additional factor level included data: Another call get_mvgam_priors() brings useful error: Following message’s advice tells us level series_2 series variable, observations series data: Re-assigning levels fixes issue:","code":"bad_times <- data.frame(   time = seq(1, 16, by = 2),   series = factor(\"series_1\"),   outcome = rnorm(8) ) bad_times #>   time   series     outcome #> 1    1 series_1  1.85866321 #> 2    3 series_1  0.82830318 #> 3    5 series_1  1.19903527 #> 4    7 series_1  0.60005124 #> 5    9 series_1  0.05876012 #> 6   11 series_1 -1.28976353 #> 7   13 series_1 -0.12633386 #> 8   15 series_1  0.38398762 get_mvgam_priors(outcome ~ 1,   data = bad_times,   family = gaussian() ) #> Error: One or more series in data is missing observations for one or more timepoints bad_times %>%   dplyr::right_join(expand.grid(     time = seq(       min(bad_times$time),       max(bad_times$time)     ),     series = factor(unique(bad_times$series),       levels = levels(bad_times$series)     )   )) %>%   dplyr::arrange(time) -> good_times good_times #>    time   series     outcome #> 1     1 series_1  1.85866321 #> 2     2 series_1          NA #> 3     3 series_1  0.82830318 #> 4     4 series_1          NA #> 5     5 series_1  1.19903527 #> 6     6 series_1          NA #> 7     7 series_1  0.60005124 #> 8     8 series_1          NA #> 9     9 series_1  0.05876012 #> 10   10 series_1          NA #> 11   11 series_1 -1.28976353 #> 12   12 series_1          NA #> 13   13 series_1 -0.12633386 #> 14   14 series_1          NA #> 15   15 series_1  0.38398762 get_mvgam_priors(outcome ~ 1,   data = good_times,   family = gaussian() ) #>                             param_name param_length           param_info #> 1                          (Intercept)            1          (Intercept) #> 2 vector<lower=0>[n_series] sigma_obs;            1 observation error sd #>                                   prior                  example_change #> 1 (Intercept) ~ student_t(3, 0.5, 2.5);     (Intercept) ~ normal(0, 1); #> 2  sigma_obs ~ inv_gamma(1.418, 0.452); sigma_obs ~ normal(0.77, 0.45); #>   new_lowerbound new_upperbound #> 1             NA             NA #> 2             NA             NA bad_levels <- data.frame(   time = 1:8,   series = factor(\"series_1\",     levels = c(       \"series_1\",       \"series_2\"     )   ),   outcome = rnorm(8) )  levels(bad_levels$series) #> [1] \"series_1\" \"series_2\" get_mvgam_priors(outcome ~ 1,   data = bad_levels,   family = gaussian() ) #> Error: Mismatch between factor levels of \"series\" and unique values of \"series\" #> Use #>   `setdiff(levels(data$series), unique(data$series))`  #> and #>   `intersect(levels(data$series), unique(data$series))` #> for guidance setdiff(levels(bad_levels$series),          unique(bad_levels$series)) #> [1] \"series_2\" bad_levels %>%   dplyr::mutate(series = droplevels(series)) -> good_levels levels(good_levels$series) #> [1] \"series_1\" get_mvgam_priors(   outcome ~ 1,   data = good_levels,   family = gaussian() ) #>                             param_name param_length           param_info #> 1                          (Intercept)            1          (Intercept) #> 2 vector<lower=0>[n_series] sigma_obs;            1 observation error sd #>                                  prior                  example_change #> 1  (Intercept) ~ student_t(3, 0, 2.5);     (Intercept) ~ normal(0, 1); #> 2 sigma_obs ~ inv_gamma(1.418, 0.452); sigma_obs ~ normal(0.12, 0.87); #>   new_lowerbound new_upperbound #> 1             NA             NA #> 2             NA             NA"},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"covariates-with-no-nas","dir":"Articles","previous_headings":"Checking data with get_mvgam_priors()","what":"Covariates with no NAs","title":"Formatting data for use in mvgam","text":"Covariates can used models just using mgcv (see ?formula.gam details formula syntax). although outcome variable can NAs, covariates . regression software silently drop raws model matrix NAs, helpful debugging. mvgam() get_mvgam_priors() functions run simple checks , hopefully return useful errors finds missing values: Just like mgcv package, mvgam can also accept data list object. useful want set linear functional predictors even distributed lag predictors. checks run mvgam still work data. change cov predictor matrix: call get_mvgam_priors() returns error:","code":"miss_dat <- data.frame(   outcome = rnorm(10),   cov = c(NA, rnorm(9)),   series = factor(\"series1\",     levels = \"series1\"   ),   time = 1:10 ) miss_dat #>        outcome          cov  series time #> 1  -1.43332110           NA series1    1 #> 2  -0.01030332 -2.177576028 series1    2 #> 3  -0.21223603 -0.117860143 series1    3 #> 4  -0.90634018  0.112294787 series1    4 #> 5  -2.10215248  0.007886198 series1    5 #> 6   1.89336046  1.877743872 series1    6 #> 7  -0.96812584  2.158756554 series1    7 #> 8  -0.10260304  0.709714522 series1    8 #> 9   0.23995957  0.766983379 series1    9 #> 10  0.06089889 -0.308211421 series1   10 get_mvgam_priors(   outcome ~ cov,   data = miss_dat,   family = gaussian() ) #>                             param_name param_length           param_info #> 1                          (Intercept)            1          (Intercept) #> 2                                  cov            1     cov fixed effect #> 3 vector<lower=0>[n_series] sigma_obs;            1 observation error sd #>                                    prior                  example_change #> 1 (Intercept) ~ student_t(3, -0.2, 2.5);     (Intercept) ~ normal(0, 1); #> 2              cov ~ student_t(3, 0, 2);             cov ~ normal(0, 1); #> 3   sigma_obs ~ inv_gamma(1.418, 0.452); sigma_obs ~ normal(-0.3, 0.11); #>   new_lowerbound new_upperbound #> 1             NA             NA #> 2             NA             NA #> 3             NA             NA miss_dat <- list(   outcome = rnorm(10),   series = factor(\"series1\",     levels = \"series1\"   ),   time = 1:10 ) miss_dat$cov <- matrix(rnorm(50), ncol = 5, nrow = 10) miss_dat$cov[2, 3] <- NA get_mvgam_priors(   outcome ~ cov,   data = miss_dat,   family = gaussian() ) #>                             param_name param_length           param_info #> 1                          (Intercept)            1          (Intercept) #> 2                                 cov1            1    cov1 fixed effect #> 3                                 cov2            1    cov2 fixed effect #> 4                                 cov3            1    cov3 fixed effect #> 5                                 cov4            1    cov4 fixed effect #> 6                                 cov5            1    cov5 fixed effect #> 7 vector<lower=0>[n_series] sigma_obs;            1 observation error sd #>                                   prior                   example_change #> 1 (Intercept) ~ student_t(3, 0.2, 2.5);      (Intercept) ~ normal(0, 1); #> 2            cov1 ~ student_t(3, 0, 2);             cov1 ~ normal(0, 1); #> 3            cov2 ~ student_t(3, 0, 2);             cov2 ~ normal(0, 1); #> 4            cov3 ~ student_t(3, 0, 2);             cov3 ~ normal(0, 1); #> 5            cov4 ~ student_t(3, 0, 2);             cov4 ~ normal(0, 1); #> 6            cov5 ~ student_t(3, 0, 2);             cov5 ~ normal(0, 1); #> 7  sigma_obs ~ inv_gamma(1.418, 0.452); sigma_obs ~ normal(-0.61, 0.83); #>   new_lowerbound new_upperbound #> 1             NA             NA #> 2             NA             NA #> 3             NA             NA #> 4             NA             NA #> 5             NA             NA #> 6             NA             NA #> 7             NA             NA"},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"plotting-with-plot_mvgam_series","dir":"Articles","previous_headings":"","what":"Plotting with plot_mvgam_series()","title":"Formatting data for use in mvgam","text":"Plotting data useful way ensure everything looks ok, ’ve gone throug checks factor levels timepoint x series combinations. plot_mvgam_series() function take supplied data plot either series line plots (choose series = '') set plots describe distribution single time series. example, plot time series data, highlight single series plot, can use:  can look closely distribution first time series:  split data training testing folds (.e. forecast evaluation), can include test data plots:","code":"plot_mvgam_series(   data = simdat$data_train,   y = \"y\",   series = \"all\" ) plot_mvgam_series(   data = simdat$data_train,   y = \"y\",   series = 1 ) plot_mvgam_series(   data = simdat$data_train,   newdata = simdat$data_test,   y = \"y\",   series = 1 )"},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"example-with-neon-tick-data","dir":"Articles","previous_headings":"","what":"Example with NEON tick data","title":"Formatting data for use in mvgam","text":"give one example data can reformatted mvgam modelling, use observations National Ecological Observatory Network (NEON) tick drag cloth samples. Ixodes scapularis widespread tick species capable transmitting diversity parasites animals humans, many zoonotic. Due medical ecological importance tick species, common goal understand factors influence abundances. NEON field team carries standardised long-term monitoring tick abundances well important indicators ecological change. Nymphal abundance . scapularis routinely recorded across NEON plots using field sampling method called drag cloth sampling, common method sampling ticks landscape. Field researchers sample ticks dragging large cloth behind terrain suspected harboring ticks, usually working grid-like pattern. sites sampled since 2014, resulting rich dataset nymph abundance time series. tick time series show strong seasonality incorporate many challenging features associated ecological data including overdispersion, high proportions missingness irregular sampling time, making useful exploring utility dynamic GAMs. begin loading NEON tick data years 2014 - 2021, downloaded NEON prepared described Clark & Wells 2022. can read bit data using call ?all_neon_tick_data exercise, use epiWeek variable index seasonality, work observations sampling plots (labelled plotID column): Now can select target species want (. scapularis), filter correct plot IDs convert epiWeek variable character numeric: Now tricky part: need fill missing observations NAs. tick data sparse field observers go sample possible epiWeek. many particular weeks observations included data. can use expand.grid() take care : Create series variable needed mvgam modelling: Now create time variable, needs track Year epiWeek unique series. n function dplyr often useful generating time index grouped dataframes: Check factor levels series: looks good, rigorous check using get_mvgam_priors(): can also set model mvgam() use run_model = FALSE ensure necessary steps creating modelling code objects run. recommended use cmdstanr backend possible, auto-formatting options available package useful checking package-generated Stan code inefficiencies can fixed lead sampling performance improvements: call runs without issue, resulting object now contains model code data objects needed initiate sampling:","code":"data(\"all_neon_tick_data\") str(dplyr::ungroup(all_neon_tick_data)) #> tibble [3,505 × 24] (S3: tbl_df/tbl/data.frame) #>  $ Year                : num [1:3505] 2015 2015 2015 2015 2015 ... #>  $ epiWeek             : chr [1:3505] \"37\" \"38\" \"39\" \"40\" ... #>  $ yearWeek            : chr [1:3505] \"201537\" \"201538\" \"201539\" \"201540\" ... #>  $ plotID              : chr [1:3505] \"BLAN_005\" \"BLAN_005\" \"BLAN_005\" \"BLAN_005\" ... #>  $ siteID              : chr [1:3505] \"BLAN\" \"BLAN\" \"BLAN\" \"BLAN\" ... #>  $ nlcdClass           : chr [1:3505] \"deciduousForest\" \"deciduousForest\" \"deciduousForest\" \"deciduousForest\" ... #>  $ decimalLatitude     : num [1:3505] 39.1 39.1 39.1 39.1 39.1 ... #>  $ decimalLongitude    : num [1:3505] -78 -78 -78 -78 -78 ... #>  $ elevation           : num [1:3505] 168 168 168 168 168 ... #>  $ totalSampledArea    : num [1:3505] 162 NA NA NA 162 NA NA NA NA 164 ... #>  $ amblyomma_americanum: num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ ixodes_scapularis   : num [1:3505] 2 NA NA NA 0 NA NA NA NA 0 ... #>  $ time                : Date[1:3505], format: \"2015-09-13\" \"2015-09-20\" ... #>  $ RHMin_precent       : num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ RHMin_variance      : num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ RHMax_precent       : num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ RHMax_variance      : num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ airTempMin_degC     : num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ airTempMin_variance : num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ airTempMax_degC     : num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ airTempMax_variance : num [1:3505] NA NA NA NA NA NA NA NA NA NA ... #>  $ soi                 : num [1:3505] -18.4 -17.9 -23.5 -28.4 -25.9 ... #>  $ cum_sdd             : num [1:3505] 173 173 173 173 173 ... #>  $ cum_gdd             : num [1:3505] 1129 1129 1129 1129 1129 ... plotIDs <- c(   \"SCBI_013\", \"SCBI_002\",   \"SERC_001\", \"SERC_005\",   \"SERC_006\", \"SERC_012\",   \"BLAN_012\", \"BLAN_005\" ) model_dat <- all_neon_tick_data %>%   dplyr::ungroup() %>%   dplyr::mutate(target = ixodes_scapularis) %>%   dplyr::filter(plotID %in% plotIDs) %>%   dplyr::select(Year, epiWeek, plotID, target) %>%   dplyr::mutate(epiWeek = as.numeric(epiWeek)) model_dat %>%   # Create all possible combos of plotID, Year and epiWeek;   # missing outcomes will be filled in as NA   dplyr::full_join(expand.grid(     plotID = unique(model_dat$plotID),     Year = unique(model_dat$Year),     epiWeek = seq(1, 52)   )) %>%   # left_join back to original data so plotID and siteID will   # match up, in case you need the siteID for anything else later on   dplyr::left_join(all_neon_tick_data %>%     dplyr::select(siteID, plotID) %>%     dplyr::distinct()) -> model_dat model_dat %>%   dplyr::mutate(     series = plotID,     y = target   ) %>%   dplyr::mutate(     siteID = factor(siteID),     series = factor(series)   ) %>%   dplyr::select(-target, -plotID) %>%   dplyr::arrange(Year, epiWeek, series) -> model_dat model_dat %>%   dplyr::ungroup() %>%   dplyr::group_by(series) %>%   dplyr::arrange(Year, epiWeek) %>%   dplyr::mutate(time = seq(1, dplyr::n())) %>%   dplyr::ungroup() -> model_dat levels(model_dat$series) #> [1] \"BLAN_005\" \"BLAN_012\" \"SCBI_002\" \"SCBI_013\" \"SERC_001\" \"SERC_005\" \"SERC_006\" #> [8] \"SERC_012\" get_mvgam_priors(   y ~ 1,   data = model_dat,   family = poisson() ) #>    param_name param_length  param_info                                  prior #> 1 (Intercept)            1 (Intercept) (Intercept) ~ student_t(3, -2.3, 2.5); #>                example_change new_lowerbound new_upperbound #> 1 (Intercept) ~ normal(0, 1);             NA             NA testmod <- mvgam(   y ~ s(epiWeek, by = series, bs = \"cc\") +     s(series, bs = \"re\"),   trend_model = AR(),   data = model_dat,   backend = \"cmdstanr\",   run_model = FALSE ) str(testmod$model_data) #> List of 25 #>  $ y           : num [1:416, 1:8] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ... #>  $ n           : int 416 #>  $ X           : num [1:3328, 1:73] 1 1 1 1 1 1 1 1 1 1 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:3328] \"1\" \"2\" \"3\" \"4\" ... #>   .. ..$ : chr [1:73] \"X.Intercept.\" \"V2\" \"V3\" \"V4\" ... #>  $ S1          : num [1:8, 1:8] 1.037 -0.416 0.419 0.117 0.188 ... #>  $ zero        : num [1:73] 0 0 0 0 0 0 0 0 0 0 ... #>  $ S2          : num [1:8, 1:8] 1.037 -0.416 0.419 0.117 0.188 ... #>  $ S3          : num [1:8, 1:8] 1.037 -0.416 0.419 0.117 0.188 ... #>  $ S4          : num [1:8, 1:8] 1.037 -0.416 0.419 0.117 0.188 ... #>  $ S5          : num [1:8, 1:8] 1.037 -0.416 0.419 0.117 0.188 ... #>  $ S6          : num [1:8, 1:8] 1.037 -0.416 0.419 0.117 0.188 ... #>  $ S7          : num [1:8, 1:8] 1.037 -0.416 0.419 0.117 0.188 ... #>  $ S8          : num [1:8, 1:8] 1.037 -0.416 0.419 0.117 0.188 ... #>  $ p_coefs     : Named num 0 #>   ..- attr(*, \"names\")= chr \"(Intercept)\" #>  $ p_taus      : num 0.933 #>  $ ytimes      : int [1:416, 1:8] 1 9 17 25 33 41 49 57 65 73 ... #>  $ n_series    : int 8 #>  $ sp          : Named num [1:9] 0.368 0.368 0.368 0.368 0.368 ... #>   ..- attr(*, \"names\")= chr [1:9] \"s(epiWeek):seriesBLAN_005\" \"s(epiWeek):seriesBLAN_012\" \"s(epiWeek):seriesSCBI_002\" \"s(epiWeek):seriesSCBI_013\" ... #>  $ y_observed  : num [1:416, 1:8] 0 0 0 0 0 0 0 0 0 0 ... #>  $ total_obs   : int 3328 #>  $ num_basis   : int 73 #>  $ n_sp        : num 9 #>  $ n_nonmissing: int 400 #>  $ obs_ind     : int [1:400] 89 93 98 101 115 118 121 124 127 130 ... #>  $ flat_ys     : num [1:400] 2 0 0 0 0 0 0 25 36 14 ... #>  $ flat_xs     : num [1:400, 1:73] 1 1 1 1 1 1 1 1 1 1 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:400] \"705\" \"737\" \"777\" \"801\" ... #>   .. ..$ : chr [1:73] \"X.Intercept.\" \"V2\" \"V3\" \"V4\" ... #>  - attr(*, \"trend_model\")= chr \"AR1\" stancode(testmod) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[8, 8] S1; // mgcv smooth penalty matrix S1 #>   matrix[8, 8] S2; // mgcv smooth penalty matrix S2 #>   matrix[8, 8] S3; // mgcv smooth penalty matrix S3 #>   matrix[8, 8] S4; // mgcv smooth penalty matrix S4 #>   matrix[8, 8] S5; // mgcv smooth penalty matrix S5 #>   matrix[8, 8] S6; // mgcv smooth penalty matrix S6 #>   matrix[8, 8] S7; // mgcv smooth penalty matrix S7 #>   matrix[8, 8] S8; // mgcv smooth penalty matrix S8 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // random effect variances #>   vector<lower=0>[1] sigma_raw; #>    #>   // random effect means #>   vector[1] mu_raw; #>    #>   // latent trend AR1 terms #>   vector<lower=-1, upper=1>[n_series] ar1; #>    #>   // latent trend variance parameters #>   vector<lower=0>[n_series] sigma; #>    #>   // latent trends #>   matrix[n, n_series] trend; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : 65] = b_raw[1 : 65]; #>   b[66 : 73] = mu_raw[1] + b_raw[66 : 73] * sigma_raw[1]; #> } #> model { #>   // prior for random effect population variances #>   sigma_raw ~ inv_gamma(1.418, 0.452); #>    #>   // prior for random effect population means #>   mu_raw ~ std_normal(); #>    #>   // prior for (Intercept)... #>   b_raw[1] ~ student_t(3, -2.3, 2.5); #>    #>   // prior for s(epiWeek):seriesBLAN_005... #>   b_raw[2 : 9] ~ multi_normal_prec(zero[2 : 9], S1[1 : 8, 1 : 8] * lambda[1]); #>    #>   // prior for s(epiWeek):seriesBLAN_012... #>   b_raw[10 : 17] ~ multi_normal_prec(zero[10 : 17], #>                                      S2[1 : 8, 1 : 8] * lambda[2]); #>    #>   // prior for s(epiWeek):seriesSCBI_002... #>   b_raw[18 : 25] ~ multi_normal_prec(zero[18 : 25], #>                                      S3[1 : 8, 1 : 8] * lambda[3]); #>    #>   // prior for s(epiWeek):seriesSCBI_013... #>   b_raw[26 : 33] ~ multi_normal_prec(zero[26 : 33], #>                                      S4[1 : 8, 1 : 8] * lambda[4]); #>    #>   // prior for s(epiWeek):seriesSERC_001... #>   b_raw[34 : 41] ~ multi_normal_prec(zero[34 : 41], #>                                      S5[1 : 8, 1 : 8] * lambda[5]); #>    #>   // prior for s(epiWeek):seriesSERC_005... #>   b_raw[42 : 49] ~ multi_normal_prec(zero[42 : 49], #>                                      S6[1 : 8, 1 : 8] * lambda[6]); #>    #>   // prior for s(epiWeek):seriesSERC_006... #>   b_raw[50 : 57] ~ multi_normal_prec(zero[50 : 57], #>                                      S7[1 : 8, 1 : 8] * lambda[7]); #>    #>   // prior for s(epiWeek):seriesSERC_012... #>   b_raw[58 : 65] ~ multi_normal_prec(zero[58 : 65], #>                                      S8[1 : 8, 1 : 8] * lambda[8]); #>    #>   // prior (non-centred) for s(series)... #>   b_raw[66 : 73] ~ std_normal(); #>    #>   // priors for AR parameters #>   ar1 ~ std_normal(); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for latent trend variance parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>    #>   // trend estimates #>   trend[1, 1 : n_series] ~ normal(0, sigma); #>   for (s in 1 : n_series) { #>     trend[2 : n, s] ~ normal(ar1[s] * trend[1 : (n - 1), s], sigma[s]); #>   } #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_ys ~ poisson_log_glm(append_col(flat_xs, flat_trends), 0.0, #>                               append_row(b, 1.0)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   vector[n_series] tau; #>   array[n, n_series] int ypred; #>   rho = log(lambda); #>   for (s in 1 : n_series) { #>     tau[s] = pow(sigma[s], -2.0); #>   } #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> }"},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"Formatting data for use in mvgam","text":"following papers resources offer useful material Dynamic GAMs can applied practice: Clark, Nicholas J. Wells, K. Dynamic Generalized Additive Models (DGAMs) forecasting discrete ecological time series. Methods Ecology Evolution. (2023): 14, 771-784. Clark, Nicholas J., et al. Beyond single-species models: leveraging multispecies forecasts navigate dynamics ecological predictability. PeerJ. (2025): 13:e18929 de Sousa, Heitor C., et al. Severe fire regimes decrease resilience ectothermic populations. Journal Animal Ecology (2024): 93(11), 1656-1669. Hannaford, Naomi E., et al. sparse Bayesian hierarchical vector autoregressive model microbial dynamics wastewater treatment plant. Computational Statistics & Data Analysis (2023): 179, 107659. Karunarathna, K..N.K., et al. Modelling nonlinear responses desert rodent species environmental change hierarchical dynamic generalized additive models. Ecological Modelling (2024): 490, 110648. Zhu, L., et al. Responses widespread pest insect extreme high temperatures stage-dependent divergent among seasonal cohorts. Functional Ecology (2025): 39, 165–180. https://doi.org/10.1111/1365-2435.14711","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/data_in_mvgam.html","id":"interested-in-contributing","dir":"Articles","previous_headings":"","what":"Interested in contributing?","title":"Formatting data for use in mvgam","text":"’m actively seeking PhD students researchers work areas ecological forecasting, multivariate model evaluation development mvgam. Please see small list opportunities website reach interested (n.clark’’uq.edu.au)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/forecast_evaluation.html","id":"simulating-discrete-time-series","dir":"Articles","previous_headings":"","what":"Simulating discrete time series","title":"Forecasting and forecast evaluation in mvgam","text":"begin simulating data show forecasts computed evaluated mvgam. sim_mvgam() function can used simulate series come variety response distributions well seasonal patterns /dynamic temporal patterns. simulate collection three time count-valued series. series share seasonal pattern different temporal dynamics. setting trend_model = GP() prop_trend = 0.75, generating time series smooth underlying temporal trends (evolving Gaussian Processes squared exponential kernel) moderate seasonal patterns. observations Poisson-distributed allow 10% observations missing. returned object list containing training testing data (sim_mvgam() automatically splits data folds us) together information data generating process used simulate data series case shared seasonal pattern. resulting time series similar might encounter dealing count-valued data can take small counts:  individual series, can plot training testing data, well specific features observed data:","code":"set.seed(1) simdat <- sim_mvgam(   T = 100,   n_series = 3,   mu = 2,   trend_model = GP(),   prop_trend = 0.75,   family = poisson(),   prop_missing = 0.10 ) str(simdat) #> List of 6 #>  $ data_train        :'data.frame':  225 obs. of  5 variables: #>   ..$ y     : int [1:225] 6 NA 11 2 5 20 7 8 NA 11 ... #>   ..$ season: int [1:225] 1 1 1 2 2 2 3 3 3 4 ... #>   ..$ year  : int [1:225] 1 1 1 1 1 1 1 1 1 1 ... #>   ..$ series: Factor w/ 3 levels \"series_1\",\"series_2\",..: 1 2 3 1 2 3 1 2 3 1 ... #>   ..$ time  : int [1:225] 1 1 1 2 2 2 3 3 3 4 ... #>  $ data_test         :'data.frame':  75 obs. of  5 variables: #>   ..$ y     : int [1:75] 4 23 8 3 NA 3 1 20 8 3 ... #>   ..$ season: int [1:75] 4 4 4 5 5 5 6 6 6 7 ... #>   ..$ year  : int [1:75] 7 7 7 7 7 7 7 7 7 7 ... #>   ..$ series: Factor w/ 3 levels \"series_1\",\"series_2\",..: 1 2 3 1 2 3 1 2 3 1 ... #>   ..$ time  : int [1:75] 76 76 76 77 77 77 78 78 78 79 ... #>  $ true_corrs        : num [1:3, 1:3] 1 0.0861 0.1161 0.0861 1 ... #>  $ true_trends       : num [1:100, 1:3] -0.851 -0.758 -0.664 -0.571 -0.48 ... #>  $ global_seasonality: num [1:100] -0.966 -0.197 0.771 1.083 0.37 ... #>  $ trend_params      :List of 2 #>   ..$ alpha: num [1:3] 0.883 0.936 1.036 #>   ..$ rho  : num [1:3] 7.54 4.01 7.49 plot_mvgam_series(   data = simdat$data_train,   series = \"all\" ) plot_mvgam_series(   data = simdat$data_train,   newdata = simdat$data_test,   series = 1 )"},{"path":"https://nicholasjclark.github.io/mvgam/articles/forecast_evaluation.html","id":"modelling-dynamics-with-splines","dir":"Articles","previous_headings":"Simulating discrete time series","what":"Modelling dynamics with splines","title":"Forecasting and forecast evaluation in mvgam","text":"first model fit uses shared cyclic spline capture repeated seasonality, well series-specific splines time capture long-term dynamics. allow temporal splines fairly complex can capture much temporal variation possible: model fits without issue: can plot conditional effects splines (link scale) see estimated highly nonlinear","code":"mod1 <- mvgam(   y ~ s(season, bs = \"cc\", k = 8) +     s(time, by = series, bs = \"cr\", k = 20),   knots = list(season = c(0.5, 12.5)),   trend_model = \"None\",   data = simdat$data_train,   silent = 2 ) summary(mod1, include_betas = FALSE) #> GAM formula: #> y ~ s(season, bs = \"cc\", k = 8) + s(time, by = series, k = 20) #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> None #>  #> N series: #> 3  #>  #> N timepoints: #> 100  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> GAM coefficient (beta) estimates: #>             2.5% 50% 97.5% Rhat n_eff #> (Intercept)  1.9 1.9     2    1  1675 #>  #> Approximate significance of GAM smooths: #>                         edf Ref.df Chi.sq p-value     #> s(season)              3.80      6   19.9 < 2e-16 *** #> s(time):seriesseries_1 8.22     19   39.1 0.47727     #> s(time):seriesseries_2 8.74     19   45.6 0.00092 *** #> s(time):seriesseries_3 6.49     19   55.6 < 2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✖ Rhats above 1.05 found for some parameters #>     Use pairs() and mcmc_plot() to investigate #> ✔ No issues with divergences #> ✖ 43 of 2000 iterations saturated the maximum tree depth of 10 (2.15%) #>     Try a larger max_treedepth to avoid saturation #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model conditional_effects(mod1, type = \"link\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/forecast_evaluation.html","id":"modelling-dynamics-with-a-correlated-ar1","dir":"Articles","previous_headings":"Simulating discrete time series","what":"Modelling dynamics with a correlated AR1","title":"Forecasting and forecast evaluation in mvgam","text":"showing produce evaluate forecasts, fit second model data two models can compared. model equivalent , except now use correlated AR(1) process model series-specific dynamics. See ?AR details. summary model now contains information autoregressive process error parameters time series: can plot posteriors parameters, parameter matter, using bayesplot routines. First autoregressive parameters:  now variance (\\(\\sigma\\)) parameters:  can plot conditional seasonal effect:  estimates seasonal component fairly similar two models, see produce similar forecasts","code":"mod2 <- mvgam(y ~ 1,   trend_formula = ~ s(season, bs = \"cc\", k = 8) - 1,   trend_knots = list(season = c(0.5, 12.5)),   trend_model = AR(cor = TRUE),   noncentred = TRUE,   data = simdat$data_train,   silent = 1 ) summary(mod2, include_betas = FALSE) #> GAM observation formula: #> y ~ 1 #>  #> GAM process formula: #> ~s(season, bs = \"cc\", k = 8) - 1 #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> AR(cor = TRUE) #>  #>  #> N process models: #> 3  #>  #> N series: #> 3  #>  #> N timepoints: #> 75  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> GAM observation model coefficient (beta) estimates: #>             2.5% 50% 97.5% Rhat n_eff #> (Intercept)  1.7   2   2.4    1   706 #>  #> Process model AR parameter estimates: #>        2.5%  50% 97.5% Rhat n_eff #> ar1[1] 0.74 0.89  0.99 1.00   688 #> ar1[2] 0.66 0.82  0.95 1.01   639 #> ar1[3] 0.87 0.96  1.00 1.00   625 #>  #> Process error parameter estimates: #>          2.5%  50% 97.5% Rhat n_eff #> sigma[1] 0.24 0.33  0.44 1.00   574 #> sigma[2] 0.31 0.43  0.59 1.01   495 #> sigma[3] 0.19 0.26  0.36 1.02   314 #>  #> Approximate significance of GAM process smooths: #>            edf Ref.df Chi.sq p-value     #> s(season) 2.29      6   20.6 3.3e-06 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model mcmc_plot(mod2, variable = \"ar\", regex = TRUE, type = \"areas\") mcmc_plot(mod2, variable = \"sigma\", regex = TRUE, type = \"areas\") conditional_effects(mod2, type = \"link\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/forecast_evaluation.html","id":"forecasting-with-the-forecast-function","dir":"Articles","previous_headings":"","what":"Forecasting with the forecast() function","title":"Forecasting and forecast evaluation in mvgam","text":"Probabilistic forecasts can computed two main ways mvgam. first take model fit training data (two example models) produce temporal predictions posterior predictive distribution feeding newdata forecast() function. crucial newdata fed forecast() function follows sequentially data used fit model (internally checked package might headache data supplied specific time-order). calling forecast() function, option generate different kinds predictions (.e. predicting link scale, response scale produce expectations; see ?forecast.mvgam details). use default produce forecasts response scale, common way evaluate forecast distributions objects created class mvgam_forecast, contain information hindcast distributions, forecast distributions true observations series data: can plot forecasts series model using S3 plot method objects class:     Clearly two models produce equivalent forecasts. come back scoring forecasts moment.","code":"fc_mod1 <- forecast(mod1, newdata = simdat$data_test) fc_mod2 <- forecast(mod2, newdata = simdat$data_test) str(fc_mod1) #> List of 16 #>  $ call              :Class 'formula'  language y ~ s(season, bs = \"cc\", k = 8) + s(time, by = series, k = 20) #>   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ family_pars       : NULL #>  $ trend_model       : chr \"None\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : Factor w/ 3 levels \"series_1\",\"series_2\",..: 1 2 3 #>  $ train_observations:List of 3 #>   ..$ series_1: int [1:75] 6 2 7 11 8 6 9 11 7 4 ... #>   ..$ series_2: int [1:75] NA 5 8 2 1 NA 2 4 0 2 ... #>   ..$ series_3: int [1:75] 11 20 NA 36 44 34 57 50 26 28 ... #>  $ train_times       :List of 3 #>   ..$ series_1: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_2: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_3: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations :List of 3 #>   ..$ series_1: int [1:25] 4 3 1 3 1 NA NA 7 9 8 ... #>   ..$ series_2: int [1:25] 23 NA 20 20 14 7 6 6 6 1 ... #>   ..$ series_3: int [1:25] 8 3 8 3 NA 1 1 9 8 NA ... #>  $ test_times        :List of 3 #>   ..$ series_1: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>   ..$ series_2: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>   ..$ series_3: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>  $ hindcasts         :List of 3 #>   ..$ series_1: num [1:2000, 1:75] 5 4 0 2 0 5 2 4 4 0 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>   ..$ series_2: num [1:2000, 1:75] 1 3 0 1 4 3 3 3 2 3 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,2]\" \"ypred[2,2]\" \"ypred[3,2]\" \"ypred[4,2]\" ... #>   ..$ series_3: num [1:2000, 1:75] 9 14 15 14 14 16 9 14 17 16 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,3]\" \"ypred[2,3]\" \"ypred[3,3]\" \"ypred[4,3]\" ... #>  $ forecasts         :List of 3 #>   ..$ series_1: num [1:2000, 1:25] 2 1 7 2 2 4 3 5 3 1 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:25] \"ypred[76,1]\" \"ypred[77,1]\" \"ypred[78,1]\" \"ypred[79,1]\" ... #>   ..$ series_2: num [1:2000, 1:25] 17 12 15 18 19 20 22 16 15 36 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:25] \"ypred[76,2]\" \"ypred[77,2]\" \"ypred[78,2]\" \"ypred[79,2]\" ... #>   ..$ series_3: num [1:2000, 1:25] 2 3 4 3 3 4 7 2 5 3 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:25] \"ypred[76,3]\" \"ypred[77,3]\" \"ypred[78,3]\" \"ypred[79,3]\" ... #>  - attr(*, \"class\")= chr \"mvgam_forecast\" plot(fc_mod1, series = 1) plot(fc_mod2, series = 1) plot(fc_mod1, series = 2) plot(fc_mod2, series = 2)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/forecast_evaluation.html","id":"forecasting-with-newdata-in-mvgam","dir":"Articles","previous_headings":"","what":"Forecasting with newdata in mvgam()","title":"Forecasting and forecast evaluation in mvgam","text":"second way can produce forecasts mvgam feed testing data directly mvgam() function newdata. include testing data missing observations automatically predicted posterior predictive distribution using generated quantities block Stan. example, can refit mod2 include testing data automatic forecasts: model already contains forecast distribution, need feed newdata forecast() function: forecasts nearly identical calculated previously:","code":"mod2 <- mvgam(y ~ 1,   trend_formula = ~ s(season, bs = \"cc\", k = 8) - 1,   trend_knots = list(season = c(0.5, 12.5)),   trend_model = AR(cor = TRUE),   noncentred = TRUE,   data = simdat$data_train,   newdata = simdat$data_test,   silent = 2 ) fc_mod2 <- forecast(mod2) plot(fc_mod2, series = 1)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/forecast_evaluation.html","id":"scoring-forecast-distributions","dir":"Articles","previous_headings":"","what":"Scoring forecast distributions","title":"Forecasting and forecast evaluation in mvgam","text":"primary purpose mvgam_forecast class readily allow forecast evaluations series data, using variety possible scoring functions. See ?mvgam::score.mvgam_forecast view types scores available. useful scoring metric Continuous Rank Probability Score (CRPS). CRPS value similar might get calculated weighted absolute error using full forecast distribution. returned list contains data.frame series data shows CRPS score evaluation testing data, along useful information fit forecast distribution. particular, given logical value (1s 0s) telling us whether true value within pre-specified credible interval (.e. coverage forecast distribution). default interval width 0.9, hope values in_interval column take 1 approximately 90% time. value can changed wish compute different coverages, say using 60% interval: can also compare forecasts sample observations using Expected Log Predictive Density (ELPD; also known log score). ELPD strictly proper scoring rule can applied distributional forecast, compute need predictions link scale rather outcome scale. advantageous change type prediction can get using forecast() function: Finally, multiple time series may also make sense use multivariate proper scoring rule. mvgam offers two options: Energy score Variogram score. first penalizes forecast distributions less well calibrated truth, second penalizes forecasts capture observed true correlation structure. score use depends goals, easy compute: returned object still provides information interval coverage individual series, single score per horizon now (provided all_series slot): can use score(s) choice compare different models. example, can compute plot difference CRPS scores series data. , negative value means AR(1) model (mod2) better, positive value means spline model (mod1) better.    correlated AR(1) model consistently gives better forecasts, difference scores tends grow forecast horizon increases. unexpected given way splines linearly extrapolate outside range training data","code":"crps_mod1 <- score(fc_mod1, score = \"crps\") str(crps_mod1) #> List of 4 #>  $ series_1  :'data.frame':  25 obs. of  5 variables: #>   ..$ score         : num [1:25] 1.136 0.88 0.322 1.119 0.29 ... #>   ..$ in_interval   : num [1:25] 1 1 1 1 1 NA NA 0 0 0 ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"crps\" \"crps\" \"crps\" \"crps\" ... #>  $ series_2  :'data.frame':  25 obs. of  5 variables: #>   ..$ score         : num [1:25] 2.01 NA 6.25 14.4 17.03 ... #>   ..$ in_interval   : num [1:25] 1 NA 1 1 1 1 0 0 0 0 ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"crps\" \"crps\" \"crps\" \"crps\" ... #>  $ series_3  :'data.frame':  25 obs. of  5 variables: #>   ..$ score         : num [1:25] 3.617 0.466 4.168 0.491 NA ... #>   ..$ in_interval   : num [1:25] 0 1 0 1 NA 1 1 0 0 NA ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"crps\" \"crps\" \"crps\" \"crps\" ... #>  $ all_series:'data.frame':  25 obs. of  3 variables: #>   ..$ score       : num [1:25] 6.76 NA 10.74 16.01 NA ... #>   ..$ eval_horizon: int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type  : chr [1:25] \"sum_crps\" \"sum_crps\" \"sum_crps\" \"sum_crps\" ... crps_mod1$series_1 #>         score in_interval interval_width eval_horizon score_type #> 1   1.1362237           1            0.9            1       crps #> 2   0.8796700           1            0.9            2       crps #> 3   0.3222703           1            0.9            3       crps #> 4   1.1191818           1            0.9            4       crps #> 5   0.2902030           1            0.9            5       crps #> 6          NA          NA            0.9            6       crps #> 7          NA          NA            0.9            7       crps #> 8   6.2283237           0            0.9            8       crps #> 9   8.3822845           0            0.9            9       crps #> 10  7.4587490           0            0.9           10       crps #> 11 21.4860392           0            0.9           11       crps #> 12 35.3964518           0            0.9           12       crps #> 13 37.4077225           0            0.9           13       crps #> 14 36.5122305           0            0.9           14       crps #> 15 39.5236040           0            0.9           15       crps #> 16 42.4870670           0            0.9           16       crps #> 17 42.6519410           0            0.9           17       crps #> 18 12.8012838           0            0.9           18       crps #> 19 13.8279528           0            0.9           19       crps #> 20  9.8057457           0            0.9           20       crps #> 21  4.8109472           0            0.9           21       crps #> 22  4.8450168           0            0.9           22       crps #> 23  2.8327210           0            0.9           23       crps #> 24  0.8850368           1            0.9           24       crps #> 25  3.8071090           0            0.9           25       crps crps_mod1 <- score(fc_mod1, score = \"crps\", interval_width = 0.6) crps_mod1$series_1 #>         score in_interval interval_width eval_horizon score_type #> 1   1.1362237           1            0.6            1       crps #> 2   0.8796700           1            0.6            2       crps #> 3   0.3222703           1            0.6            3       crps #> 4   1.1191818           1            0.6            4       crps #> 5   0.2902030           1            0.6            5       crps #> 6          NA          NA            0.6            6       crps #> 7          NA          NA            0.6            7       crps #> 8   6.2283237           0            0.6            8       crps #> 9   8.3822845           0            0.6            9       crps #> 10  7.4587490           0            0.6           10       crps #> 11 21.4860392           0            0.6           11       crps #> 12 35.3964518           0            0.6           12       crps #> 13 37.4077225           0            0.6           13       crps #> 14 36.5122305           0            0.6           14       crps #> 15 39.5236040           0            0.6           15       crps #> 16 42.4870670           0            0.6           16       crps #> 17 42.6519410           0            0.6           17       crps #> 18 12.8012838           0            0.6           18       crps #> 19 13.8279528           0            0.6           19       crps #> 20  9.8057457           0            0.6           20       crps #> 21  4.8109472           0            0.6           21       crps #> 22  4.8450168           0            0.6           22       crps #> 23  2.8327210           0            0.6           23       crps #> 24  0.8850368           0            0.6           24       crps #> 25  3.8071090           0            0.6           25       crps link_mod1 <- forecast(mod1, newdata = simdat$data_test, type = \"link\") score(link_mod1, score = \"elpd\")$series_1 #>         score eval_horizon score_type #> 1   -2.232486            1       elpd #> 2   -2.021430            2       elpd #> 3   -1.249183            3       elpd #> 4   -2.266812            4       elpd #> 5   -1.252127            5       elpd #> 6          NA            6       elpd #> 7          NA            7       elpd #> 8   -7.733385            8       elpd #> 9   -9.085766            9       elpd #> 10  -8.671512           10       elpd #> 11 -21.237948           11       elpd #> 12 -35.248924           12       elpd #> 13 -34.666270           13       elpd #> 14 -35.566371           14       elpd #> 15 -38.164107           15       elpd #> 16 -37.377479           16       elpd #> 17 -46.300607           17       elpd #> 18 -13.527263           18       elpd #> 19 -14.289913           19       elpd #> 20  -9.302897           20       elpd #> 21  -7.084533           21       elpd #> 22  -7.179676           22       elpd #> 23  -5.787679           23       elpd #> 24  -3.175058           24       elpd #> 25  -6.243910           25       elpd energy_mod2 <- score(fc_mod2, score = \"energy\") str(energy_mod2) #> List of 4 #>  $ series_1  :'data.frame':  25 obs. of  3 variables: #>   ..$ in_interval   : num [1:25] 1 1 1 1 1 NA NA 1 1 1 ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>  $ series_2  :'data.frame':  25 obs. of  3 variables: #>   ..$ in_interval   : num [1:25] 1 NA 1 1 1 1 1 1 1 1 ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>  $ series_3  :'data.frame':  25 obs. of  3 variables: #>   ..$ in_interval   : num [1:25] 1 1 1 1 NA 1 1 1 1 NA ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>  $ all_series:'data.frame':  25 obs. of  3 variables: #>   ..$ score       : num [1:25] 4.75 NA 4.99 5.33 NA ... #>   ..$ eval_horizon: int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type  : chr [1:25] \"energy\" \"energy\" \"energy\" \"energy\" ... energy_mod2$all_series #>        score eval_horizon score_type #> 1   4.754920            1     energy #> 2         NA            2     energy #> 3   4.989503            3     energy #> 4   5.326173            4     energy #> 5         NA            5     energy #> 6         NA            6     energy #> 7         NA            7     energy #> 8   3.900958            8     energy #> 9   4.043380            9     energy #> 10        NA           10     energy #> 11 12.842520           11     energy #> 12 22.035085           12     energy #> 13        NA           13     energy #> 14 20.567052           14     energy #> 15 23.911131           15     energy #> 16 24.922317           16     energy #> 17 28.014677           17     energy #> 18  6.071522           18     energy #> 19 10.428866           19     energy #> 20  4.043061           20     energy #> 21  3.084471           21     energy #> 22  3.457620           22     energy #> 23        NA           23     energy #> 24  9.183656           24     energy #> 25  7.921793           25     energy crps_mod1 <- score(fc_mod1, score = \"crps\") crps_mod2 <- score(fc_mod2, score = \"crps\")  diff_scores <- crps_mod2$series_1$score -   crps_mod1$series_1$score plot(diff_scores,   pch = 16, cex = 1.25, col = \"darkred\",   ylim = c(     -1 * max(abs(diff_scores), na.rm = TRUE),     max(abs(diff_scores), na.rm = TRUE)   ),   bty = \"l\",   xlab = \"Forecast horizon\",   ylab = expression(CRPS[AR1] ~ -~ CRPS[spline]) ) abline(h = 0, lty = \"dashed\", lwd = 2) ar1_better <- length(which(diff_scores < 0)) title(main = paste0(   \"AR(1) better in \",   ar1_better,   \" of \",   length(diff_scores),   \" evaluations\",   \"\\nMean difference = \",   round(mean(diff_scores, na.rm = TRUE), 2) )) diff_scores <- crps_mod2$series_2$score -   crps_mod1$series_2$score plot(diff_scores,   pch = 16, cex = 1.25, col = \"darkred\",   ylim = c(     -1 * max(abs(diff_scores), na.rm = TRUE),     max(abs(diff_scores), na.rm = TRUE)   ),   bty = \"l\",   xlab = \"Forecast horizon\",   ylab = expression(CRPS[AR1] ~ -~ CRPS[spline]) ) abline(h = 0, lty = \"dashed\", lwd = 2) ar1_better <- length(which(diff_scores < 0)) title(main = paste0(   \"AR(1) better in \",   ar1_better,   \" of \",   length(diff_scores),   \" evaluations\",   \"\\nMean difference = \",   round(mean(diff_scores, na.rm = TRUE), 2) )) diff_scores <- crps_mod2$series_3$score -   crps_mod1$series_3$score plot(diff_scores,   pch = 16, cex = 1.25, col = \"darkred\",   ylim = c(     -1 * max(abs(diff_scores), na.rm = TRUE),     max(abs(diff_scores), na.rm = TRUE)   ),   bty = \"l\",   xlab = \"Forecast horizon\",   ylab = expression(CRPS[AR1] ~ -~ CRPS[spline]) ) abline(h = 0, lty = \"dashed\", lwd = 2) ar1_better <- length(which(diff_scores < 0)) title(main = paste0(   \"AR(1) better in \",   ar1_better,   \" of \",   length(diff_scores),   \" evaluations\",   \"\\nMean difference = \",   round(mean(diff_scores, na.rm = TRUE), 2) ))"},{"path":"https://nicholasjclark.github.io/mvgam/articles/forecast_evaluation.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"Forecasting and forecast evaluation in mvgam","text":"following papers resources offer useful material Bayesian forecasting proper scoring rules: Clark N.J., et al. Beyond single-species models: leveraging multispecies forecasts navigate dynamics ecological predictability. PeerJ 13:e18929 (2025) https://doi.org/10.7717/peerj.18929 Hyndman, Rob J., George Athanasopoulos. Forecasting: principles practice. OTexts, (2018). Gneiting, Tilmann, Adrian E. Raftery. Strictly proper scoring rules, prediction, estimation Journal American statistical Association 102.477 (2007) 359-378. Simonis, Juniper L., et al. Evaluating probabilistic ecological forecasts Ecology 102.8 (2021) e03431.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/forecast_evaluation.html","id":"interested-in-contributing","dir":"Articles","previous_headings":"","what":"Interested in contributing?","title":"Forecasting and forecast evaluation in mvgam","text":"’m actively seeking PhD students researchers work areas ecological forecasting, multivariate model evaluation development mvgam. Please see small list opportunities website reach interested (n.clark’’uq.edu.au)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"dynamic-gams","dir":"Articles","previous_headings":"","what":"Dynamic GAMs","title":"Overview of the mvgam package","text":"mvgam designed propagate unobserved temporal processes capture latent dynamics observed time series. works state-space format, temporal trend evolving independently observation process. introduction package worked examples also shown seminar: Ecological Forecasting Dynamic Generalized Additive Models. Briefly, assume \\(\\tilde{\\boldsymbol{y}}_{,t}\\) conditional expectation response variable \\(\\boldsymbol{}\\) time \\(\\boldsymbol{t}\\). Assuming \\(\\boldsymbol{y_i}\\) drawn exponential distribution invertible link function, linear predictor multivariate Dynamic GAM can written : \\[~~~1:N_{series}~...\\] \\[~t~~1:N_{timepoints}~...\\] \\[g^{-1}(\\tilde{\\boldsymbol{y}}_{,t})=\\alpha_{}+\\sum\\limits_{j=1}^J\\boldsymbol{s}_{,j,t}\\boldsymbol{x}_{j,t}+\\boldsymbol{Z}\\boldsymbol{z}_{k,t}\\,,\\] \\(\\alpha\\) unknown intercepts, \\(\\boldsymbol{s}\\)’s unknown smooth functions covariates (\\(\\boldsymbol{x}\\)’s), can potentially vary among response series, \\(\\boldsymbol{z}\\) dynamic latent processes. smooth function \\(\\boldsymbol{s_j}\\) composed basis expansions whose coefficients, must estimated, control functional relationship \\(\\boldsymbol{x}_{j}\\) \\(g^{-1}(\\tilde{\\boldsymbol{y}})\\). size basis expansion limits smooth’s potential complexity. larger set basis functions allows greater flexibility. information GAMs can smooth data, see blogpost interpret nonlinear effects Generalized Additive Models. Latent processes captured \\(\\boldsymbol{Z}\\boldsymbol{z}_{,t}\\), \\(\\boldsymbol{Z}\\) \\(~~k\\) matrix loading coefficients (can fixed combination fixed freely estimated parameters) \\(\\boldsymbol{z}_{k,t}\\) set \\(K\\) latent factors can also include GAM linear predictors (see State-Space models vignette), N-mixtures vignette example jsdgam get idea flexible processes can . Several advantages GAMs can model diversity response families, including discrete distributions (.e. Poisson, Negative Binomial, Gamma) accommodate common ecological features zero-inflation overdispersion, can formulated include hierarchical smoothing multivariate responses. mvgam supports number different observation families, summarized :","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"supported-observation-families","dir":"Articles","previous_headings":"","what":"Supported observation families","title":"Overview of the mvgam package","text":"supported observation families, extra parameters need estimated (.e. \\(\\sigma\\) Gaussian model \\(\\phi\\) Negative Binomial model) default estimated independently series. However, users can opt force series share extra observation parameters using share_obs_params = TRUE mvgam(). Note default link functions currently changed.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"supported-temporal-dynamic-processes","dir":"Articles","previous_headings":"","what":"Supported temporal dynamic processes","title":"Overview of the mvgam package","text":"stated , latent processes can take wide variety forms, can multivariate allow different observational variables interact correlated. using mvgam() function, user chooses different process models trend_model argument. Available process models described detail .","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"correlated-multivariate-processes","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Correlated multivariate processes","title":"Overview of the mvgam package","text":"one observational unit (usually referred ‘series’) included data \\((N_{series} > 1)\\), use trend_model = ZMVN() set model outcomes different observational units may correlated according : \\[\\begin{align*} z_{t} & \\sim \\text{MVNormal}(0, \\Sigma) \\end{align*}\\] covariance matrix \\(\\Sigma\\) capture potentially correlated process errors. parameterised using Cholesky factorization, requires priors series-level variances \\(\\sigma\\) strength correlations using Stan’s lkj_corr_cholesky distribution. Note trend_model assume measurements occur time, users can specify variable data represents unit analysis (.e. outcomes counts different species across different sites regions, example; see `?ZMVN() guidelines).","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"independent-random-walks","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Independent Random Walks","title":"Overview of the mvgam package","text":"Use trend_model = 'RW' trend_model = RW() set model series data independent latent temporal dynamics form: \\[\\begin{align*} z_{,t} & \\sim \\text{Normal}(z_{,t-1}, \\sigma_i) \\end{align*}\\] Process error parameters \\(\\sigma\\) modeled independently series. moving average process required, use trend_model = RW(ma = TRUE) set following: \\[\\begin{align*} z_{,t} & = z_{,t-1} + \\theta_i * error_{,t-1} + error_{,t} \\\\ error_{,t} & \\sim \\text{Normal}(0, \\sigma_i) \\end{align*}\\] Moving average coefficients \\(\\theta\\) independently estimated series forced stationary default \\((abs(\\theta)<1)\\). moving averages order \\(q=1\\) currently allowed.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"multivariate-random-walks","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Multivariate Random Walks","title":"Overview of the mvgam package","text":"one series included data \\((N_{series} > 1)\\), multivariate Random Walk can set using trend_model = RW(cor = TRUE), resulting following: \\[\\begin{align*} z_{t} & \\sim \\text{MVNormal}(z_{t-1}, \\Sigma) \\end{align*}\\] latent process estimate \\(z_t\\) now takes form vector. covariance matrix \\(\\Sigma\\) capture contemporaneously correlated process errors. parameterised using Cholesky factorization, requires priors series-level variances \\(\\sigma\\) strength correlations using Stan’s lkj_corr_cholesky distribution. Moving average terms can also included multivariate random walks, case moving average coefficients \\(\\theta\\) parameterised \\(N_{series} * N_{series}\\) matrix","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"autoregressive-processes","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Autoregressive processes","title":"Overview of the mvgam package","text":"Autoregressive models \\(p=3\\), autoregressive coefficients estimated independently series, can used specifying trend_model = 'AR1', trend_model = 'AR2', trend_model = 'AR3', trend_model = AR(p = 1, 2, 3). example, univariate AR(1) model takes form: \\[\\begin{align*} z_{,t} & \\sim \\text{Normal}(ar1_i * z_{,t-1}, \\sigma_i) \\end{align*}\\] options Random Walks, additional options available placing priors autoregressive coefficients. default, coefficients forced stationarity, users can impose restriction changing upper lower bounds priors. See ?get_mvgam_priors details.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"vector-autoregressive-processes","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Vector Autoregressive processes","title":"Overview of the mvgam package","text":"Vector Autoregression order \\(p=1\\) can specified \\(N_{series} > 1\\) using trend_model = 'VAR1' trend_model = VAR(). VAR(1) model takes form: \\[\\begin{align*} z_{t} & \\sim \\text{Normal}(* z_{t-1}, \\Sigma) \\end{align*}\\] \\(\\) \\(N_{series} * N_{series}\\) matrix autoregressive coefficients diagonals capture lagged self-dependence (.e. effect process time \\(t\\) estimate time \\(t+1\\)), -diagonals capture lagged cross-dependence (.e. effect process time \\(t\\) process another series time \\(t+1\\)). default, covariance matrix \\(\\Sigma\\) assume process error covariance fixing -diagonals \\(0\\). allow correlated errors, use trend_model = 'VAR1cor' trend_model = VAR(cor = TRUE). moving average order \\(q=1\\) can also included using trend_model = VAR(ma = TRUE, cor = TRUE). Note VAR models, stationarity process enforced structured prior distribution described detail Heaps 2022 Heaps, Sarah E. “Enforcing stationarity prior vector autoregressions.” Journal Computational Graphical Statistics 32.1 (2023): 74-83.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"hierarchical-processes","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Hierarchical processes","title":"Overview of the mvgam package","text":"Several -mentioned trend_model options can modified account grouping structures data setting hierarchical latent processes. optional grouping variable (gr; must factor supplied data) exists, users can model hierarchical residual correlation structures. residual correlations specific level gr modelled hierarchically: \\[\\begin{align*} \\Omega_{group} & = \\alpha_{cor}\\Omega_{global} + (1 - \\alpha_{cor})\\Omega_{group, local} \\end{align*}\\] \\(\\Omega_{global}\\) global correlation matrix, \\(\\Omega_{group, local}\\) local deviation correlation matrix \\(\\alpha_{cor}\\) weighting parameter controlling strongly local correlation matrix \\(\\Omega_{group}\\) (.e. derived correlation matrix used level grouping factor gr) shrunk towards global correlation matrix \\(\\Omega_{global}\\) (larger values \\(\\alpha_{cor}\\) indicate greater degree shrinkage, .e. greater degree partial pooling). option valuable many types designs observational units (.e. financial assets species, example) measured different strata (.e. regions, countries experimental units, example). Currently hierarchical correlations can included AR(), VAR() ZMVN() trend_model options.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"gaussian-processes","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Gaussian Processes","title":"Overview of the mvgam package","text":"final option modelling temporal dynamics use Gaussian Process squared exponential kernel. set independently series (currently multivariate GP option), using trend_model = 'GP'. dynamics latent process modelled : \\[\\begin{align*} z & \\sim \\text{MVNormal}(0, \\Sigma_{error}) \\\\ \\Sigma_{error}[t_i, t_j] & = \\alpha^2 * exp(-0.5 * ((|t_i - t_j| / \\rho))^2) \\end{align*}\\] latent dynamic process evolves complex, high-dimensional Multivariate Normal distribution depends \\(\\rho\\) (often called length scale parameter) control quickly correlations model’s errors decay function time. models, covariance decays exponentially fast squared distance (time) observations. functions also depend parameter \\(\\alpha\\), controls marginal variability temporal function points; words controls much GP term contributes linear predictor. mvgam capitalizes advances allow GPs approximated using Hilbert space basis functions, considerably speed computation little cost accuracy prediction performance.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"piecewise-logistic-and-linear-trends","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Piecewise logistic and linear trends","title":"Overview of the mvgam package","text":"Modeling growth many types time series often similar modeling population growth natural ecosystems, series exhibits nonlinear growth saturates particular carrying capacity. logistic trend model available {mvgam} allows time-varying capacity \\(C(t)\\) well non-constant growth rate. Changes base growth rate \\(k\\) incorporated explicitly defining changepoints throughout training period growth rate allowed vary. changepoint vector \\(\\) represented vector 1s 0s, rate growth time \\(t\\) represented \\(k+(t)^T\\delta\\). Potential changepoints selected uniformly across training period, number changepoints, well flexibility potential rate changes changepoints, can controlled using trend_model = PW(). full piecewise logistic growth model : \\[\\begin{align*} z_t & = \\frac{C_t}{1 + \\exp(-(k+(t)^T\\delta)(t-(m+(t)^T\\gamma)))}  \\end{align*}\\] time series appear exhibit saturating growth, piece-wise constant rate growth can often provide useful trend model. piecewise linear trend defined : \\[\\begin{align*} z_t & = (k+(t)^T\\delta)t + (m+(t)^T\\gamma)  \\end{align*}\\] trend models, \\(m\\) offset parameter controls trend intercept. parameter, recommended include intercept observation formula identifiable. can read full description piecewise linear logistic trends paper Taylor Letham. Sean J. Taylor Benjamin Letham. “Forecasting scale.” American Statistician 72.1 (2018): 37-45.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"continuous-time-ar1-processes","dir":"Articles","previous_headings":"Supported temporal dynamic processes","what":"Continuous time AR(1) processes","title":"Overview of the mvgam package","text":"trend models mvgam() function expect time measured regularly-spaced, discrete intervals (.e. one measurement per week, one per year example). time series taken irregular intervals ’d like model autoregressive properties . trend_model = CAR() can useful set models, currently support autoregressive processes order 1. evolution latent dynamic process follows form: \\[\\begin{align*} z_{,t} & \\sim \\text{Normal}(ar1_i^{distance} * z_{,t-1}, \\sigma_i) \\end{align*}\\] \\(distance\\) vector non-negative measurements time differences successive observations. models perhaps widely known Ornstein–Uhlenbeck processes. See Examples section ?CAR illustration set models .","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"regression-formulae","dir":"Articles","previous_headings":"","what":"Regression formulae","title":"Overview of the mvgam package","text":"mvgam supports observation model regression formula, built mgcv package, well optional process model regression formula. formulae supplied exactly like supplied glm() except smooth terms, s(), te(), ti() t2(), time-varying effects using dynamic(), monotonically increasing (using s(x, bs = 'moi')) decreasing splines (using s(x, bs = 'mod'); see ?smooth.construct.moi.smooth.spec details), well Gaussian Process functions using gp(), can added right hand side (. supported mvgam formulae). See ?mvgam_formulae guidance. setting State-Space models, optional process model formula can used (see State-Space model vignette shared latent states vignette guidance using trend formulae).","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"example-time-series-data","dir":"Articles","previous_headings":"","what":"Example time series data","title":"Overview of the mvgam package","text":"‘portal_data’ object contains time series rodent captures Portal Project, long-term monitoring study based near town Portal, Arizona. Researchers operating standardized set baited traps within 24 experimental plots site since 1970’s. Sampling follows lunar monthly cycle, observations occurring average 28 days apart. However, missing observations occur due difficulties accessing site (weather events, COVID disruptions etc…). can read full sampling protocol preprint Ernest et al Biorxiv. data come pre-loaded mvgam package, can read little help page using ?portal_data. working data, important inspect data structured, first using head: glimpse function dplyr also useful understanding variables structured focus analyses time series captures one specific rodent species, Desert Pocket Mouse Chaetodipus penicillatus. species interesting goes kind “hibernation” colder months, leading low captures winter period","code":"data(\"portal_data\") head(portal_data) #>   time series captures  ndvi_ma12    mintemp #> 1    1     DM       20 -0.1721441 -0.7963381 #> 2    1     DO        2 -0.1721441 -0.7963381 #> 3    1     PB        0 -0.1721441 -0.7963381 #> 4    1     PP        0 -0.1721441 -0.7963381 #> 5    2     DM       NA -0.2373635 -1.3347160 #> 6    2     DO       NA -0.2373635 -1.3347160 dplyr::glimpse(portal_data) #> Rows: 320 #> Columns: 5 #> $ time      <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, … #> $ series    <fct> DM, DO, PB, PP, DM, DO, PB, PP, DM, DO, PB, PP, DM, DO, PB, … #> $ captures  <int> 20, 2, 0, 0, NA, NA, NA, NA, 36, 5, 0, 0, 40, 3, 0, 1, 29, 3… #> $ ndvi_ma12 <dbl> -0.172144125, -0.172144125, -0.172144125, -0.172144125, -0.2… #> $ mintemp   <dbl> -0.79633807, -0.79633807, -0.79633807, -0.79633807, -1.33471…"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"manipulating-data-for-modelling","dir":"Articles","previous_headings":"","what":"Manipulating data for modelling","title":"Overview of the mvgam package","text":"Manipulating data ‘long’ format necessary modelling mvgam. ‘long’ format, mean series x time observation needs entry dataframe list object wish use data modelling. simple example can viewed simulating data using sim_mvgam function. See ?sim_mvgam details Notice four different time series simulated data, spread outcome values different columns. Rather, single column outcome variable, labelled y simulated data. also must supply variable labelled time ensure modelling software knows arrange time series building models. setup still allows us formulate multivariate time series models, can see State-Space vignette. steps needed shape portal_data object correct form. First, create time variable, select column representing counts target species (PP), select appropriate variables can use predictors data now contain six variables:series, factor indexing time series observation belongs toyear, year samplingtime, indicator time step observation belongs tocount, response variable representing number captures species PP sampling observationmintemp, monthly average minimum temperature time stepndvi_ma12, 12-month moving average monthly Normalized Difference Vegetation Index time step Now check data structure can also summarize multiple variables, helpful search data ranges identify missing values NAs response variable count. observations generally thrown modelling packages . see work tutorials, mvgam keeps data predictions can automatically returned full dataset. time series descriptive features can plotted using plot_mvgam_series():","code":"data <- sim_mvgam(n_series = 4, T = 24) head(data$data_train, 12) #>    y season year   series time #> 1  0      1    1 series_1    1 #> 2  0      1    1 series_2    1 #> 3  3      1    1 series_3    1 #> 4  0      1    1 series_4    1 #> 5  3      2    1 series_1    2 #> 6  3      2    1 series_2    2 #> 7  3      2    1 series_3    2 #> 8  5      2    1 series_4    2 #> 9  1      3    1 series_1    3 #> 10 0      3    1 series_2    3 #> 11 4      3    1 series_3    3 #> 12 5      3    1 series_4    3 portal_data %>%   # Filter the data to only contain captures of the 'PP'    dplyr::filter(series == 'PP') %>%   droplevels() %>%   dplyr::mutate(count = captures) %>%   # Add a 'year' variable   dplyr::mutate(year = sort(rep(1:8, 12))[time]) %>%   # Select the variables of interest to keep in the model_data   dplyr::select(series, year, time, count, mintemp, ndvi_ma12) -> model_data head(model_data) #>   series year time count     mintemp   ndvi_ma12 #> 1     PP    1    1     0 -0.79633807 -0.17214413 #> 2     PP    1    2    NA -1.33471597 -0.23736348 #> 3     PP    1    3     0 -1.24166462 -0.21212064 #> 4     PP    1    4     1 -1.08048145 -0.16043812 #> 5     PP    1    5     7 -0.42447625 -0.08267729 #> 6     PP    1    6     7  0.06532892 -0.03692877 dplyr::glimpse(model_data) #> Rows: 80 #> Columns: 6 #> $ series    <fct> PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, … #> $ year      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, … #> $ time      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… #> $ count     <int> 0, NA, 0, 1, 7, 7, 8, 8, 4, NA, 0, 0, 0, 0, 0, 0, NA, 2, 4, … #> $ mintemp   <dbl> -0.79633807, -1.33471597, -1.24166462, -1.08048145, -0.42447… #> $ ndvi_ma12 <dbl> -0.172144125, -0.237363477, -0.212120638, -0.160438125, -0.0… summary(model_data) #>  series       year           time           count           mintemp        #>  PP:80   Min.   :1.00   Min.   : 1.00   Min.   : 0.000   Min.   :-2.0978   #>          1st Qu.:2.00   1st Qu.:20.75   1st Qu.: 1.000   1st Qu.:-1.0808   #>          Median :4.00   Median :40.50   Median : 5.000   Median :-0.4091   #>          Mean   :3.85   Mean   :40.50   Mean   : 5.222   Mean   :-0.2151   #>          3rd Qu.:5.25   3rd Qu.:60.25   3rd Qu.: 8.000   3rd Qu.: 0.6133   #>          Max.   :7.00   Max.   :80.00   Max.   :21.000   Max.   : 1.4530   #>                                         NA's   :17                         #>    ndvi_ma12        #>  Min.   :-0.66884   #>  1st Qu.:-0.20869   #>  Median :-0.16517   #>  Mean   :-0.09501   #>  3rd Qu.:-0.03440   #>  Max.   : 0.74831   #> plot_mvgam_series(data = model_data, series = 1, y = \"count\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"glms-with-temporal-random-effects","dir":"Articles","previous_headings":"","what":"GLMs with temporal random effects","title":"Overview of the mvgam package","text":"first task fit Generalized Linear Model (GLM) can adequately capture features count observations (integer data, lower bound zero, missing values) also attempting model temporal variation. almost ready fit first model, GLM Poisson observations, log link function random (hierarchical) intercepts year. allow us capture prior belief , although year unique, sampled population effects, years connected thus might contain valuable information one another. done capitalizing partial pooling properties hierarchical models. Hierarchical (also known random) effects offer many advantages modelling data grouping structures (.e. multiple species, locations, years etc…). ability incorporate time series models huge advantage traditional models ARIMA Exponential Smoothing. fit model, need convert year factor can use random effect basis mvgam. See ?smooth.terms ?smooth.construct.re.smooth.spec details re basis construction used mvgam mgcv Preview dataset ensure year now factor unique factor level year data now ready first mvgam model. syntax familiar users previously built models mgcv. refresher, see ?formula.gam examples ?gam. Random effects can specified using s wrapper re basis. Note can also suppress primary intercept using usual R formula syntax - 1. mvgam number possible observation families can used, see ?mvgam_families information. use Stan fitting engine, deploys Hamiltonian Monte Carlo (HMC) full Bayesian inference. default, 4 HMC chains run using warmup 500 iterations collecting 500 posterior samples chain. package also aim use Cmdstan backend possible, recommended users --date installation Cmdstan associated cmdstanr interface machines (note can set backend using backend argument: see ?mvgam details). Interested users consult Stan user’s guide information software enormous variety models can tackled HMC. model can described mathematically timepoint \\(t\\) follows: \\[\\begin{align*} \\boldsymbol{count}_t & \\sim \\text{Poisson}(\\lambda_t) \\\\ log(\\lambda_t) & = \\beta_{year[year_t]} \\\\ \\beta_{year} & \\sim \\text{Normal}(\\mu_{year}, \\sigma_{year}) \\end{align*}\\] \\(\\beta_{year}\\) effects drawn population distribution parameterized common mean \\((\\mu_{year})\\) variance \\((\\sigma_{year})\\). Priors model parameters can interrogated changed using similar functionality options available brms. example, default priors \\((\\mu_{year})\\) \\((\\sigma_{year})\\) can viewed using following code: See examples ?get_mvgam_priors find different ways priors can altered. model finished, first step inspect summary ensure major diagnostic warnings produced quickly summarise posterior distributions key parameters diagnostic messages bottom summary show HMC sampler encounter problems difficult posterior spaces. good sign. Posterior distributions model parameters can extracted way object class brmsfit can (see ?mvgam::mvgam_draws details). example, can extract coefficients related GAM linear predictor (.e. \\(\\beta\\)’s) data.frame using: model fitted mvgam, underlying Stan code can viewed using stancode() function:","code":"model_data %>%   # Create a 'year_fac' factor version of 'year'   dplyr::mutate(year_fac = factor(year)) -> model_data dplyr::glimpse(model_data) #> Rows: 80 #> Columns: 7 #> $ series    <fct> PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, PP, … #> $ year      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, … #> $ time      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… #> $ count     <int> 0, NA, 0, 1, 7, 7, 8, 8, 4, NA, 0, 0, 0, 0, 0, 0, NA, 2, 4, … #> $ mintemp   <dbl> -0.79633807, -1.33471597, -1.24166462, -1.08048145, -0.42447… #> $ ndvi_ma12 <dbl> -0.172144125, -0.237363477, -0.212120638, -0.160438125, -0.0… #> $ year_fac  <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, … levels(model_data$year_fac) #> [1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" model1 <- mvgam(   count ~ s(year_fac, bs = \"re\") - 1,   family = poisson(),   data = model_data ) get_mvgam_priors(count ~ s(year_fac, bs = \"re\") - 1,   family = poisson(),   data = model_data ) #>                      param_name param_length           param_info #> 1             vector[1] mu_raw;            1 s(year_fac) pop mean #> 2 vector<lower=0>[1] sigma_raw;            1   s(year_fac) pop sd #>                                  prior                example_change #> 1               mu_raw ~ std_normal();  mu_raw ~ normal(0.45, 0.81); #> 2 sigma_raw ~ inv_gamma(1.418, 0.452); sigma_raw ~ exponential(0.8); #>   new_lowerbound new_upperbound #> 1             NA             NA #> 2             NA             NA summary(model1) #> GAM formula: #> count ~ s(year_fac, bs = \"re\") - 1 #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> None #>  #> N series: #> 1  #>  #> N timepoints: #> 80  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> GAM coefficient (beta) estimates: #>                2.5% 50% 97.5% Rhat n_eff #> s(year_fac).1 0.920 1.3   1.6    1  2359 #> s(year_fac).2 0.870 1.2   1.5    1  2652 #> s(year_fac).3 0.064 0.6   1.0    1  2082 #> s(year_fac).4 2.100 2.3   2.5    1  2256 #> s(year_fac).5 1.100 1.5   1.8    1  2520 #> s(year_fac).6 1.500 1.8   2.1    1  2405 #> s(year_fac).7 1.800 2.1   2.3    1  2508 #>  #> GAM group-level estimates: #>                   2.5% 50% 97.5% Rhat n_eff #> mean(s(year_fac)) 0.85 1.5   1.9 1.01   367 #> sd(s(year_fac))   0.36 0.6   1.2 1.01   497 #>  #> Approximate significance of GAM smooths: #>              edf Ref.df Chi.sq p-value     #> s(year_fac) 6.01      7    237  <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model beta_post <- as.data.frame(model1, variable = \"betas\") dplyr::glimpse(beta_post) #> Rows: 2,000 #> Columns: 7 #> $ `s(year_fac).1` <dbl> 1.214940, 1.327580, 1.312270, 1.277200, 1.580130, 0.92… #> $ `s(year_fac).2` <dbl> 1.314350, 1.107400, 1.242660, 1.254060, 1.283390, 1.11… #> $ `s(year_fac).3` <dbl> 0.5728820, 0.7694690, 0.2130090, 0.7907590, 0.6334420,… #> $ `s(year_fac).4` <dbl> 2.32520, 2.33122, 2.19439, 2.26896, 2.38990, 2.27307, … #> $ `s(year_fac).5` <dbl> 1.58084, 1.24458, 1.50030, 1.58874, 1.44873, 1.56224, … #> $ `s(year_fac).6` <dbl> 1.74901, 2.02590, 1.80032, 1.88439, 1.82428, 1.81131, … #> $ `s(year_fac).7` <dbl> 2.08108, 2.30094, 2.28501, 1.88996, 1.93482, 2.14432, … stancode(model1) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // random effect variances #>   vector<lower=0>[1] sigma_raw; #>    #>   // random effect means #>   vector[1] mu_raw; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : 7] = mu_raw[1] + b_raw[1 : 7] * sigma_raw[1]; #> } #> model { #>   // prior for random effect population variances #>   sigma_raw ~ inv_gamma(1.418, 0.452); #>    #>   // prior for random effect population means #>   mu_raw ~ std_normal(); #>    #>   // prior (non-centred) for s(year_fac)... #>   b_raw[1 : 7] ~ std_normal(); #>   { #>     // likelihood functions #>     flat_ys ~ poisson_log_glm(flat_xs, 0.0, b); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   array[n, n_series] int ypred; #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> }"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"plotting-effects-and-residuals","dir":"Articles","previous_headings":"GLMs with temporal random effects","what":"Plotting effects and residuals","title":"Overview of the mvgam package","text":"Now interrogating model. can get sense variation yearly intercepts summary , easier understand using targeted plots. Plot posterior distributions temporal random effects using plot.mvgam type = 're'. See ?plot.mvgam details types plots can produced fitted mvgam objects","code":"plot(model1, type = \"re\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"bayesplot-support","dir":"Articles","previous_headings":"GLMs with temporal random effects","what":"bayesplot support","title":"Overview of the mvgam package","text":"can also capitalize useful MCMC plotting functions bayesplot package visualize posterior distributions diagnostics (see ?mvgam::mcmc_plot.mvgam details):  can also use wide range posterior checking functions available bayesplot (see ?mvgam::ppc_check.mvgam details):  clearly variation yearly intercept estimates. translate time-varying predictions? understand , can plot posterior hindcasts model training period using plot.mvgam type = 'forecast'  wish extract hindcasts downstream analyses, hindcast function can used. return list object class mvgam_forecast. hindcasts slot, matrix posterior retrodictions returned series data (one series example): can also extract hindcasts linear predictor scale, case log scale (Poisson GLM used log link function). Sometimes can useful asking targeted questions drivers variation: regression analysis, key question whether residuals show patterns can indicative un-modelled sources variation. GLMs, can use modified residual called Dunn-Smyth, randomized quantile, residual. Inspect Dunn-Smyth residuals model using plot.mvgam type = 'residuals'","code":"mcmc_plot(   object = model1,   variable = \"betas\",   type = \"areas\" ) pp_check(object = model1) plot(model1, type = \"forecast\") hc <- hindcast(model1) str(hc) #> List of 15 #>  $ call              :Class 'formula'  language count ~ s(year_fac, bs = \"re\") - 1 #>   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ trend_model       : chr \"None\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : chr \"PP\" #>  $ train_observations:List of 1 #>   ..$ PP: int [1:80] 0 NA 0 1 7 7 8 8 4 NA ... #>  $ train_times       :List of 1 #>   ..$ PP: int [1:80] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations : NULL #>  $ test_times        : NULL #>  $ hindcasts         :List of 1 #>   ..$ PP: num [1:2000, 1:80] 1 4 2 1 2 4 1 6 5 4 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:80] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>  $ forecasts         : NULL #>  - attr(*, \"class\")= chr \"mvgam_forecast\" hc <- hindcast(model1, type = \"link\") range(hc$hindcasts$PP) #> [1] -0.367547  2.593740 plot(model1, type = \"residuals\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"automatic-forecasting-for-new-data","dir":"Articles","previous_headings":"","what":"Automatic forecasting for new data","title":"Overview of the mvgam package","text":"temporal random effects sense “time”. , yearly random intercept restricted way similar previous yearly intercept. drawback becomes evident predict new year. , can repeat exercise time split data training testing sets re-running model. can supply test set newdata. splitting, make use filter function dplyr can view test data forecast plot see predictions capture temporal variation test set  hindcast function, can use forecast function automatically extract posterior distributions predictions. also returns object class mvgam_forecast, now contain hindcasts forecasts series data:","code":"model_data %>%   dplyr::filter(time <= 70) -> data_train model_data %>%   dplyr::filter(time > 70) -> data_test model1b <- mvgam(   count ~ s(year_fac, bs = \"re\") - 1,   family = poisson(),   data = data_train,   newdata = data_test ) plot(model1b, type = \"forecast\", newdata = data_test) fc <- forecast(model1b) str(fc) #> List of 16 #>  $ call              :Class 'formula'  language count ~ s(year_fac, bs = \"re\") - 1 #>   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ family_pars       : NULL #>  $ trend_model       : chr \"None\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : Factor w/ 1 level \"PP\": 1 #>  $ train_observations:List of 1 #>   ..$ PP: int [1:70] 0 NA 0 1 7 7 8 8 4 NA ... #>  $ train_times       :List of 1 #>   ..$ PP: int [1:70] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations :List of 1 #>   ..$ PP: int [1:10] NA 4 11 8 5 2 5 8 14 14 #>  $ test_times        :List of 1 #>   ..$ PP: int [1:10] 71 72 73 74 75 76 77 78 79 80 #>  $ hindcasts         :List of 1 #>   ..$ PP: num [1:2000, 1:70] 6 3 2 2 3 3 4 4 4 0 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:70] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>  $ forecasts         :List of 1 #>   ..$ PP: num [1:2000, 1:10] 4 13 10 13 7 7 5 4 5 5 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:10] \"ypred[71,1]\" \"ypred[72,1]\" \"ypred[73,1]\" \"ypred[74,1]\" ... #>  - attr(*, \"class\")= chr \"mvgam_forecast\""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"adding-predictors-as-fixed-effects","dir":"Articles","previous_headings":"","what":"Adding predictors as “fixed” effects","title":"Overview of the mvgam package","text":"users familiar GLMs know nearly always wish include predictor variables may explain variation observations. Predictors easily incorporated GLMs / GAMs. , update model including parametric (fixed) effect ndvi_ma12 linear predictor: model can described mathematically follows: \\[\\begin{align*} \\boldsymbol{count}_t & \\sim \\text{Poisson}(\\lambda_t) \\\\ log(\\lambda_t) & = \\beta_{year[year_t]} + \\beta_{ndvi} * \\boldsymbol{ndvi}_t \\\\ \\beta_{year} & \\sim \\text{Normal}(\\mu_{year}, \\sigma_{year}) \\\\ \\beta_{ndvi} & \\sim \\text{Normal}(0, 1) \\end{align*}\\] \\(\\beta_{year}\\) effects now another predictor \\((\\beta_{ndvi})\\) applies ndvi_ma12 value timepoint \\(t\\). Inspect summary model Rather printing summary time, can also quickly look posterior empirical quantiles fixed effect ndvi (linear predictor coefficients) using coef: Look estimated effect ndvi using using histogram. can done first extracting posterior coefficients: posterior distribution effect ndvi stored ndvi_ma12 column. quick histogram confirms inference log(counts) respond positively increases ndvi:","code":"model2 <- mvgam(   count ~ s(year_fac, bs = \"re\") +     ndvi_ma12 - 1,   family = poisson(),   data = data_train,   newdata = data_test ) summary(model2) #> GAM formula: #> count ~ ndvi_ma12 + s(year_fac, bs = \"re\") - 1 #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> None #>  #> N series: #> 1  #>  #> N timepoints: #> 80  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> GAM coefficient (beta) estimates: #>                 2.5%   50% 97.5% Rhat n_eff #> ndvi_ma12     -0.410 0.043  0.47    1  1582 #> s(year_fac).1  0.920 1.300  1.60    1  2230 #> s(year_fac).2  0.860 1.200  1.50    1  2421 #> s(year_fac).3  0.061 0.590  1.10    1  2217 #> s(year_fac).4  2.000 2.300  2.50    1  2076 #> s(year_fac).5  1.100 1.500  1.80    1  2270 #> s(year_fac).6  1.600 1.800  2.10    1  2140 #> s(year_fac).7 -0.120 1.400  2.70    1   988 #>  #> GAM group-level estimates: #>                   2.5%  50% 97.5% Rhat n_eff #> mean(s(year_fac)) 0.67 1.40   1.9 1.02   373 #> sd(s(year_fac))   0.33 0.58   1.3 1.02   329 #>  #> Approximate significance of GAM smooths: #>              edf Ref.df Chi.sq p-value     #> s(year_fac) 5.24      7    179  <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model coef(model2) #>                      2.5%        50%     97.5% Rhat n_eff #> ndvi_ma12     -0.40996485 0.04283945 0.4673025    1  1582 #> s(year_fac).1  0.91901945 1.26033000 1.5707212    1  2230 #> s(year_fac).2  0.86327525 1.20950500 1.5203245    1  2421 #> s(year_fac).3  0.06073426 0.59372800 1.0606170    1  2217 #> s(year_fac).4  2.02198850 2.27009500 2.4819213    1  2076 #> s(year_fac).5  1.10297525 1.45693500 1.7714813    1  2270 #> s(year_fac).6  1.58039150 1.84542500 2.1097990    1  2140 #> s(year_fac).7 -0.11933482 1.39343500 2.7083200    1   988 beta_post <- as.data.frame(model2, variable = \"betas\") dplyr::glimpse(beta_post) #> Rows: 2,000 #> Columns: 8 #> $ ndvi_ma12       <dbl> 0.15092000, -0.07224160, -0.01072470, 0.06178290, 0.32… #> $ `s(year_fac).1` <dbl> 1.260620, 1.237480, 1.336680, 1.391520, 1.176470, 1.13… #> $ `s(year_fac).2` <dbl> 0.986840, 1.238950, 1.241640, 1.344040, 1.203340, 1.26… #> $ `s(year_fac).3` <dbl> 0.6400060, 0.5338640, 0.5596370, 0.5185390, 0.5265630,… #> $ `s(year_fac).4` <dbl> 2.49096, 2.25690, 2.16193, 2.27607, 2.36063, 2.29226, … #> $ `s(year_fac).5` <dbl> 1.49968, 1.45541, 1.40851, 1.39896, 1.48283, 1.51829, … #> $ `s(year_fac).6` <dbl> 1.87853, 1.90285, 1.79225, 1.76814, 1.93750, 1.87616, … #> $ `s(year_fac).7` <dbl> 1.384440, 1.273460, 0.672738, 1.240540, 0.947571, 1.05… hist(beta_post$ndvi_ma12,   xlim = c(     -1 * max(abs(beta_post$ndvi_ma12)),     max(abs(beta_post$ndvi))   ),   col = \"darkred\",   border = \"white\",   xlab = expression(beta[NDVI]),   ylab = \"\",   yaxt = \"n\",   main = \"\",   lwd = 2 ) abline(v = 0, lwd = 2.5)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"marginaleffects-support","dir":"Articles","previous_headings":"Adding predictors as “fixed” effects","what":"marginaleffects support","title":"Overview of the mvgam package","text":"Given model used nonlinear link function (log link example), can still difficult fully understand relationship model estimating predictor response. Fortunately, marginaleffects package makes relatively straightforward. Objects class mvgam can used marginaleffects inspect contrasts, scenario-based predictions, conditional marginal effects, outcome scale. Like brms, mvgam simple conditional_effects function make quick informative plots main effects, rely marginaleffects support. likely go-function quickly understanding patterns fitted mvgam models","code":"conditional_effects(model2)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"adding-predictors-as-smooths","dir":"Articles","previous_headings":"","what":"Adding predictors as smooths","title":"Overview of the mvgam package","text":"Smooth functions, using penalized splines, major feature mvgam. Nonlinear splines commonly viewed variations random effects coefficients control shape spline drawn joint, penalized distribution. strategy often used ecological time series analysis capture smooth temporal variation processes seek study. construct smoothing splines, workhorse package mgcv calculate set basis functions collectively control shape complexity resulting spline. often helpful visualize basis functions get better sense splines work. ’ll create set 6 basis functions represent possible variation effect time outcome.addition constructing basis functions, mgcv also creates penalty matrix \\(S\\), contains known coefficients work constrain wiggliness resulting smooth function. fitting GAM data, must estimate smoothing parameters (\\(\\lambda\\)) penalize matrices, resulting constrained basis coefficients smoother functions less likely overfit data. key fitting GAMs Bayesian framework, can jointly estimate \\(\\lambda\\)’s using informative priors prevent overfitting expand complexity models can tackle. see practice, can now fit model replaces yearly random effects smooth function time. need reasonably complex function (large k) try accommodate temporal variation observations. Following useful advice Gavin Simpson, use b-spline basis temporal smooth. longer intercepts year, also retain primary intercept term model (-1 formula now): model can described mathematically follows: \\[\\begin{align*} \\boldsymbol{count}_t & \\sim \\text{Poisson}(\\lambda_t) \\\\ log(\\lambda_t) & = f(\\boldsymbol{time})_t + \\beta_{ndvi} * \\boldsymbol{ndvi}_t  \\\\ f(\\boldsymbol{time}) & = \\sum_{k=1}^{K}b * \\beta_{smooth} \\\\ \\beta_{smooth} & \\sim \\text{MVNormal}(0, (\\Omega * \\lambda)^{-1}) \\\\ \\beta_{ndvi} & \\sim \\text{Normal}(0, 1) \\end{align*}\\] smooth function \\(f_{time}\\) built summing across set weighted basis functions. basis functions \\((b)\\) constructed using thin plate regression basis mgcv. weights \\((\\beta_{smooth})\\) drawn penalized multivariate normal distribution precision matrix \\((\\Omega\\)) multiplied smoothing penalty \\((\\lambda)\\). \\(\\lambda\\) becomes large, acts squeeze covariances among weights \\((\\beta_{smooth})\\), leading less wiggly spline. Note sometimes multiple smoothing penalties contribute covariance matrix, showing one simplicity. View summary summary now contains posterior estimates smoothing parameters well basis coefficients nonlinear effect time. can visualize conditional_effects :  Inspect underlying Stan code gain idea spline penalized: line // prior s(time)... shows spline basis coefficients drawn zero-centred multivariate normal distribution. precision matrix \\(S\\) penalized two different smoothing parameters (\\(\\lambda\\)’s) enforce smoothness reduce overfitting","code":"model3 <- mvgam(   count ~ s(time, bs = \"bs\", k = 15) +     ndvi_ma12,   family = poisson(),   data = data_train,   newdata = data_test ) summary(model3) #> GAM formula: #> count ~ s(time, bs = \"bs\", k = 15) + ndvi_ma12 #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> None #>  #> N series: #> 1  #>  #> N timepoints: #> 80  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> GAM coefficient (beta) estimates: #>               2.5%    50% 97.5% Rhat n_eff #> (Intercept)   0.80  1.100  1.40 1.00   659 #> ndvi_ma12     0.34  1.900  3.70 1.00   927 #> s(time).1    -9.70 -5.800 -2.40 1.00   515 #> s(time).2     1.30  3.500  6.10 1.01   495 #> s(time).3   -10.00 -6.100 -2.80 1.00   436 #> s(time).4    -1.50  0.830  3.50 1.01   413 #> s(time).5    -3.00 -0.330  2.20 1.01   386 #> s(time).6    -6.60 -3.700 -0.93 1.01   445 #> s(time).7    -1.70  0.630  3.30 1.01   372 #> s(time).8    -2.30 -0.088  2.30 1.01   376 #> s(time).9    -0.42  2.100  5.00 1.01   379 #> s(time).10   -5.60 -3.200 -0.93 1.00   408 #> s(time).11   -2.60  0.560  4.10 1.01   548 #> s(time).12   -6.80 -5.000 -3.00 1.01   601 #> s(time).13    1.90  5.100  8.70 1.01   391 #> s(time).14  -12.00 -3.100  3.80 1.01   389 #>  #> Approximate significance of GAM smooths: #>          edf Ref.df Chi.sq p-value     #> s(time) 11.8     14    102  <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model conditional_effects(model3, type = \"link\") stancode(model3) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[14, 28] S1; // mgcv smooth penalty matrix S1 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : num_basis] = b_raw[1 : num_basis]; #> } #> model { #>   // prior for (Intercept)... #>   b_raw[1] ~ student_t(3, 1.4, 2.5); #>    #>   // prior for ndvi_ma12... #>   b_raw[2] ~ student_t(3, 0, 2); #>    #>   // prior for s(time)... #>   b_raw[3 : 16] ~ multi_normal_prec(zero[3 : 16], #>                                     S1[1 : 14, 1 : 14] * lambda[1] #>                                     + S1[1 : 14, 15 : 28] * lambda[2]); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>   { #>     // likelihood functions #>     flat_ys ~ poisson_log_glm(flat_xs, 0.0, b); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   array[n, n_series] int ypred; #>   rho = log(lambda); #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> }"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"latent-dynamics-in-mvgam","dir":"Articles","previous_headings":"","what":"Latent dynamics in mvgam","title":"Overview of the mvgam package","text":"Forecasts model ideal:  happening? forecasts driven almost entirely variation temporal spline, extrapolating linearly forever beyond edge training data. slight wiggles near end training set result wildly different forecasts. visualize , can plot extrapolated temporal functions --sample test set two models. extrapolated functions first model, 15 basis functions:  model well. Clearly need somehow account strong temporal autocorrelation modelling data without using smooth function time. Now onto another prominent feature mvgam: ability include (possibly latent) autocorrelated residuals regression models. , use trend_model argument (see ?mvgam_trends details different dynamic trend models supported). model use separate sub-model latent residuals evolve AR1 process (.e. error current time point function error previous time point, plus stochastic noise). also include smooth function ndvi_ma12 model, rather parametric term used , showcase mvgam can include combinations smooths dynamic components: model can described mathematically follows: \\[\\begin{align*} \\boldsymbol{count}_t & \\sim \\text{Poisson}(\\lambda_t) \\\\ log(\\lambda_t) & = f(\\boldsymbol{ndvi})_t + z_t \\\\ z_t & \\sim \\text{Normal}(ar1 * z_{t-1}, \\sigma_{error}) \\\\ ar1 & \\sim \\text{Normal}(0, 1)[-1, 1] \\\\ \\sigma_{error} & \\sim \\text{Exponential}(2) \\\\ f(\\boldsymbol{ndvi}) & = \\sum_{k=1}^{K}b * \\beta_{smooth} \\\\ \\beta_{smooth} & \\sim \\text{MVNormal}(0, (\\Omega * \\lambda)^{-1}) \\end{align*}\\] term \\(z_t\\) captures autocorrelated latent residuals, modelled using AR1 process. can also notice model estimating autocorrelated errors full time period, even though time points missing observations. useful getting realistic estimates residual autocorrelation parameters. Summarise model see now returns posterior summaries latent AR1 process: View posterior hindcasts / forecasts compare sample test data  trend evolving AR1 process, can also view:  -sample model performance can interrogated using leave-one-cross-validation utilities loo package (higher value preferred metric): higher estimated log predictive density (ELPD) value dynamic model suggests provides better fit -sample data. Though obvious model provides better forecasts, can quantify forecast performance models 3 4 using forecast score functions. compare models based Discrete Ranked Probability Scores (lower value preferred metric) strongly negative value suggests score dynamic model (model 4) much smaller score model smooth function time (model 3)","code":"plot(model3, type = \"forecast\", newdata = data_test) plot_mvgam_smooth(   model3,   smooth = \"s(time)\",   # feed newdata to the plot function to generate   # predictions of the temporal smooth to the end of the   # testing period   newdata = data.frame(     time = 1:max(data_test$time),     ndvi_ma12 = 0   ) ) abline(v = max(data_train$time), lty = \"dashed\", lwd = 2) model4 <- mvgam(   count ~ s(ndvi_ma12, k = 6),   family = poisson(),   data = data_train,   newdata = data_test,   trend_model = AR() ) summary(model4) #> GAM formula: #> count ~ s(ndvi_ma12, k = 6) #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> AR() #>  #>  #> N series: #> 1  #>  #> N timepoints: #> 80  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> GAM coefficient (beta) estimates: #>                  2.5%     50% 97.5% Rhat n_eff #> (Intercept)    -1.500  0.7200 1.600 1.03   133 #> s(ndvi_ma12).1 -0.130  0.0061 0.190 1.00  1002 #> s(ndvi_ma12).2 -0.180 -0.0057 0.160 1.00  1089 #> s(ndvi_ma12).3 -0.072 -0.0015 0.063 1.01   934 #> s(ndvi_ma12).4 -0.360  0.0240 0.530 1.00   969 #> s(ndvi_ma12).5 -0.280  0.0650 0.500 1.00   940 #>  #> Approximate significance of GAM smooths: #>               edf Ref.df Chi.sq p-value #> s(ndvi_ma12) 1.01      5   0.37       1 #>  #> Latent trend parameter AR estimates: #>          2.5%  50% 97.5% Rhat n_eff #> ar1[1]   0.63 0.83  0.98 1.01   273 #> sigma[1] 0.57 0.80  1.10 1.01   319 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model plot(model4, type = \"forecast\", newdata = data_test) plot(model4, type = \"trend\", newdata = data_test) loo_compare(model3, model4) #>        elpd_diff se_diff #> model3    0.0       0.0  #> model4 -842.2     151.5 fc_mod3 <- forecast(model3) fc_mod4 <- forecast(model4) score_mod3 <- score(fc_mod3, score = \"drps\") score_mod4 <- score(fc_mod4, score = \"drps\") sum(score_mod4$PP$score, na.rm = TRUE) -    sum(score_mod3$PP$score, na.rm = TRUE) #> [1] -578.8946"},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"Overview of the mvgam package","text":"following papers resources offer useful material Dynamic GAMs can applied practice: Clark, Nicholas J. Wells, K. Dynamic Generalized Additive Models (DGAMs) forecasting discrete ecological time series. Methods Ecology Evolution. (2023): 14, 771-784. Clark, Nicholas J., et al. Beyond single-species models: leveraging multispecies forecasts navigate dynamics ecological predictability. PeerJ. (2025): 13:e18929 de Sousa, Heitor C., et al. Severe fire regimes decrease resilience ectothermic populations. Journal Animal Ecology (2024): 93(11), 1656-1669. Hannaford, Naomi E., et al. sparse Bayesian hierarchical vector autoregressive model microbial dynamics wastewater treatment plant. Computational Statistics & Data Analysis (2023): 179, 107659. Karunarathna, K..N.K., et al. Modelling nonlinear responses desert rodent species environmental change hierarchical dynamic generalized additive models. Ecological Modelling (2024): 490, 110648. Zhu, L., et al. Responses widespread pest insect extreme high temperatures stage-dependent divergent among seasonal cohorts. Functional Ecology (2025): 39, 165–180. https://doi.org/10.1111/1365-2435.14711","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","id":"interested-in-contributing","dir":"Articles","previous_headings":"","what":"Interested in contributing?","title":"Overview of the mvgam package","text":"’m actively seeking PhD students researchers work areas ecological forecasting, multivariate model evaluation development mvgam. Please see small list opportunities website reach interested (n.clark’’uq.edu.au)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/nmixtures.html","id":"n-mixture-models","dir":"Articles","previous_headings":"","what":"N-mixture models","title":"N-mixtures in mvgam","text":"N-mixture model fairly recent addition ecological modeller’s toolkit designed make inferences variation abundance species observations imperfect (Royle 2004). Briefly, assume \\(\\boldsymbol{Y_{,r}}\\) number individuals recorded site \\(\\) replicate sampling observation \\(r\\) (recorded non-negative integer). multiple replicate surveys done within short enough period satisfy assumption population remained closed (.e. substantial change true population size replicate surveys), can account fact observations aren’t perfect. done assuming replicate observations Binomial random variables parameterized true “latent” abundance \\(N\\) detection probability \\(p\\): \\[\\begin{align*} \\boldsymbol{Y_{,r}} & \\sim \\text{Binomial}(N_i, p_r) \\\\ N_{} & \\sim \\text{Poisson}(\\lambda_i)  \\end{align*}\\] Using set linear predictors, can estimate effects covariates \\(\\boldsymbol{X}\\) expected latent abundance (log link \\(\\lambda\\)) , jointly, effects possibly different covariates (call \\(\\boldsymbol{Q}\\)) detection probability (logit link \\(p\\)): \\[\\begin{align*} log(\\lambda) & = \\beta \\boldsymbol{X} \\\\ logit(p) & = \\gamma \\boldsymbol{Q}\\end{align*}\\] mvgam can handle type model designed propagate unobserved temporal processes evolve independently observation process State-space format. setup adapts well N-mixture models can thought State-space models latent state discrete variable representing “true” unknown population size. convenient can incorporate package’s diverse effect types (.e. multidimensional splines, time-varying effects, monotonic effects, random effects etc…) linear predictors. required work marginalization trick allows Stan’s sampling algorithms handle discrete parameters (see method “integrating ” discrete parameters works nice blog post Maxwell Joseph). family nmix() used set N-mixture models mvgam, still need little bit data wrangling ensure data set correct format (especially true one replicate survey per time period). important aspects : (1) set observation series trend_map arguments ensure replicate surveys mapped correct latent abundance model (2) inclusion cap variable defines maximum possible integer value use observation estimating latent abundance. two examples give reasonable overview can done.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/nmixtures.html","id":"example-1-a-two-species-system-with-nonlinear-trends","dir":"Articles","previous_headings":"","what":"Example 1: a two-species system with nonlinear trends","title":"N-mixtures in mvgam","text":"First use simple simulation multiple replicate observations taken timepoint two different species. simulation produces observations single site six years, five replicate surveys per year. species simulated different nonlinear temporal trends different detection probabilities. now, detection probability fixed (.e. change time association covariates). Notice add cap variable, need static, define maximum possible value think latent abundance timepoint. simply needs large enough get reasonable idea latent N values likely, without adding much computational cost: data format isn’t difficult set , differ traditional multidimensional array setup commonly used fitting N-mixture models software packages. Next ensure species series IDs included factor variables, case ’d like allow certain effects vary species Preview dataset get idea structured:","code":"set.seed(999) # Simulate observations for species 1, which shows a declining trend and 0.7 detection probability data.frame(   site = 1,   # five replicates per year; six years   replicate = rep(1:5, 6),   time = sort(rep(1:6, 5)),   species = \"sp_1\",   # true abundance declines nonlinearly   truth = c(     rep(28, 5),     rep(26, 5),     rep(23, 5),     rep(16, 5),     rep(14, 5),     rep(14, 5)   ),   # observations are taken with detection prob = 0.7   obs = c(     rbinom(5, 28, 0.7),     rbinom(5, 26, 0.7),     rbinom(5, 23, 0.7),     rbinom(5, 15, 0.7),     rbinom(5, 14, 0.7),     rbinom(5, 14, 0.7)   ) ) %>%   # add 'series' information, which is an identifier of site, replicate and species   dplyr::mutate(     series = paste0(       \"site_\", site,       \"_\", species,       \"_rep_\", replicate     ),     time = as.numeric(time),     # add a 'cap' variable that defines the maximum latent N to     # marginalize over when estimating latent abundance; in other words     # how large do we realistically think the true abundance could be?     cap = 100   ) %>%   dplyr::select(-replicate) -> testdat  # Now add another species that has a different temporal trend and a smaller # detection probability (0.45 for this species) testdat <- testdat %>%   dplyr::bind_rows(data.frame(     site = 1,     replicate = rep(1:5, 6),     time = sort(rep(1:6, 5)),     species = \"sp_2\",     truth = c(       rep(4, 5),       rep(7, 5),       rep(15, 5),       rep(16, 5),       rep(19, 5),       rep(18, 5)     ),     obs = c(       rbinom(5, 4, 0.45),       rbinom(5, 7, 0.45),       rbinom(5, 15, 0.45),       rbinom(5, 16, 0.45),       rbinom(5, 19, 0.45),       rbinom(5, 18, 0.45)     )   ) %>%     dplyr::mutate(       series = paste0(         \"site_\", site,         \"_\", species,         \"_rep_\", replicate       ),       time = as.numeric(time),       cap = 50     ) %>%     dplyr::select(-replicate)) testdat$species <- factor(testdat$species,   levels = unique(testdat$species) ) testdat$series <- factor(testdat$series,   levels = unique(testdat$series) ) dplyr::glimpse(testdat) #> Rows: 60 #> Columns: 7 #> $ site    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… #> $ time    <dbl> 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5,… #> $ species <fct> sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp… #> $ truth   <dbl> 28, 28, 28, 28, 28, 26, 26, 26, 26, 26, 23, 23, 23, 23, 23, 16… #> $ obs     <int> 20, 19, 23, 17, 18, 21, 18, 21, 19, 18, 17, 16, 20, 11, 19, 9,… #> $ series  <fct> site_1_sp_1_rep_1, site_1_sp_1_rep_2, site_1_sp_1_rep_3, site_… #> $ cap     <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 10… head(testdat, 12) #>    site time species truth obs            series cap #> 1     1    1    sp_1    28  20 site_1_sp_1_rep_1 100 #> 2     1    1    sp_1    28  19 site_1_sp_1_rep_2 100 #> 3     1    1    sp_1    28  23 site_1_sp_1_rep_3 100 #> 4     1    1    sp_1    28  17 site_1_sp_1_rep_4 100 #> 5     1    1    sp_1    28  18 site_1_sp_1_rep_5 100 #> 6     1    2    sp_1    26  21 site_1_sp_1_rep_1 100 #> 7     1    2    sp_1    26  18 site_1_sp_1_rep_2 100 #> 8     1    2    sp_1    26  21 site_1_sp_1_rep_3 100 #> 9     1    2    sp_1    26  19 site_1_sp_1_rep_4 100 #> 10    1    2    sp_1    26  18 site_1_sp_1_rep_5 100 #> 11    1    3    sp_1    23  17 site_1_sp_1_rep_1 100 #> 12    1    3    sp_1    23  16 site_1_sp_1_rep_2 100"},{"path":"https://nicholasjclark.github.io/mvgam/articles/nmixtures.html","id":"setting-up-the-trend_map","dir":"Articles","previous_headings":"Example 1: a two-species system with nonlinear trends","what":"Setting up the trend_map","title":"N-mixtures in mvgam","text":"Finally, need set trend_map object. crucial allowing multiple observations linked latent process model (see information argument Shared latent states vignette). case, mapping operates species site state set replicate observations time point share exact latent abundance model: Notice replicates species 1 site 1 share process (.e. trend). ensure replicates Binomial draws latent N.","code":"testdat %>%   # each unique combination of site*species is a separate process   dplyr::mutate(trend = as.numeric(factor(paste0(site, species)))) %>%   dplyr::select(trend, series) %>%   dplyr::distinct() -> trend_map trend_map #>    trend            series #> 1      1 site_1_sp_1_rep_1 #> 2      1 site_1_sp_1_rep_2 #> 3      1 site_1_sp_1_rep_3 #> 4      1 site_1_sp_1_rep_4 #> 5      1 site_1_sp_1_rep_5 #> 6      2 site_1_sp_2_rep_1 #> 7      2 site_1_sp_2_rep_2 #> 8      2 site_1_sp_2_rep_3 #> 9      2 site_1_sp_2_rep_4 #> 10     2 site_1_sp_2_rep_5"},{"path":"https://nicholasjclark.github.io/mvgam/articles/nmixtures.html","id":"modelling-with-the-nmix-family","dir":"Articles","previous_headings":"Example 1: a two-species system with nonlinear trends","what":"Modelling with the nmix() family","title":"N-mixtures in mvgam","text":"Now ready fit model using mvgam(). model allow species different detection probabilities different temporal trends. use Cmdstan backend, default use Hamiltonian Monte Carlo full Bayesian inference View automatically-generated Stan code get sense marginalization latent N works posterior summary model shows converged nicely loo() functionality works just mvgam models aid model comparison / selection (though note Pareto K values often give warnings mixture models may helpful) Plot estimated smooths time species’ latent abundance process (log scale)  marginaleffects support allows useful prediction-based interrogations different scales (though note time writing Vignette, must development version marginaleffects installed nmix() models supported; use remotes::install_github('vincentarelbundock/marginaleffects') install). Objects use family nmix() additional prediction scales can used (.e. link, response, detection latent_N). example, estimated detection probabilities per species, show model done nice job estimating parameters:  common goal N-mixture modelling estimate true latent abundance. model automatically generated predictions unknown latent abundance conditional observations. can extract produce decent plots using small function Latent abundance plots vs simulated truths species shown . , red points show imperfect observations, black line shows true latent abundance, ribbons show credible intervals estimates:   can see estimates species correctly captured true temporal variation magnitudes abundance","code":"mod <- mvgam(   # the observation formula sets up linear predictors for   # detection probability on the logit scale   formula = obs ~ species - 1,    # the trend_formula sets up the linear predictors for   # the latent abundance processes on the log scale   trend_formula = ~ s(time, by = trend, k = 4) + species,    # the trend_map takes care of the mapping   trend_map = trend_map,    # nmix() family and data   family = nmix(),   data = testdat,    # priors can be set in the usual way   priors = c(     prior(std_normal(), class = b),     prior(normal(1, 1.5), class = Intercept_trend)   ),   samples = 1000 ) code(mod) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp_trend; // number of trend smoothing parameters #>   int<lower=0> n_lv; // number of dynamic factors #>   int<lower=0> n_series; // number of series #>   matrix[n_series, n_lv] Z; // matrix mapping series to latent states #>   int<lower=0> num_basis; // total number of basis coefficients #>   int<lower=0> num_basis_trend; // number of trend basis coefficients #>   vector[num_basis_trend] zero_trend; // prior locations for trend basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   matrix[n * n_lv, num_basis_trend] X_trend; // trend model design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   array[n, n_lv] int ytimes_trend; #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[total_obs] int<lower=0> cap; // upper limits of latent abundances #>   array[total_obs] int ytimes_array; // sorted ytimes #>   array[n, n_series] int<lower=0> ytimes_pred; // time-ordered matrix for prediction #>   int<lower=0> K_groups; // number of unique replicated observations #>   int<lower=0> K_reps; // maximum number of replicate observations #>   array[K_groups] int<lower=0> K_starts; // col of K_inds where each group starts #>   array[K_groups] int<lower=0> K_stops; // col of K_inds where each group ends #>   array[K_groups, K_reps] int<lower=0> K_inds; // indices of replicated observations #>   matrix[3, 6] S_trend1; // mgcv smooth penalty matrix S_trend1 #>   matrix[3, 6] S_trend2; // mgcv smooth penalty matrix S_trend2 #>   array[total_obs] int<lower=0> flat_ys; // flattened observations #> } #> transformed data { #>   matrix[total_obs, num_basis] X_ordered = X[ytimes_array,  : ]; #>   array[K_groups] int<lower=0> Y_max; #>   array[K_groups] int<lower=0> N_max; #>   for (k in 1 : K_groups) { #>     Y_max[k] = max(flat_ys[K_inds[k, K_starts[k] : K_stops[k]]]); #>     N_max[k] = max(cap[K_inds[k, K_starts[k] : K_stops[k]]]); #>   } #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>   vector[num_basis_trend] b_raw_trend; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp_trend] lambda_trend; #> } #> transformed parameters { #>   // detection probability #>   vector[total_obs] p; #>    #>   // latent states #>   matrix[n, n_lv] LV; #>    #>   // latent states and loading matrix #>   vector[n * n_lv] trend_mus; #>   matrix[n, n_series] trend; #>    #>   // basis coefficients #>   vector[num_basis] b; #>   vector[num_basis_trend] b_trend; #>    #>   // observation model basis coefficients #>   b[1 : num_basis] = b_raw[1 : num_basis]; #>    #>   // process model basis coefficients #>   b_trend[1 : num_basis_trend] = b_raw_trend[1 : num_basis_trend]; #>    #>   // detection probability #>   p = X_ordered * b; #>    #>   // latent process linear predictors #>   trend_mus = X_trend * b_trend; #>   for (j in 1 : n_lv) { #>     LV[1 : n, j] = trend_mus[ytimes_trend[1 : n, j]]; #>   } #>    #>   // derived latent states #>   for (i in 1 : n) { #>     for (s in 1 : n_series) { #>       trend[i, s] = dot_product(Z[s,  : ], LV[i,  : ]); #>     } #>   } #> } #> model { #>   // prior for speciessp_1... #>   b_raw[1] ~ std_normal(); #>    #>   // prior for speciessp_2... #>   b_raw[2] ~ std_normal(); #>    #>   // dynamic process models #>    #>   // prior for (Intercept)_trend... #>   b_raw_trend[1] ~ normal(1, 1.5); #>    #>   // prior for speciessp_2_trend... #>   b_raw_trend[2] ~ std_normal(); #>    #>   // prior for s(time):trendtrend1_trend... #>   b_raw_trend[3 : 5] ~ multi_normal_prec(zero_trend[3 : 5], #>                                          S_trend1[1 : 3, 1 : 3] #>                                          * lambda_trend[1] #>                                          + S_trend1[1 : 3, 4 : 6] #>                                            * lambda_trend[2]); #>    #>   // prior for s(time):trendtrend2_trend... #>   b_raw_trend[6 : 8] ~ multi_normal_prec(zero_trend[6 : 8], #>                                          S_trend2[1 : 3, 1 : 3] #>                                          * lambda_trend[3] #>                                          + S_trend2[1 : 3, 4 : 6] #>                                            * lambda_trend[4]); #>   lambda_trend ~ normal(5, 30); #>   { #>     // likelihood functions #>     array[total_obs] real flat_trends; #>     array[total_obs] real flat_ps; #>     flat_trends = to_array_1d(trend); #>     flat_ps = to_array_1d(p); #>      #>     // loop over replicate sampling window (each site*time*species combination) #>     for (k in 1 : K_groups) { #>       // all log_lambdas are identical because they represent site*time #>       // covariates; so just use the first measurement #>       real log_lambda = flat_trends[K_inds[k, 1]]; #>        #>       // logit-scale detection probilities for the replicate observations #>       vector[size(K_inds[k, K_starts[k] : K_stops[k]])] logit_p = to_vector(flat_ps[K_inds[k, K_starts[k] : K_stops[k]]]); #>        #>       // K values and observed counts for these replicates #>       int K_max = N_max[k]; #>       int K_min = Y_max[k]; #>       array[size(K_inds[k, K_starts[k] : K_stops[k]])] int N_obs = flat_ys[K_inds[k, K_starts[k] : K_stops[k]]]; #>       int possible_N = K_max - K_min; #>        #>       // marginalize over possible latent counts analytically #>       real ff = exp(log_lambda) * prod(1 - inv_logit(logit_p)); #>       real prob_n = 1; #>       for (i in 1 : possible_N) { #>         real N = K_max - i + 1; #>         real k_obs = 1; #>         for (j in 1 : size(N_obs)) { #>           k_obs *= N / (N - N_obs[j]); #>         } #>         prob_n = 1 + prob_n * ff * k_obs / N; #>       } #>        #>       // add log(pr_n) to prob(K_min) #>       target += poisson_log_lpmf(K_min | log_lambda) #>                 + binomial_logit_lpmf(N_obs | K_min, logit_p) + log(prob_n); #>     } #>   } #> } #> generated quantities { #>   vector[n_lv] penalty = rep_vector(1e12, n_lv); #>   vector[total_obs] detprob = inv_logit(p); #>   vector[n_sp_trend] rho_trend = log(lambda_trend); #> } summary(mod) #> GAM observation formula: #> obs ~ species - 1 #>  #> GAM process formula: #> ~s(time, by = trend, k = 4) + species #>  #> Family: #> nmix #>  #> Link function: #> log #>  #> Trend model: #> None #>  #> N process models: #> 2  #>  #> N series: #> 10  #>  #> N timepoints: #> 6  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1500; warmup = 500; thin = 1  #> Total post-warmup draws = 4000 #>  #>  #> GAM observation model coefficient (beta) estimates: #>              2.5%   50% 97.5% Rhat n_eff #> speciessp_1 -0.34 0.710  1.40    1  1454 #> speciessp_2 -1.20 0.042  0.87    1  1813 #>  #> GAM process model coefficient (beta) estimates: #>                               2.5%     50%  97.5% Rhat n_eff #> (Intercept)_trend            2.700  3.0000  3.500    1  1348 #> speciessp_2_trend           -1.200 -0.6400  0.150    1  1491 #> s(time):trendtrend1.1_trend -0.080  0.0160  0.190    1  1195 #> s(time):trendtrend1.2_trend -0.220  0.0053  0.250    1  2612 #> s(time):trendtrend1.3_trend -0.460 -0.2500 -0.042    1  2330 #> s(time):trendtrend2.1_trend -0.210 -0.0140  0.087    1   567 #> s(time):trendtrend2.2_trend -0.200  0.0280  0.390    1  1334 #> s(time):trendtrend2.3_trend  0.056  0.3200  0.610    1  1587 #>  #> Approximate significance of GAM process smooths: #>                       edf Ref.df Chi.sq p-value #> s(time):seriestrend1 1.03      3    2.2    0.79 #> s(time):seriestrend2 1.14      3    1.1    0.86 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model loo(mod) #>  #> Computed from 4000 by 60 log-likelihood matrix. #>  #>          Estimate   SE #> elpd_loo   -237.6 14.7 #> p_loo        91.2 13.8 #> looic       475.2 29.4 #> ------ #> MCSE of elpd_loo is NA. #> MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.1]). #>  #> Pareto k diagnostic values: #>                          Count Pct.    Min. ESS #> (-Inf, 0.7]   (good)     30    50.0%   438      #>    (0.7, 1]   (bad)       4     6.7%   <NA>     #>    (1, Inf)   (very bad) 26    43.3%   <NA>     #> See help('pareto-k-diagnostic') for details. plot(mod, type = \"smooths\", trend_effects = TRUE) marginaleffects::plot_predictions(mod,   condition = \"species\",   type = \"detection\" ) +   ylab(\"Pr(detection)\") +   ylim(c(0, 1)) +   theme_classic() +   theme(legend.position = \"none\") hc <- hindcast(mod, type = \"latent_N\")  # Function to plot latent abundance estimates vs truth plot_latentN <- function(hindcasts, data, species = \"sp_1\") {   all_series <- unique(data %>%     dplyr::filter(species == !!species) %>%     dplyr::pull(series))    # Grab the first replicate that represents this series   # so we can get the true simulated values   series <- as.numeric(all_series[1])   truths <- data %>%     dplyr::arrange(time, series) %>%     dplyr::filter(series == !!levels(data$series)[series]) %>%     dplyr::pull(truth)    # In case some replicates have missing observations,   # pull out predictions for ALL replicates and average over them   hcs <- do.call(rbind, lapply(all_series, function(x) {     ind <- which(names(hindcasts$hindcasts) %in% as.character(x))     hindcasts$hindcasts[[ind]]   }))    # Calculate posterior empirical quantiles of predictions   pred_quantiles <- data.frame(t(apply(hcs, 2, function(x) {     quantile(x, probs = c(       0.05, 0.2, 0.3, 0.4,       0.5, 0.6, 0.7, 0.8, 0.95     ))   })))   pred_quantiles$time <- 1:NROW(pred_quantiles)   pred_quantiles$truth <- truths    # Grab observations   data %>%     dplyr::filter(series %in% all_series) %>%     dplyr::select(time, obs) -> observations    # Plot   ggplot(pred_quantiles, aes(x = time, group = 1)) +     geom_ribbon(aes(ymin = X5., ymax = X95.), fill = \"#DCBCBC\") +     geom_ribbon(aes(ymin = X30., ymax = X70.), fill = \"#B97C7C\") +     geom_line(aes(x = time, y = truth),       colour = \"black\", linewidth = 1     ) +     geom_point(aes(x = time, y = truth),       shape = 21, colour = \"white\", fill = \"black\",       size = 2.5     ) +     geom_jitter(       data = observations, aes(x = time, y = obs),       width = 0.06,       shape = 21, fill = \"darkred\", colour = \"white\", size = 2.5     ) +     labs(       y = \"Latent abundance (N)\",       x = \"Time\",       title = species     ) } plot_latentN(hc, testdat, species = \"sp_1\") plot_latentN(hc, testdat, species = \"sp_2\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/nmixtures.html","id":"example-2-a-larger-survey-with-possible-nonlinear-effects","dir":"Articles","previous_headings":"","what":"Example 2: a larger survey with possible nonlinear effects","title":"N-mixtures in mvgam","text":"Now another example larger dataset. use data Jeff Doser’s simulation example wonderful spAbundance package. simulated data include one continuous site-level covariate, one factor site-level covariate two continuous sample-level covariates. example allow us examine can include possibly nonlinear effects latent process detection probability models. Download data grab observations / covariate measurements one species Next wrangle appropriate ‘long’ data format, adding indicators time series working mvgam. also add cap variable represent maximum latent N marginalize observation data include observations 225 sites three replicates per site, though observations missing final step data preparation course trend_map, sets mapping observation replicates latent abundance models. done way example Now ready fit model using mvgam(). use penalized splines continuous covariate effects detect possible nonlinear associations. also showcase mvgam can make use different approximation algorithms available Stan using meanfield variational Bayes approximator (reduces computation time around 90 seconds around 12 seconds example) Inspect model summary don’t bother looking estimates individual spline coefficients. Notice longer receive information convergence use MCMC sampling model can make use marginaleffects support interrogating model targeted predictions. First, can inspect estimated average detection probability Next investigate estimated effects covariates latent abundance using conditional_effects() function specifying type = 'link'; return plots expectation scale effect continuous covariate expected latent abundance  effect factor covariate expected latent abundance, estimated hierarchical random effect  Now can investigate estimated effects covariates detection probability using type = 'detection' covariate smooths estimated somewhat nonlinear logit scale according model summary (based approximate significances). inspecting conditional effects covariate probability scale intuitive useful   targeted predictions also easy marginaleffects support. example, can ask: detection probability change change detection covariates?  model found support important covariate effects, course ’d want interrogate well model predicts think possible spatial effects capture unmodelled variation latent abundance (can easily incorporated linear predictors using spatial smooths).","code":"# Date link load(url(\"https://github.com/doserjef/spAbundance/raw/main/data/dataNMixSim.rda\")) data.one.sp <- dataNMixSim  # Pull out observations for one species data.one.sp$y <- data.one.sp$y[1, , ]  # Abundance covariates that don't change across repeat sampling observations abund.cov <- dataNMixSim$abund.covs[, 1] abund.factor <- as.factor(dataNMixSim$abund.covs[, 2])  # Detection covariates that can change across repeat sampling observations # Note that `NA`s are not allowed for covariates in mvgam, so we randomly # impute them here det.cov <- dataNMixSim$det.covs$det.cov.1[, ] det.cov[is.na(det.cov)] <- rnorm(length(which(is.na(det.cov)))) det.cov2 <- dataNMixSim$det.covs$det.cov.2 det.cov2[is.na(det.cov2)] <- rnorm(length(which(is.na(det.cov2)))) mod_data <- do.call(   rbind,   lapply(1:NROW(data.one.sp$y), function(x) {     data.frame(       y = data.one.sp$y[x, ],       abund_cov = abund.cov[x],       abund_fac = abund.factor[x],       det_cov = det.cov[x, ],       det_cov2 = det.cov2[x, ],       replicate = 1:NCOL(data.one.sp$y),       site = paste0(\"site\", x)     )   }) ) %>%   dplyr::mutate(     species = \"sp_1\",     series = as.factor(paste0(site, \"_\", species, \"_\", replicate))   ) %>%   dplyr::mutate(     site = factor(site, levels = unique(site)),     species = factor(species, levels = unique(species)),     time = 1,     cap = max(data.one.sp$y, na.rm = TRUE) + 20   ) NROW(mod_data) #> [1] 675 dplyr::glimpse(mod_data) #> Rows: 675 #> Columns: 11 #> $ y         <int> 1, NA, NA, NA, 2, 2, NA, 1, NA, NA, 0, 1, 0, 0, 0, 0, NA, NA… #> $ abund_cov <dbl> -0.3734384, -0.3734384, -0.3734384, 0.7064305, 0.7064305, 0.… #> $ abund_fac <fct> 3, 3, 3, 4, 4, 4, 9, 9, 9, 2, 2, 2, 3, 3, 3, 2, 2, 2, 1, 1, … #> $ det_cov   <dbl> -1.28279990, 0.36376941, 0.01343699, 0.81359850, 0.19548086,… #> $ det_cov2  <dbl> 2.03047314, 1.37976746, 0.44161953, -0.29263182, 1.04555361,… #> $ replicate <int> 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, … #> $ site      <fct> site1, site1, site1, site2, site2, site2, site3, site3, site… #> $ species   <fct> sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, sp_1, … #> $ series    <fct> site1_sp_1_1, site1_sp_1_2, site1_sp_1_3, site2_sp_1_1, site… #> $ time      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #> $ cap       <dbl> 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, … head(mod_data) #>    y  abund_cov abund_fac     det_cov   det_cov2 replicate  site species #> 1  1 -0.3734384         3 -1.28279990  2.0304731         1 site1    sp_1 #> 2 NA -0.3734384         3  0.36376941  1.3797675         2 site1    sp_1 #> 3 NA -0.3734384         3  0.01343699  0.4416195         3 site1    sp_1 #> 4 NA  0.7064305         4  0.81359850 -0.2926318         1 site2    sp_1 #> 5  2  0.7064305         4  0.19548086  1.0455536         2 site2    sp_1 #> 6  2  0.7064305         4  0.96730338  1.9197118         3 site2    sp_1 #>         series time cap #> 1 site1_sp_1_1    1  33 #> 2 site1_sp_1_2    1  33 #> 3 site1_sp_1_3    1  33 #> 4 site2_sp_1_1    1  33 #> 5 site2_sp_1_2    1  33 #> 6 site2_sp_1_3    1  33 mod_data %>%   # each unique combination of site*species is a separate process   dplyr::mutate(trend = as.numeric(factor(paste0(site, species)))) %>%   dplyr::select(trend, series) %>%   dplyr::distinct() -> trend_map  trend_map %>%   dplyr::arrange(trend) %>%   head(12) #>    trend         series #> 1      1 site100_sp_1_1 #> 2      1 site100_sp_1_2 #> 3      1 site100_sp_1_3 #> 4      2 site101_sp_1_1 #> 5      2 site101_sp_1_2 #> 6      2 site101_sp_1_3 #> 7      3 site102_sp_1_1 #> 8      3 site102_sp_1_2 #> 9      3 site102_sp_1_3 #> 10     4 site103_sp_1_1 #> 11     4 site103_sp_1_2 #> 12     4 site103_sp_1_3 mod <- mvgam(   # effects of covariates on detection probability;   # here we use penalized splines for both continuous covariates   formula = y ~ s(det_cov, k = 4) + s(det_cov2, k = 4),    # effects of the covariates on latent abundance;   # here we use a penalized spline for the continuous covariate and   # hierarchical intercepts for the factor covariate   trend_formula = ~ s(abund_cov, k = 4) +     s(abund_fac, bs = \"re\"),    # link multiple observations to each site   trend_map = trend_map,    # nmix() family and supplied data   family = nmix(),   data = mod_data,    # standard normal priors on key regression parameters   priors = c(     prior(std_normal(), class = \"b\"),     prior(std_normal(), class = \"Intercept\"),     prior(std_normal(), class = \"Intercept_trend\"),     prior(std_normal(), class = \"sigma_raw_trend\")   ),    # use Stan's variational inference for quicker results   algorithm = \"meanfield\",    # no need to compute \"series-level\" residuals   residuals = FALSE,   samples = 1000 ) summary(mod, include_betas = FALSE) #> GAM observation formula: #> y ~ s(det_cov, k = 3) + s(det_cov2, k = 3) #>  #> GAM process formula: #> ~s(abund_cov, k = 3) + s(abund_fac, bs = \"re\") #>  #> Family: #> nmix #>  #> Link function: #> log #>  #> Trend model: #> None #>  #> N process models: #> 225  #>  #> N series: #> 675  #>  #> N timepoints: #> 1  #>  #> Status: #> Fitted using Stan  #> 1 chains, each with iter = 1000; warmup = ; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM observation model coefficient (beta) estimates: #>             2.5%  50% 97.5% Rhat n.eff #> (Intercept) 0.11 0.45  0.77  NaN   NaN #>  #> Approximate significance of GAM observation smooths: #>              edf Ref.df Chi.sq p-value     #> s(det_cov)  1.01      2    172 2.4e-08 *** #> s(det_cov2) 1.03      2    546 6.7e-07 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> GAM process model coefficient (beta) estimates: #>                    2.5%  50% 97.5% Rhat n.eff #> (Intercept)_trend 0.035 0.17   0.3  NaN   NaN #>  #> GAM process model group-level estimates: #>                           2.5%   50% 97.5% Rhat n.eff #> mean(s(abund_fac))_trend -0.53 -0.33 -0.13  NaN   NaN #> sd(s(abund_fac))_trend    0.26  0.39  0.60  NaN   NaN #>  #> Approximate significance of GAM process smooths: #>               edf Ref.df Chi.sq p-value   #> s(abund_cov) 1.01      2   3.17   0.070 . #> s(abund_fac) 8.75     10  17.59   0.036 * #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Posterior approximation used: no diagnostics to compute #>  #> Use how_to_cite() to get started describing this model marginaleffects::avg_predictions(mod, type = \"detection\") #>  #>  Estimate 2.5 % 97.5 % #>     0.591 0.522  0.655 #>  #> Type:  detection abund_plots <- plot(   conditional_effects(mod,     type = \"link\",     effects = c(       \"abund_cov\",       \"abund_fac\"     )   ),   plot = FALSE ) abund_plots[[1]] +   ylab(\"Expected latent abundance\") abund_plots[[2]] +   ylab(\"Expected latent abundance\") det_plots <- plot(   conditional_effects(mod,     type = \"detection\",     effects = c(       \"det_cov\",       \"det_cov2\"     )   ),   plot = FALSE ) det_plots[[1]] +   ylab(\"Pr(detection)\") det_plots[[2]] +   ylab(\"Pr(detection)\") fivenum_round <- function(x) round(fivenum(x, na.rm = TRUE), 2)  marginaleffects::plot_predictions(mod,   newdata = marginaleffects::datagrid(     det_cov = unique,     det_cov2 = fivenum_round   ),   by = c(\"det_cov\", \"det_cov2\"),   type = \"detection\" ) +   theme_classic() +   ylab(\"Pr(detection)\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/nmixtures.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"N-mixtures in mvgam","text":"following papers resources offer useful material N-mixture models ecological population dynamics investigations: Guélat, Jérôme, Kéry, Marc. “Effects Spatial Autocorrelation Imperfect Detection Species Distribution Models.” Methods Ecology Evolution 9 (2018): 1614–25. Kéry, Marc, Royle Andrew J. “Applied hierarchical modeling ecology: Analysis distribution, abundance species richness R BUGS: Volume 2: Dynamic advanced models”. London, UK: Academic Press (2020). Royle, Andrew J. “N‐mixture models estimating population size spatially replicated counts.” Biometrics 60.1 (2004): 108-115.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/nmixtures.html","id":"interested-in-contributing","dir":"Articles","previous_headings":"","what":"Interested in contributing?","title":"N-mixtures in mvgam","text":"’m actively seeking PhD students researchers work areas ecological forecasting, multivariate model evaluation development mvgam. Please see small list opportunities website reach interested (n.clark’’uq.edu.au)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"the-trend_map-argument","dir":"Articles","previous_headings":"","what":"The trend_map argument","title":"Shared latent states in mvgam","text":"trend_map argument mvgam() function optional data.frame can used specify series depend latent process models (called “trends” mvgam). can particularly useful wish force multiple observed time series depend latent trend process, different observation processes. argument supplied, latent factor model set setting use_lv = TRUE using supplied trend_map set shared trends. Users familiar MARSS family packages recognize way specifying \\(Z\\) matrix. data.frame needs column names series trend, integer values trend column state trend series depend . series column single unique entry time series data, names perfectly match factor levels series variable data). example, simulate collection three integer-valued time series (using sim_mvgam), following trend_map force first two series share latent trend process: can see factor levels trend_map match data:","code":"set.seed(122) simdat <- sim_mvgam(   trend_model = AR(),   prop_trend = 0.6,   mu = c(0, 1, 2),   family = poisson() ) trend_map <- data.frame(   series = unique(simdat$data_train$series),   trend = c(1, 1, 2) ) trend_map #>     series trend #> 1 series_1     1 #> 2 series_2     1 #> 3 series_3     2 all.equal(levels(trend_map$series),            levels(simdat$data_train$series)) #> [1] TRUE"},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"checking-trend_map-with-run_model-false","dir":"Articles","previous_headings":"The trend_map argument","what":"Checking trend_map with run_model = FALSE","title":"Shared latent states in mvgam","text":"Supplying trend_map mvgam function simple model, setting run_model = FALSE, allows us inspect constructed Stan code data objects used condition model. set model series different observation process (different intercept per series case), two latent dynamic process models evolve independent AR1 processes also contain shared nonlinear smooth function capture repeated seasonality. model complicated show can learn shared independent effects collections time series mvgam framework: Inspecting Stan code shows model dynamic factor model loadings constructed reflect supplied trend_map: Notice line states “lv_coefs = Z;”. uses supplied \\(Z\\) matrix construct loading coefficients. supplied matrix now looks exactly like ’d use create similar model MARSS package:","code":"fake_mod <- mvgam(   y ~     # observation model formula, which has a     # different intercept per series     series - 1,    # process model formula, which has a shared seasonal smooth   # (each latent process model shares the SAME smooth)   trend_formula = ~ s(season, bs = \"cc\", k = 6),    # AR1 dynamics (each latent process model has DIFFERENT)   # dynamics; processes are estimated using the noncentred   # parameterisation for improved efficiency   trend_model = AR(),   noncentred = TRUE,    # supplied trend_map   trend_map = trend_map,    # data and observation family   family = poisson(),   data = simdat$data_train,   run_model = FALSE ) stancode(fake_mod) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp_trend; // number of trend smoothing parameters #>   int<lower=0> n_lv; // number of dynamic factors #>   int<lower=0> n_series; // number of series #>   matrix[n_series, n_lv] Z; // matrix mapping series to latent states #>   int<lower=0> num_basis; // total number of basis coefficients #>   int<lower=0> num_basis_trend; // number of trend basis coefficients #>   vector[num_basis_trend] zero_trend; // prior locations for trend basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   matrix[n * n_lv, num_basis_trend] X_trend; // trend model design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   array[n, n_lv] int ytimes_trend; #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   matrix[4, 4] S_trend1; // mgcv smooth penalty matrix S_trend1 #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> transformed data { #>    #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>   vector[num_basis_trend] b_raw_trend; #>    #>   // latent state SD terms #>   vector<lower=0>[n_lv] sigma; #>    #>   // latent state AR1 terms #>   vector<lower=-1, upper=1>[n_lv] ar1; #>    #>   // raw latent states #>   matrix[n, n_lv] LV_raw; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp_trend] lambda_trend; #> } #> transformed parameters { #>   // raw latent states #>   vector[n * n_lv] trend_mus; #>   matrix[n, n_series] trend; #>    #>   // basis coefficients #>   vector[num_basis] b; #>    #>   // latent states #>   matrix[n, n_lv] LV; #>   vector[num_basis_trend] b_trend; #>    #>   // observation model basis coefficients #>   b[1 : num_basis] = b_raw[1 : num_basis]; #>    #>   // process model basis coefficients #>   b_trend[1 : num_basis_trend] = b_raw_trend[1 : num_basis_trend]; #>    #>   // latent process linear predictors #>   trend_mus = X_trend * b_trend; #>   LV = LV_raw .* rep_matrix(sigma', rows(LV_raw)); #>   for (j in 1 : n_lv) { #>     LV[1, j] += trend_mus[ytimes_trend[1, j]]; #>     for (i in 2 : n) { #>       LV[i, j] += trend_mus[ytimes_trend[i, j]] #>                   + ar1[j] * (LV[i - 1, j] - trend_mus[ytimes_trend[i - 1, j]]); #>     } #>   } #>    #>   // derived latent states #>   for (i in 1 : n) { #>     for (s in 1 : n_series) { #>       trend[i, s] = dot_product(Z[s,  : ], LV[i,  : ]); #>     } #>   } #> } #> model { #>   // prior for seriesseries_1... #>   b_raw[1] ~ student_t(3, 0, 2); #>    #>   // prior for seriesseries_2... #>   b_raw[2] ~ student_t(3, 0, 2); #>    #>   // prior for seriesseries_3... #>   b_raw[3] ~ student_t(3, 0, 2); #>    #>   // priors for AR parameters #>   ar1 ~ std_normal(); #>    #>   // priors for latent state SD parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>   to_vector(LV_raw) ~ std_normal(); #>    #>   // dynamic process models #>    #>   // prior for (Intercept)_trend... #>   b_raw_trend[1] ~ student_t(3, 0, 2); #>    #>   // prior for s(season)_trend... #>   b_raw_trend[2 : 5] ~ multi_normal_prec(zero_trend[2 : 5], #>                                          S_trend1[1 : 4, 1 : 4] #>                                          * lambda_trend[1]); #>   lambda_trend ~ normal(5, 30); #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_ys ~ poisson_log_glm(append_col(flat_xs, flat_trends), 0.0, #>                               append_row(b, 1.0)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp_trend] rho_trend; #>   vector[n_lv] penalty; #>   array[n, n_series] int ypred; #>   penalty = 1.0 / (sigma .* sigma); #>   rho_trend = log(lambda_trend); #>    #>   matrix[n_series, n_lv] lv_coefs = Z; #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> } fake_mod$model_data$Z #>      [,1] [,2] #> [1,]    1    0 #> [2,]    1    0 #> [3,]    0    1"},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"fitting-and-inspecting-the-model","dir":"Articles","previous_headings":"The trend_map argument","what":"Fitting and inspecting the model","title":"Shared latent states in mvgam","text":"Though model doesn’t perfectly match data-generating process (allowed series different underlying dynamics), can still fit show resulting inferences look like: summary model informative shows two latent process models estimated, even though three observed time series. model converges well series 1 2 share exact latent process estimates, estimates series 3 different:    However, forecasts series’ 1 2 differ different intercepts observation model","code":"full_mod <- mvgam(   y ~ series - 1,   trend_formula = ~ s(season, bs = \"cc\", k = 6),   trend_model = AR(),   noncentred = TRUE,   trend_map = trend_map,   family = poisson(),   data = simdat$data_train,   silent = 2 ) summary(full_mod) #> GAM observation formula: #> y ~ series - 1 #>  #> GAM process formula: #> ~s(season, bs = \"cc\", k = 6) #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> AR() #>  #>  #> N process models: #> 2  #>  #> N series: #> 3  #>  #> N timepoints: #> 75  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> GAM observation model coefficient (beta) estimates: #>                 2.5%   50% 97.5% Rhat n_eff #> seriesseries_1 -2.80 -0.74   1.3    1   638 #> seriesseries_2 -1.80  0.21   2.2    1   628 #> seriesseries_3 -0.81  1.20   3.2    1   640 #>  #> Process model AR parameter estimates: #>         2.5%    50% 97.5% Rhat n_eff #> ar1[1] -0.59 -0.240  0.15 1.01   623 #> ar1[2] -0.30  0.025  0.33 1.01   303 #>  #> Process error parameter estimates: #>          2.5%  50% 97.5% Rhat n_eff #> sigma[1] 0.37 0.53  0.72 1.00   749 #> sigma[2] 0.48 0.62  0.78 1.01   731 #>  #> GAM process model coefficient (beta) estimates: #>                     2.5%    50% 97.5% Rhat n_eff #> (Intercept)_trend -1.200  0.830  2.80 1.00   625 #> s(season).1_trend -0.310 -0.052  0.20 1.00   819 #> s(season).2_trend -0.053  0.240  0.50 1.00   848 #> s(season).3_trend -0.500 -0.200  0.10 1.01   927 #> s(season).4_trend  0.380  0.700  0.98 1.01   485 #>  #> Approximate significance of GAM process smooths: #>            edf Ref.df Chi.sq p-value     #> s(season) 3.09      4   16.4 8.7e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model plot(full_mod, type = \"trend\", series = 1) plot(full_mod, type = \"trend\", series = 2) plot(full_mod, type = \"trend\", series = 3)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"example-signal-detection","dir":"Articles","previous_headings":"","what":"Example: signal detection","title":"Shared latent states in mvgam","text":"Now explore complicated example. simulate true hidden signal trying track. signal depends nonlinearly covariate (called productivity, represents measure productive landscape ). signal also demonstrates fairly large amount temporal autocorrelation: Plot signal inspect ’s evolution time  Next simulate three sensors trying track hidden signal. sensors different observation errors can depend nonlinearly second external covariate, called temperature example. makes use gamSim Plot sensor observations  now plot observed relationships three sensors temperature covariate","code":"set.seed(123) # simulate a nonlinear relationship using the mgcv function gamSim signal_dat <- mgcv::gamSim(n = 100, eg = 1, scale = 1) #> Gu & Wahba 4 term additive model  # productivity is one of the variables in the simulated data productivity <- signal_dat$x2  # simulate the true signal, which already has a nonlinear relationship # with productivity; we will add in a fairly strong AR1 process to # contribute to the signal true_signal <- as.vector(scale(signal_dat$y) +   arima.sim(100, model = list(ar = 0.8, sd = 0.1))) plot(   true_signal,   type = \"l\",   bty = \"l\", lwd = 2,   ylab = \"True signal\",   xlab = \"Time\" ) # Function to simulate a monotonic response to a covariate sim_monotonic <- function(x, a = 2.2, b = 2) {   out <- exp(a * x) / (6 + exp(b * x)) * -1   return(2.5 * as.vector(scale(out))) }  # Simulated temperature covariate temperature <- runif(100, -2, 2)  # Simulate the three series sim_series <- function(n_series = 3, true_signal) {   temp_effects <- mgcv::gamSim(n = 100, eg = 7, scale = 0.05)   alphas <- rnorm(n_series, sd = 2)    do.call(rbind, lapply(seq_len(n_series), function(series) {     data.frame(       observed = rnorm(length(true_signal),         mean = alphas[series] +           sim_monotonic(temperature,                              runif(1, 2.2, 3),                             runif(1, 2.2, 3)) +           true_signal,         sd = runif(1, 1, 2)       ),       series = paste0(\"sensor_\", series),       time = 1:length(true_signal),       temperature = temperature,       productivity = productivity,       true_signal = true_signal     )   })) } model_dat <- sim_series(true_signal = true_signal) %>%   dplyr::mutate(series = factor(series)) #> Gu & Wahba 4 term additive model, correlated predictors plot_mvgam_series(   data = model_dat, y = \"observed\",   series = \"all\" ) plot(   observed ~ temperature,   data = model_dat %>%     dplyr::filter(series == \"sensor_1\"),   pch = 16, bty = \"l\",   ylab = \"Sensor 1\",   xlab = \"Temperature\" ) plot(   observed ~ temperature,   data = model_dat %>%     dplyr::filter(series == \"sensor_2\"),   pch = 16, bty = \"l\",   ylab = \"Sensor 2\",   xlab = \"Temperature\" ) plot(   observed ~ temperature,   data = model_dat %>%     dplyr::filter(series == \"sensor_3\"),   pch = 16, bty = \"l\",   ylab = \"Sensor 3\",   xlab = \"Temperature\" )"},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"the-shared-signal-model","dir":"Articles","previous_headings":"Example: signal detection","what":"The shared signal model","title":"Shared latent states in mvgam","text":"Now can formulate fit model allows sensor’s observation error depend nonlinearly temperature allowing true signal depend nonlinearly productivity. fixing values trend column 1 trend_map, assuming observation sensors tracking latent signal. use informative priors two variance components (process error observation error), reflect prior belief observation error smaller overall true process error View reduced version model summary many spline coefficients model","code":"mod <- mvgam(   formula =   # formula for observations, allowing for different   # intercepts and hierarchical smooth effects of temperature     observed ~ series +       s(temperature, k = 10) +       s(series, temperature, bs = \"sz\", k = 8),   trend_formula =   # formula for the latent signal, which can depend   # nonlinearly on productivity     ~ s(productivity, k = 8) - 1,   trend_model =   # in addition to productivity effects, the signal is   # assumed to exhibit temporal autocorrelation     AR(),   noncentred = TRUE,   trend_map =   # trend_map forces all sensors to track the same   # latent signal     data.frame(       series = unique(model_dat$series),       trend = c(1, 1, 1)     ),    # informative priors on process error   # and observation error will help with convergence   priors = c(     prior(normal(2, 0.5), class = sigma),     prior(normal(1, 0.5), class = sigma_obs)   ),    # Gaussian observations   family = gaussian(),   data = model_dat,   silent = 2 ) summary(mod, include_betas = FALSE) #> GAM observation formula: #> observed ~ series + s(temperature, k = 10) + s(series, temperature,  #>     bs = \"sz\", k = 8) #>  #> GAM process formula: #> ~s(productivity, k = 8) - 1 #>  #> Family: #> gaussian #>  #> Link function: #> identity #>  #> Trend model: #> AR() #>  #>  #> N process models: #> 1  #>  #> N series: #> 3  #>  #> N timepoints: #> 100  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1100; warmup = 600; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> Observation error parameter estimates: #>              2.5% 50% 97.5% Rhat n_eff #> sigma_obs[1]  1.3 1.5   1.8    1  2413 #> sigma_obs[2]  1.1 1.4   1.6    1  2315 #> sigma_obs[3]  1.3 1.5   1.7    1  1805 #>  #> GAM observation model coefficient (beta) estimates: #>                 2.5%   50% 97.5% Rhat n_eff #> (Intercept)     0.11  1.20  3.60    1   539 #> seriessensor_2 -2.40 -1.60 -0.72    1  1133 #> seriessensor_3 -0.55  0.51  1.60    1  1387 #>  #> Approximate significance of GAM observation smooths: #>                        edf Ref.df  Chi.sq p-value     #> s(temperature)        3.94      9 1439.12  <2e-16 *** #> s(series,temperature) 2.40     16    1.17       1     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Process model AR parameter estimates: #>        2.5%  50% 97.5% Rhat n_eff #> ar1[1] 0.58 0.78  0.96    1   491 #>  #> Process error parameter estimates: #>          2.5% 50% 97.5% Rhat n_eff #> sigma[1] 0.89 1.2   1.5    1   532 #>  #> Approximate significance of GAM process smooths: #>                  edf Ref.df Chi.sq p-value   #> s(productivity) 2.55      7   32.2   0.054 . #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model"},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"inspecting-effects-on-both-process-and-observation-models","dir":"Articles","previous_headings":"Example: signal detection","what":"Inspecting effects on both process and observation models","title":"Shared latent states in mvgam","text":"Don’t pay much attention approximate p-values smooth terms. calculation values incredibly sensitive estimates smoothing parameters don’t tend find meaningful. meaningful, however, prediction-based plots smooth functions. main effects can quickly plotted conditional_effects:  conditional_effects simply wrapper flexible plot_predictions function marginaleffects package. can get useful plots effects using function customisation:  successfully estimated effects, nonlinear, impact hidden process observations. single joint model. can always challenges models, particularly estimating process observation error time.","code":"conditional_effects(mod, type = \"link\") plot_predictions(   mod,   condition = c(\"temperature\", \"series\", \"series\"),   points = 0.5 ) +   theme(legend.position = \"none\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"recovering-the-hidden-signal","dir":"Articles","previous_headings":"Example: signal detection","what":"Recovering the hidden signal","title":"Shared latent states in mvgam","text":"final key question whether can successfully recover true hidden signal. trend slot returned model parameters estimates signal, can easily plot using mvgam S3 method plot. can also overlay true values hidden signal, shows model done good job recovering :","code":"plot(mod,       type = \"trend\") +   ggplot2::geom_point(data = data.frame(time = 1:100,                                         y = true_signal),                       mapping = ggplot2::aes(x = time,                                              y = y))"},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"Shared latent states in mvgam","text":"following papers resources offer lot useful material types State-Space models can applied practice: Auger‐Méthé, Marie, et al. “guide state–space modeling ecological time series.” Ecological Monographs 91.4 (2021): e01470. Clark, Nicholas J., et al. Beyond single-species models: leveraging multispecies forecasts navigate dynamics ecological predictability. PeerJ. (2025): 13:e18929 Holmes, Elizabeth E., Eric J. Ward, Wills Kellie. “MARSS: multivariate autoregressive state-space models analyzing time-series data.” R Journal. 4.1 (2012): 11. Ward, Eric J., et al. “Inferring spatial structure time‐series data: using multivariate state‐space models detect metapopulation structure California sea lions Gulf California, Mexico.” Journal Applied Ecology 47.1 (2010): 47-56.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/shared_states.html","id":"interested-in-contributing","dir":"Articles","previous_headings":"","what":"Interested in contributing?","title":"Shared latent states in mvgam","text":"’m actively seeking PhD students researchers work areas ecological forecasting, multivariate model evaluation development mvgam. Please reach interested (n.clark’’uq.edu.au)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"time-varying-effects","dir":"Articles","previous_headings":"","what":"Time-varying effects","title":"Time-varying effects in mvgam","text":"Dynamic fixed-effect coefficients (often referred dynamic linear models) can readily incorporated GAMs / DGAMs. mvgam, dynamic() formula wrapper offers convenient interface set . plan incorporate range dynamic options (random walk, AR1 etc…) moment low-rank Gaussian Process (GP) smooths allowed (making use either gp basis mgcv Hilbert space approximate GPs). advantageous splines random walk effects several reasons. First, GPs force time-varying effect smooth. often makes sense reality, expect regression coefficient change rapidly one time point next. Second, GPs provide information ‘global’ dynamics time-varying effect length-scale parameters. means can use provide accurate forecasts effect expected change future, something couldn’t well used splines estimate effect. example illustrates.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"simulating-time-varying-effects","dir":"Articles","previous_headings":"Time-varying effects","what":"Simulating time-varying effects","title":"Time-varying effects in mvgam","text":"Simulate time-varying coefficient using squared exponential Gaussian Process function length scale \\(\\rho\\)=10. using internal function mvgam (sim_gp function): plot time-varying coefficient shows changes smoothly time:  Next need simulate values covariate, call temp (represent \\(temperature\\)). case just use standard normal distribution simulate covariate: Finally, simulate outcome variable, Gaussian observation process (observation error) time-varying effect \\(temperature\\)  Gather data data.frame fitting models, split data training testing folds.","code":"set.seed(1111) N <- 200 beta_temp <- mvgam:::sim_gp(rnorm(1),   alpha_gp = 0.75,   rho_gp = 10,   h = N ) + 0.5 plot(beta_temp,   type = \"l\", lwd = 3,   bty = \"l\", xlab = \"Time\", ylab = \"Coefficient\",   col = \"darkred\" ) box(bty = \"l\", lwd = 2) temp <- rnorm(N, sd = 1) out <- rnorm(N,   mean = 4 + beta_temp * temp,   sd = 0.25 ) time <- seq_along(temp) plot(out,   type = \"l\", lwd = 3,   bty = \"l\", xlab = \"Time\", ylab = \"Outcome\",   col = \"darkred\" ) box(bty = \"l\", lwd = 2) data <- data.frame(out, temp, time) data_train <- data[1:190, ] data_test <- data[191:200, ]"},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"the-dynamic-function","dir":"Articles","previous_headings":"Time-varying effects","what":"The dynamic() function","title":"Time-varying effects in mvgam","text":"Time-varying coefficients can fairly easily set using s() gp() wrapper functions mvgam formulae fitting nonlinear effect time using covariate interest numeric variable (see ?mgcv::s ?brms::gp details). dynamic() formula wrapper offers way automate process, eventually allow broader variety time-varying effects (random walk AR processes). Depending arguments specified dynamic, either set low-rank GP smooth function using s() bs = 'gp' fixed value length scale parameter \\(\\rho\\), set Hilbert space approximate GP using gp() function c=5/4 \\(\\rho\\) estimated (see ?dynamic details). first example use s() option, mis-specify \\(\\rho\\) parameter , practice, never known. call dynamic() set following smooth: s(time, = temp, bs = \"gp\", m = c(-2, 8, 2), k = 40) Inspect model summary, shows dynamic() wrapper used construct low-rank Gaussian Process smooth function: model used spline gp basis, ’s smooths can visualised just like gam. can plot estimates -sample --sample periods see Gaussian Process function produces sensible smooth forecasts. supply full dataset newdata argument plot_mvgam_smooth() inspect posterior forecasts time-varying smooth function. Overlay true simulated function see model adequately estimated ’s dynamics training testing data partitions  can also use plot_predictions() marginaleffects package visualise time-varying coefficient effect estimated different values \\(temperature\\):  results sensible forecasts observations well  syntax similar wish estimate parameters underlying Gaussian Process, time using Hilbert space approximation. simply omit rho argument dynamic() make happen. set call similar gp(time, = 'temp', c = 5/4, k = 40). model summary now contains estimates marginal deviation length scale parameters underlying Gaussian Process function: Effects gp() terms can also plotted smooths:","code":"mod <- mvgam(out ~ dynamic(temp, rho = 8, stationary = TRUE, k = 40),   family = gaussian(),   data = data_train,   silent = 2 ) summary(mod, include_betas = FALSE) #> GAM formula: #> out ~ s(time, by = temp, bs = \"gp\", m = c(-2, 8, 2), k = 40) #>  #> Family: #> gaussian #>  #> Link function: #> identity #>  #> Trend model: #> None #>  #> N series: #> 1  #>  #> N timepoints: #> 190  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> Observation error parameter estimates: #>              2.5%  50% 97.5% Rhat n_eff #> sigma_obs[1] 0.23 0.25  0.28    1  2854 #>  #> GAM coefficient (beta) estimates: #>             2.5% 50% 97.5% Rhat n_eff #> (Intercept)    4   4   4.1    1  2580 #>  #> Approximate significance of GAM smooths: #>               edf Ref.df Chi.sq p-value     #> s(time):temp 16.3     40    171  <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model plot_mvgam_smooth(mod, smooth = 1, newdata = data) abline(v = 190, lty = \"dashed\", lwd = 2) lines(beta_temp, lwd = 2.5, col = \"white\") lines(beta_temp, lwd = 2) require(marginaleffects) range_round <- function(x) {   round(range(x, na.rm = TRUE), 2) } plot_predictions(mod,   newdata = datagrid(     time = unique,     temp = range_round   ),   by = c(\"time\", \"temp\", \"temp\"),   type = \"link\" ) fc <- forecast(mod, newdata = data_test) plot(fc) mod <- mvgam(out ~ dynamic(temp, k = 40),   family = gaussian(),   data = data_train,   silent = 2 ) summary(mod, include_betas = FALSE) #> GAM formula: #> out ~ gp(time, by = temp, c = 5/4, k = 40, scale = TRUE) #>  #> Family: #> gaussian #>  #> Link function: #> identity #>  #> Trend model: #> None #>  #> N series: #> 1  #>  #> N timepoints: #> 190  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> Observation error parameter estimates: #>              2.5%  50% 97.5% Rhat n_eff #> sigma_obs[1] 0.24 0.26   0.3    1  2916 #>  #> GAM coefficient (beta) estimates: #>             2.5% 50% 97.5% Rhat n_eff #> (Intercept)    4   4   4.1    1  4300 #>  #> GAM gp term marginal deviation (alpha) and length scale (rho) estimates: #>                     2.5%   50% 97.5% Rhat n_eff #> alpha_gp(time):temp 0.63 0.900 1.400    1   749 #> rho_gp(time):temp   0.02 0.051 0.069    1   661 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model plot_mvgam_smooth(mod, smooth = 1, newdata = data) abline(v = 190, lty = \"dashed\", lwd = 2) lines(beta_temp, lwd = 2.5, col = \"white\") lines(beta_temp, lwd = 2)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"salmon-survival-example","dir":"Articles","previous_headings":"","what":"Salmon survival example","title":"Time-varying effects in mvgam","text":"use openly available data marine survival Chinook salmon illustrate time-varying effects can used improve ecological time series models. Scheuerell Williams (2005) used dynamic linear model examine relationship marine survival Chinook salmon index ocean upwelling strength along west coast USA. authors hypothesized stronger upwelling April create better growing conditions phytoplankton, translate zooplankton provide better foraging opportunities juvenile salmon entering ocean. data survival measured proportional variable 42 years (1964–2005) available MARSS package: First need prepare data modelling. variable CUI.apr standardized make easier sampler estimate underlying GP parameters time-varying effect. also need convert survival back proportion, current form logit-transformed (time series packages handle proportional data). usual, also need create time indicator series indicator working mvgam: Inspect data Plot features outcome variable, shows proportional variable particular restrictions want model:","code":"load(url(\"https://github.com/atsa-es/MARSS/raw/master/data/SalmonSurvCUI.rda\")) dplyr::glimpse(SalmonSurvCUI) #> Rows: 42 #> Columns: 3 #> $ year    <int> 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 19… #> $ logit.s <dbl> -3.46, -3.32, -3.58, -3.03, -3.61, -3.35, -3.93, -4.19, -4.82,… #> $ CUI.apr <int> 57, 5, 43, 11, 47, -21, 25, -2, -1, 43, 2, 35, 0, 1, -1, 6, -7… SalmonSurvCUI %>%   # create a time variable   dplyr::mutate(time = dplyr::row_number()) %>%   # create a series variable   dplyr::mutate(series = as.factor(\"salmon\")) %>%   # z-score the covariate CUI.apr   dplyr::mutate(CUI.apr = as.vector(scale(CUI.apr))) %>%   # convert logit-transformed survival back to proportional   dplyr::mutate(survival = plogis(logit.s)) -> model_data dplyr::glimpse(model_data) #> Rows: 42 #> Columns: 6 #> $ year     <int> 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1… #> $ logit.s  <dbl> -3.46, -3.32, -3.58, -3.03, -3.61, -3.35, -3.93, -4.19, -4.82… #> $ CUI.apr  <dbl> 2.37949804, 0.03330223, 1.74782994, 0.30401713, 1.92830654, -… #> $ time     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18… #> $ series   <fct> salmon, salmon, salmon, salmon, salmon, salmon, salmon, salmo… #> $ survival <dbl> 0.030472033, 0.034891409, 0.027119717, 0.046088827, 0.0263393… plot_mvgam_series(data = model_data, y = \"survival\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"a-state-space-beta-regression","dir":"Articles","previous_headings":"Salmon survival example","what":"A State-Space Beta regression","title":"Time-varying effects in mvgam","text":"mvgam can easily handle data bounded 0 1 Beta observation model (using mgcv function betar(), see ?mgcv::betar details). First fit simple State-Space model uses AR1 dynamic process model predictors Beta observation model: summary model shows good behaviour Hamiltonian Monte Carlo sampler provides useful summaries Beta observation model parameters: plot underlying dynamic component shows easily handled temporal evolution time series:","code":"mod0 <- mvgam(   formula = survival ~ 1,   trend_model = AR(),   noncentred = TRUE,   priors = prior(normal(-3.5, 0.5), class = Intercept),   family = betar(),   data = model_data,   silent = 2 ) summary(mod0) #> GAM formula: #> survival ~ 1 #>  #> Family: #> beta #>  #> Link function: #> logit #>  #> Trend model: #> AR() #>  #>  #> N series: #> 1  #>  #> N timepoints: #> 42  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> Observation precision parameter estimates: #>        2.5% 50% 97.5% Rhat n_eff #> phi[1]   83 220   550 1.03   188 #>  #> GAM coefficient (beta) estimates: #>             2.5%  50% 97.5% Rhat n_eff #> (Intercept) -4.6 -4.3    -4 1.01   716 #>  #> Latent trend parameter AR estimates: #>           2.5%  50% 97.5% Rhat n_eff #> ar1[1]   -0.30 0.68  0.98 1.01   525 #> sigma[1]  0.11 0.39  0.64 1.03   181 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model plot(mod0, type = \"trend\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"including-time-varying-upwelling-effects","dir":"Articles","previous_headings":"Salmon survival example","what":"Including time-varying upwelling effects","title":"Time-varying effects in mvgam","text":"Now can increase complexity model constructing fitting State-Space model time-varying effect coastal upwelling index addition autoregressive dynamics. use Beta observation model capture restrictions proportional observations, time include dynamic() effect CUI.apr latent process model. specify \\(\\rho\\) parameter, instead opting estimate using Hilbert space approximate GP: summary model now includes estimates time-varying GP parameters: estimates underlying dynamic process, hindcasts, haven’t changed much:   process error parameter \\(\\sigma\\) slightly smaller model first model:  process error need flexible second model? estimates dynamic process now informed partly time-varying effect upwelling, can visualise link scale using plot():","code":"mod1 <- mvgam(   formula = survival ~ 1,   trend_formula = ~ dynamic(CUI.apr, k = 25, scale = FALSE) - 1,   trend_model = AR(),   noncentred = TRUE,   priors = prior(normal(-3.5, 0.5), class = Intercept),   family = betar(),   data = model_data,   silent = 2 ) summary(mod1, include_betas = FALSE) #> GAM observation formula: #> survival ~ 1 #>  #> GAM process formula: #> ~dynamic(CUI.apr, k = 25, scale = FALSE) - 1 #>  #> Family: #> beta #>  #> Link function: #> logit #>  #> Trend model: #> AR() #>  #>  #> N process models: #> 1  #>  #> N series: #> 1  #>  #> N timepoints: #> 42  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> Observation precision parameter estimates: #>        2.5% 50% 97.5% Rhat n_eff #> phi[1]  170 350   670    1   744 #>  #> GAM observation model coefficient (beta) estimates: #>             2.5%  50% 97.5% Rhat n_eff #> (Intercept) -4.4 -3.7  -2.9 1.01   869 #>  #> Process model AR parameter estimates: #>        2.5%  50% 97.5% Rhat n_eff #> ar1[1] 0.59 0.93     1 1.01   764 #>  #> Process error parameter estimates: #>          2.5%  50% 97.5% Rhat n_eff #> sigma[1] 0.18 0.32  0.52    1   753 #>  #> GAM process model gp term marginal deviation (alpha) and length scale (rho) estimates: #>                         2.5%  50% 97.5% Rhat n_eff #> alpha_gp(time):CUI.apr 0.013 0.31   1.3    1   900 #> rho_gp(time):CUI.apr   1.300 5.70  42.0    1   659 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model plot(mod1, type = \"trend\") plot(mod1, type = \"forecast\") # Extract estimates of the process error 'sigma' for each model mod0_sigma <- as.data.frame(mod0, variable = \"sigma\", regex = TRUE) %>%   dplyr::mutate(model = \"Mod0\") mod1_sigma <- as.data.frame(mod1, variable = \"sigma\", regex = TRUE) %>%   dplyr::mutate(model = \"Mod1\") sigmas <- rbind(mod0_sigma, mod1_sigma)  # Plot using ggplot2 require(ggplot2) ggplot(sigmas, aes(y = `sigma[1]`, fill = model)) +   geom_density(alpha = 0.3, colour = NA) +   coord_flip() plot(mod1, type = \"smooths\", trend_effects = TRUE)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"comparing-model-predictive-performances","dir":"Articles","previous_headings":"Salmon survival example","what":"Comparing model predictive performances","title":"Time-varying effects in mvgam","text":"key question fitting multiple time series models whether one provides better predictions . several options mvgam exploring quantitatively. First, can compare models based -sample approximate leave-one-cross-validation implemented popular loo package: second model larger Expected Log Predictive Density (ELPD), meaning slightly favoured simpler model include time-varying upwelling effect. However, two models certainly differ much. metric compares -sample performance, hoping use models produce reasonable forecasts. Luckily, mvgam also routines comparing models using approximate leave-future-cross-validation. refit models reduced training set (starting time point 30) produce approximate 1-step ahead forecasts. forecasts used estimate forecast ELPD expanding training set one time point time. use Pareto-smoothed importance sampling reweight posterior predictions, acting kind particle filter don’t need refit model often (can read process works Bürkner et al. 2020). model time-varying upwelling effect tends provides better 1-step ahead forecasts, higher total forecast ELPD can also plot ELPDs model contrast. , values less zero suggest time-varying predictor model (Mod1) gives better 1-step ahead forecasts:  useful exercise expand model think kinds predictors might impact measurement error, easily implemented observation formula mvgam(). now, leave model -.","code":"loo_compare(mod0, mod1) #>      elpd_diff se_diff #> mod0     0.0       0.0 #> mod1 -1402.1     148.0 lfo_mod0 <- lfo_cv(mod0, min_t = 30) lfo_mod1 <- lfo_cv(mod1, min_t = 30) sum(lfo_mod0$elpds) #> [1] 35.65626 sum(lfo_mod1$elpds) #> [1] 37.10818 plot(   x = 1:length(lfo_mod0$elpds) + 30,   y = lfo_mod0$elpds - lfo_mod1$elpds,   ylab = \"ELPDmod0 - ELPDmod1\",   xlab = \"Evaluation time point\",   pch = 16,   col = \"darkred\",   bty = \"l\" ) abline(h = 0, lty = \"dashed\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"Time-varying effects in mvgam","text":"following papers resources offer lot useful material dynamic linear models can applied / evaluated practice: Bürkner, PC, Gabry, J Vehtari, Approximate leave-future-cross-validation Bayesian time series models. Journal Statistical Computation Simulation. 90:14 (2020) 2499-2523. Herrero, Asier, et al. individual landscape back: time‐varying effects climate herbivory tree sapling growth distribution limits. Journal Ecology 104.2 (2016): 430-442. Holmes, Elizabeth E., Eric J. Ward, Wills Kellie. “MARSS: multivariate autoregressive state-space models analyzing time-series data.” R Journal. 4.1 (2012): 11. Scheuerell, Mark D., John G. Williams. Forecasting climate induced changes survival Snake River Spring/Summer Chinook Salmon (Oncorhynchus Tshawytscha) Fisheries Oceanography 14 (2005): 448–57.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/time_varying_effects.html","id":"interested-in-contributing","dir":"Articles","previous_headings":"","what":"Interested in contributing?","title":"Time-varying effects in mvgam","text":"’m actively seeking PhD students researchers work areas ecological forecasting, multivariate model evaluation development mvgam. Please see small list opportunities website reach interested (n.clark’’uq.edu.au)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"state-space-models","dir":"Articles","previous_headings":"","what":"State-Space Models","title":"State-Space models in mvgam","text":"State-Space models allow us separately make inferences underlying dynamic process model interested (.e. evolution time series collection time series) observation model (.e. way survey / measure underlying process). extremely useful ecology observations always imperfect / noisy measurements thing interested measuring. also helpful often know covariates impact ability measure accurately (.e. take accurate counts rodents thunderstorm happening) covariates might impact underlying process (highly unlikely rodent abundance responds one storm, instead probably responds longer-term weather climate variation). State-Space model allows us model components single unified modelling framework. major advantage mvgam can include nonlinear effects random effects model components also capturing dynamic processes.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"lake-washington-plankton-data","dir":"Articles","previous_headings":"State-Space Models","what":"Lake Washington plankton data","title":"State-Space models in mvgam","text":"data use illustrate can fit State-Space models mvgam long-term monitoring study plankton counts (cells per mL) taken Lake Washington Washington, USA. data available part MARSS package can downloaded using following: work five different groups plankton: usual, preparing data correct format mvgam modelling takes little bit wrangling dplyr: Inspect data structure Note z-scored counts example make easier specify priors (though completely necessary; often better build model respects properties actual outcome variables)  missing observations, isn’t issue modelling mvgam. useful property understand counts tend highly seasonal. plots z-scored counts z-scored temperature measurements lake month:   try capture seasonality process model, easy given flexibility GAMs. Next split data training testing splits: Now time fit models. requires bit thinking can best tackle seasonal variation likely dependence structure data. algae interacting part complex system within lake, certainly expect lagged cross-dependencies underling dynamics. capture seasonal variation, multivariate dynamic model forced try capture , lead poor convergence unstable results (feasibly capture cyclic dynamics complex multi-species Lotka-Volterra model, ordinary differential equation approaches beyond scope mvgam).","code":"load(url(\"https://github.com/atsa-es/MARSS/raw/master/data/lakeWAplankton.rda\")) outcomes <- c(\"Greens\", \"Bluegreens\", \"Diatoms\", \"Unicells\", \"Other.algae\") # loop across each plankton group to create the long datframe plankton_data <- do.call(rbind, lapply(outcomes, function(x) {   # create a group-specific dataframe with counts labelled 'y'   # and the group name in the 'series' variable   data.frame(     year = lakeWAplanktonTrans[, \"Year\"],     month = lakeWAplanktonTrans[, \"Month\"],     y = lakeWAplanktonTrans[, x],     series = x,     temp = lakeWAplanktonTrans[, \"Temp\"]   ) })) %>%   # change the 'series' label to a factor   dplyr::mutate(series = factor(series)) %>%   # filter to only include some years in the data   dplyr::filter(year >= 1965 & year < 1975) %>%   dplyr::arrange(year, month) %>%   dplyr::group_by(series) %>%   # z-score the counts so they are approximately standard normal   dplyr::mutate(y = as.vector(scale(y))) %>%   # add the time indicator   dplyr::mutate(time = dplyr::row_number()) %>%   dplyr::ungroup() head(plankton_data) #> # A tibble: 6 × 6 #>    year month       y series       temp  time #>   <dbl> <dbl>   <dbl> <fct>       <dbl> <int> #> 1  1965     1 -0.542  Greens      -1.23     1 #> 2  1965     1 -0.344  Bluegreens  -1.23     1 #> 3  1965     1 -0.0768 Diatoms     -1.23     1 #> 4  1965     1 -1.52   Unicells    -1.23     1 #> 5  1965     1 -0.491  Other.algae -1.23     1 #> 6  1965     2 NA      Greens      -1.32     2 dplyr::glimpse(plankton_data) #> Rows: 600 #> Columns: 6 #> $ year   <dbl> 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 196… #> $ month  <dbl> 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, … #> $ y      <dbl> -0.54241769, -0.34410776, -0.07684901, -1.52243490, -0.49055442… #> $ series <fct> Greens, Bluegreens, Diatoms, Unicells, Other.algae, Greens, Blu… #> $ temp   <dbl> -1.2306562, -1.2306562, -1.2306562, -1.2306562, -1.2306562, -1.… #> $ time   <int> 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, … plot_mvgam_series(data = plankton_data, series = \"all\") plankton_data %>%   dplyr::filter(series == \"Other.algae\") %>%   ggplot(aes(x = time, y = temp)) +   geom_line(size = 1.1) +   geom_line(aes(y = y),     col = \"white\",     size = 1.3   ) +   geom_line(aes(y = y),     col = \"darkred\",     size = 1.1   ) +   ylab(\"z-score\") +   xlab(\"Time\") +   ggtitle(\"Temperature (black) vs Other algae (red)\") plankton_data %>%   dplyr::filter(series == \"Diatoms\") %>%   ggplot(aes(x = time, y = temp)) +   geom_line(size = 1.1) +   geom_line(aes(y = y),     col = \"white\",     size = 1.3   ) +   geom_line(aes(y = y),     col = \"darkred\",     size = 1.1   ) +   ylab(\"z-score\") +   xlab(\"Time\") +   ggtitle(\"Temperature (black) vs Diatoms (red)\") plankton_train <- plankton_data %>%   dplyr::filter(time <= 112) plankton_test <- plankton_data %>%   dplyr::filter(time > 112)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"capturing-seasonality","dir":"Articles","previous_headings":"State-Space Models","what":"Capturing seasonality","title":"State-Space models in mvgam","text":"First fit model include dynamic component, just see can reproduce seasonal variation observations. model introduces hierarchical multidimensional smooths, time series share “global” tensor product month temp variables, capturing expectation algal seasonality responds temperature variation. response depend year temperatures recorded (.e. response warm temperatures Spring different response warm temperatures Autumn). model also fits series-specific deviation smooths (.e. one tensor product per series) capture algal group’s seasonality differs overall “global” seasonality. Note include series-specific intercepts model series z-scored mean 0. “global” tensor product smooth function can quickly visualized:  plot, red indicates -average linear predictors white indicates -average. can plot deviation smooths algal groups see vary “global” pattern:   multidimensional smooths done good job capturing seasonal variation observations:    basic model gives us confidence can capture seasonal variation observations. model captured remaining temporal dynamics, obvious inspect Dunn-Smyth residuals series:","code":"notrend_mod <- mvgam(   y ~     # tensor of temp and month to capture     # \"global\" seasonality     te(temp, month, k = c(4, 4)) +      # series-specific deviation tensor products     te(temp, month, k = c(4, 4), by = series) - 1,   family = gaussian(),   data = plankton_train,   newdata = plankton_test,   trend_model = \"None\" ) plot_mvgam_smooth(notrend_mod, smooth = 1) plot_mvgam_smooth(notrend_mod, smooth = 2) plot_mvgam_smooth(notrend_mod, smooth = 3) plot(notrend_mod, type = \"forecast\", series = 1) plot(notrend_mod, type = \"forecast\", series = 2) plot(notrend_mod, type = \"forecast\", series = 3) plot(notrend_mod, type = \"residuals\", series = 1) plot(notrend_mod, type = \"residuals\", series = 3)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"multiseries-dynamics","dir":"Articles","previous_headings":"State-Space Models","what":"Multiseries dynamics","title":"State-Space models in mvgam","text":"Now time get multivariate State-Space models. fit two models can incorporate lagged cross-dependencies latent process models. first model assumes process errors operate independently one another, second assumes may contemporaneous correlations process errors. models include Vector Autoregressive component process means, can model complex community dynamics. models can described mathematically follows: \\[\\begin{align*} \\boldsymbol{count}_t & \\sim \\text{Normal}(\\mu_{obs[t]}, \\sigma_{obs}) \\\\ \\mu_{obs[t]} & = process_t \\\\ process_t & \\sim \\text{MVNormal}(\\mu_{process[t]}, \\Sigma_{process}) \\\\ \\mu_{process[t]} & = * process_{t-1} + f_{global}(\\boldsymbol{month},\\boldsymbol{temp})_t + f_{series}(\\boldsymbol{month},\\boldsymbol{temp})_t \\\\ f_{global}(\\boldsymbol{month},\\boldsymbol{temp}) & = \\sum_{k=1}^{K}b_{global} * \\beta_{global} \\\\ f_{series}(\\boldsymbol{month},\\boldsymbol{temp}) & = \\sum_{k=1}^{K}b_{series} * \\beta_{series} \\end{align*}\\] can see terms observation model apart underlying process model. easily add covariates observation model felt explain systematic observation errors. also assume independent observation processes (covariance structure observation errors \\(\\sigma_{obs}\\)). present, mvgam support multivariate observation models. feature added future versions. However underlying process model multivariate, lot going . component Vector Autoregressive part, process mean time \\(t\\) \\((\\mu_{process[t]})\\) vector evolves function vector-valued process model time \\(t-1\\). \\(\\) matrix captures dynamics self-dependencies diagonal possibly asymmetric cross-dependencies -diagonals, also incorporating nonlinear smooth functions capture seasonality series. contemporaneous process errors modeled \\(\\Sigma_{process}\\), can constrained process errors independent (.e. setting -diagonals 0) can fully parameterized using Cholesky decomposition (using Stan’s \\(LKJcorr\\) distribution place prior strength inter-species correlations). interested inner-workings, mvgam makes use recent breakthrough Sarah Heaps enforce stationarity Bayesian VAR processes. advantageous often don’t expect forecast variance increase without bound forever future, many estimated VARs tend behave way. Ok lot take . Let’s fit models try inspect going assume. first, need update mvgam’s default priors observation process errors. default, mvgam uses fairly wide Student-T prior parameters avoid overly informative. observations z-scored expect large process observation errors. However, also expect small observation errors either know measurements perfect. let’s update priors parameters. , get see formula latent process (.e. trend) model used mvgam: Get names parameters whose priors can modified: default prior distributions: Setting priors easy mvgam can use brms routines. use informative Normal priors error components, impose lower bound 0.2 observation errors: may noticed something else unique model: intercept term observation formula. shared intercept parameter can sometimes unidentifiable respect latent VAR process, particularly series similar long-run averages (case z-scored). often get better convergence State-Space models drop parameter. mvgam accomplishes fixing coefficient intercept zero. Now can fit first model, assumes process errors contemporaneously uncorrelated","code":"priors <- get_mvgam_priors(   # observation formula, which has no terms in it   y ~ -1,    # process model formula, which includes the smooth functions   trend_formula = ~ te(temp, month, k = c(4, 4)) +     te(temp, month, k = c(4, 4), by = trend) - 1,    # VAR1 model with uncorrelated process errors   trend_model = VAR(),   family = gaussian(),   data = plankton_train ) priors[, 3] #>  [1] \"(Intercept)\"                                                                                                                                                                                                                                                            #>  [2] \"process error sd\"                                                                                                                                                                                                                                                       #>  [3] \"diagonal autocorrelation population mean\"                                                                                                                                                                                                                               #>  [4] \"off-diagonal autocorrelation population mean\"                                                                                                                                                                                                                           #>  [5] \"diagonal autocorrelation population variance\"                                                                                                                                                                                                                           #>  [6] \"off-diagonal autocorrelation population variance\"                                                                                                                                                                                                                       #>  [7] \"shape1 for diagonal autocorrelation precision\"                                                                                                                                                                                                                          #>  [8] \"shape1 for off-diagonal autocorrelation precision\"                                                                                                                                                                                                                      #>  [9] \"shape2 for diagonal autocorrelation precision\"                                                                                                                                                                                                                          #> [10] \"shape2 for off-diagonal autocorrelation precision\"                                                                                                                                                                                                                      #> [11] \"observation error sd\"                                                                                                                                                                                                                                                   #> [12] \"te(temp,month) smooth parameters, te(temp,month):trendtrend1 smooth parameters, te(temp,month):trendtrend2 smooth parameters, te(temp,month):trendtrend3 smooth parameters, te(temp,month):trendtrend4 smooth parameters, te(temp,month):trendtrend5 smooth parameters\" priors[, 4] #>  [1] \"(Intercept) ~ student_t(3, -0.1, 2.5);\" #>  [2] \"sigma ~ inv_gamma(1.418, 0.452);\"       #>  [3] \"es[1] = 0;\"                             #>  [4] \"es[2] = 0;\"                             #>  [5] \"fs[1] = sqrt(0.455);\"                   #>  [6] \"fs[2] = sqrt(0.455);\"                   #>  [7] \"gs[1] = 1.365;\"                         #>  [8] \"gs[2] = 1.365;\"                         #>  [9] \"hs[1] = 0.071175;\"                      #> [10] \"hs[2] = 0.071175;\"                      #> [11] \"sigma_obs ~ inv_gamma(1.418, 0.452);\"   #> [12] \"lambda_trend ~ normal(5, 30);\" priors <- c(   prior(normal(0.5, 0.1), class = sigma_obs, lb = 0.2),   prior(normal(0.5, 0.25), class = sigma) ) var_mod <- mvgam(   # observation formula, which is empty   forumla = y ~ -1,    # process model formula, which includes the smooth functions   trend_formula = ~ te(temp, month, k = c(4, 4)) +     te(temp, month, k = c(4, 4), by = trend) - 1,    # VAR1 model with uncorrelated process errors   trend_model = VAR(),   family = gaussian(),   data = plankton_train,   newdata = plankton_test,    # include the updated priors   priors = priors,   silent = 2 )"},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"inspecting-ss-models","dir":"Articles","previous_headings":"State-Space Models","what":"Inspecting SS models","title":"State-Space models in mvgam","text":"model’s summary bit different mvgam summaries. separates parameters based whether belong observation model latent process model. may often covariates impact observations latent process, can fairly complex models component. notice parameters fully converged, particularly VAR coefficients (called output) process errors (Sigma). Note set include_betas = FALSE stop summary printing output spline coefficients, can dense hard interpret: convergence model isn’t fabulous (moment). can plot smooth functions, time operate process model. can see plot using trend_effects = TRUE plotting functions:  autoregressive coefficient matrix particular interest , captures lagged dependencies cross-dependencies latent process model. Unfortunately bayesplot doesn’t know matrix parameters see actually transpose VAR matrix. Using dir = 'v' facet_args argument accomplish :  lot happening matrix. cell captures lagged effect process column process row next timestep. example, effect cell [1,3] shows increase process series 3 (Greens) time \\(t\\) expected impact process series 1 (Bluegreens) time \\(t+1\\). latent process model now capturing effects smooth seasonal effects. process error \\((\\Sigma)\\) captures unmodelled variation process models. , fixed -diagonals 0, histograms look like flat boxes:  observation error estimates \\((\\sigma_{obs})\\) represent much model thinks might miss true count take imperfect measurements:  still bit hard identify overall, especially trying estimate process observation error. Often need make strong assumptions important determining unexplained variation observations.","code":"summary(var_mod, include_betas = FALSE) #> GAM observation formula: #> y ~ 1 #>  #> GAM process formula: #> ~te(temp, month, k = c(4, 4)) + te(temp, month, k = c(4, 4),  #>     by = trend) - 1 #>  #> Family: #> gaussian #>  #> Link function: #> identity #>  #> Trend model: #> VAR() #>  #>  #> N process models: #> 5  #>  #> N series: #> 5  #>  #> N timepoints: #> 120  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1500; warmup = 1000; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> Observation error parameter estimates: #>              2.5%  50% 97.5% Rhat n_eff #> sigma_obs[1] 0.20 0.26  0.34 1.02   309 #> sigma_obs[2] 0.26 0.40  0.54 1.01   232 #> sigma_obs[3] 0.42 0.63  0.80 1.07    59 #> sigma_obs[4] 0.26 0.38  0.50 1.01   354 #> sigma_obs[5] 0.30 0.43  0.54 1.00   267 #>  #> GAM observation model coefficient (beta) estimates: #>             2.5% 50% 97.5% Rhat n_eff #> (Intercept)    0   0     0  NaN   NaN #>  #> Process model VAR parameter estimates: #>          2.5%    50% 97.5% Rhat n_eff #> A[1,1]  0.620  0.790 0.910 1.01   355 #> A[1,2] -0.390 -0.130 0.041 1.01   277 #> A[1,3] -0.160  0.017 0.180 1.00   600 #> A[1,4] -0.058  0.064 0.200 1.00   614 #> A[1,5] -0.038  0.110 0.350 1.01   330 #> A[2,1] -0.510 -0.200 0.025 1.00   220 #> A[2,2]  0.079  0.420 0.730 1.00   220 #> A[2,3] -0.310  0.012 0.350 1.02   161 #> A[2,4] -0.067  0.130 0.410 1.01   234 #> A[2,5] -0.034  0.220 0.630 1.01   243 #> A[3,1] -0.360 -0.036 0.170 1.01   513 #> A[3,2] -0.500 -0.029 0.350 1.00   416 #> A[3,3] -0.024  0.480 0.820 1.07    68 #> A[3,4] -0.091  0.140 0.570 1.03   177 #> A[3,5] -0.280  0.040 0.410 1.00   680 #> A[4,1] -0.420 -0.120 0.075 1.01   209 #> A[4,2] -0.650 -0.180 0.110 1.01   178 #> A[4,3] -0.270  0.069 0.470 1.02   177 #> A[4,4]  0.510  0.740 0.960 1.01   272 #> A[4,5] -0.052  0.190 0.600 1.01   254 #> A[5,1] -0.096  0.059 0.260 1.00   646 #> A[5,2] -0.450 -0.120 0.120 1.01   234 #> A[5,3] -0.200  0.055 0.310 1.01   251 #> A[5,4] -0.210 -0.036 0.140 1.00   413 #> A[5,5]  0.470  0.740 0.930 1.00   507 #>  #> Process error parameter estimates: #>             2.5%  50% 97.5% Rhat n_eff #> Sigma[1,1] 0.068 0.11  0.18 1.01   541 #> Sigma[1,2] 0.000 0.00  0.00  NaN   NaN #> Sigma[1,3] 0.000 0.00  0.00  NaN   NaN #> Sigma[1,4] 0.000 0.00  0.00  NaN   NaN #> Sigma[1,5] 0.000 0.00  0.00  NaN   NaN #> Sigma[2,1] 0.000 0.00  0.00  NaN   NaN #> Sigma[2,2] 0.054 0.16  0.30 1.02   168 #> Sigma[2,3] 0.000 0.00  0.00  NaN   NaN #> Sigma[2,4] 0.000 0.00  0.00  NaN   NaN #> Sigma[2,5] 0.000 0.00  0.00  NaN   NaN #> Sigma[3,1] 0.000 0.00  0.00  NaN   NaN #> Sigma[3,2] 0.000 0.00  0.00  NaN   NaN #> Sigma[3,3] 0.045 0.30  0.68 1.09    43 #> Sigma[3,4] 0.000 0.00  0.00  NaN   NaN #> Sigma[3,5] 0.000 0.00  0.00  NaN   NaN #> Sigma[4,1] 0.000 0.00  0.00  NaN   NaN #> Sigma[4,2] 0.000 0.00  0.00  NaN   NaN #> Sigma[4,3] 0.000 0.00  0.00  NaN   NaN #> Sigma[4,4] 0.098 0.21  0.34 1.01   400 #> Sigma[4,5] 0.000 0.00  0.00  NaN   NaN #> Sigma[5,1] 0.000 0.00  0.00  NaN   NaN #> Sigma[5,2] 0.000 0.00  0.00  NaN   NaN #> Sigma[5,3] 0.000 0.00  0.00  NaN   NaN #> Sigma[5,4] 0.000 0.00  0.00  NaN   NaN #> Sigma[5,5] 0.060 0.13  0.26 1.01   243 #>  #> Approximate significance of GAM process smooths: #>                              edf Ref.df Chi.sq p-value   #> te(temp,month)              4.98     15  33.02   0.038 * #> te(temp,month):seriestrend1 1.16     15   7.33   0.998   #> te(temp,month):seriestrend2 2.71     15  64.58   0.471   #> te(temp,month):seriestrend3 1.55     15   1.50   1.000   #> te(temp,month):seriestrend4 1.32     15   5.55   0.999   #> te(temp,month):seriestrend5 1.62     15  12.47   0.977   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model plot(var_mod, \"smooths\", trend_effects = TRUE) mcmc_plot(   var_mod,   variable = 'A',   regex = TRUE,   type = 'hist',   facet_args = list(dir = 'v') ) mcmc_plot(   var_mod,   variable = 'Sigma',   regex = TRUE,   type = 'hist',   facet_args = list(dir = 'v') ) mcmc_plot(var_mod, variable = \"sigma_obs\", regex = TRUE, type = \"hist\")"},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"correlated-process-errors","dir":"Articles","previous_headings":"State-Space Models","what":"Correlated process errors","title":"State-Space models in mvgam","text":"Let’s see estimates improve allow process errors correlated. , need first update priors observation errors: now can fit correlated process error model \\((\\Sigma)\\) matrix now captures evidence contemporaneously correlated process error:  symmetric matrix tells us support correlated process errors, several -diagonal entries strongly non-zero. easier interpret estimates convert covariance matrix correlation matrix. compute posterior median process error correlations:","code":"priors <- c(   prior(normal(0.5, 0.1), class = sigma_obs, lb = 0.2),   prior(normal(0.5, 0.25), class = sigma) ) varcor_mod <- mvgam(   # observation formula, which remains empty   formula = y ~ -1,    # process model formula, which includes the smooth functions   trend_formula = ~ te(temp, month, k = c(4, 4)) +     te(temp, month, k = c(4, 4), by = trend) - 1,    # VAR1 model with correlated process errors   trend_model = VAR(cor = TRUE),   family = gaussian(),   data = plankton_train,   newdata = plankton_test,    # include the updated priors   priors = priors,   silent = 2 ) mcmc_plot(   varcor_mod,   variable = 'Sigma',   regex = TRUE,   type = 'hist',   facet_args = list(dir = 'v') ) Sigma_post <- as.matrix(   varcor_mod,    variable = \"Sigma\",    regex = TRUE ) median_correlations <- cov2cor(   matrix(apply(Sigma_post, 2, median),          nrow = 5,           ncol = 5   ) ) rownames(median_correlations) <-    colnames(median_correlations) <-    levels(plankton_train$series)  round(median_correlations, 2) #>             Bluegreens Diatoms Greens Other.algae Unicells #> Bluegreens        1.00   -0.20  -0.03        0.17     0.50 #> Diatoms          -0.20    1.00   0.19        0.46     0.19 #> Greens           -0.03    0.19   1.00        0.35    -0.04 #> Other.algae       0.17    0.46   0.35        1.00     0.27 #> Unicells          0.50    0.19  -0.04        0.27     1.00"},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"impulse-response-functions","dir":"Articles","previous_headings":"State-Space Models","what":"Impulse response functions","title":"State-Space models in mvgam","text":"Vector Autoregressions can capture complex lagged dependencies, often difficult understand member time series thought interact one another. method commonly used directly test possible interactions compute Impulse Response Function (IRF). \\(h\\) represents simulated forecast horizon, IRF asks remaining series might respond times \\((t+1):h\\) focal series given innovation “shock” time \\(t = 0\\). mvgam can compute Generalized Orthogonalized IRFs models included latent VAR dynamics. simply feed fitted model irf() function use S3 plot() function view estimated responses. default, irf() compute IRFs separately imposing positive shocks one standard deviation series VAR process. compute Generalized IRFs horizon 12 timesteps: summary IRFs can computed using summary() function: easier understand responses using plots. example, can plot expected responses remaining series positive shock series 3 (Greens) using plot() function:  series plots makes clear series expected show instantaneous responses shock Greens (due correlated process errors) well delayed nonlinear responses time (due complex lagged dependence structure captured \\(\\) matrix). hopefully makes clear IRFs important tool analysis multivariate autoregressive models. can also use IRFs calculate relative contribution shock forecast error variance focal series. method, known Forecast Error Variance Decomposition (FEVD), useful get idea amount information series contributes evolution series Vector Autoregression:  plot shows median contribution forecast error variance series.","code":"irfs <- irf(varcor_mod, h = 12) summary(irfs) #> # A tibble: 300 × 5 #>    shock                horizon irfQ50 irfQ2.5 irfQ97.5 #>    <chr>                  <int>  <dbl>   <dbl>    <dbl> #>  1 Process1 -> Process1       1 0.353   0.270     0.446 #>  2 Process1 -> Process1       2 0.298   0.236     0.371 #>  3 Process1 -> Process1       3 0.252   0.196     0.318 #>  4 Process1 -> Process1       4 0.214   0.159     0.280 #>  5 Process1 -> Process1       5 0.181   0.126     0.250 #>  6 Process1 -> Process1       6 0.153   0.0971    0.227 #>  7 Process1 -> Process1       7 0.130   0.0734    0.204 #>  8 Process1 -> Process1       8 0.111   0.0541    0.184 #>  9 Process1 -> Process1       9 0.0951  0.0396    0.169 #> 10 Process1 -> Process1      10 0.0808  0.0288    0.155 #> # ℹ 290 more rows plot(irfs, series = 3) fevds <- fevd(varcor_mod, h = 12) plot(fevds)"},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"comparing-forecast-scores","dir":"Articles","previous_headings":"State-Space Models","what":"Comparing forecast scores","title":"State-Space models in mvgam","text":"model better? can compute variogram score sample forecasts get sense model better job capturing dependence structure true evaluation set:  can also compute energy score sample forecasts get sense model provides forecasts better calibrated:  models tend provide similar forecasts, though correlated error model slightly better overall. probably need use extensive rolling forecast evaluation exercise felt like needed choose one production. mvgam offers utilities (.e. see ?lfo_cv guidance). Alternatively, use forecasts models creating evenly-weighted ensemble forecast distribution. capability available using ensemble() function mvgam (see ?ensemble guidance). Using how_to_cite() models VAR dynamics give information restricted remain stationary: advanced hierarchical panel VAR models can also handled using gr subgr arguments VAR(). models useful data set series (subgr) measured different regions (gr), species measured different sampling regions financial series measured different countries.","code":"# create forecast objects for each model fcvar <- forecast(var_mod) fcvarcor <- forecast(varcor_mod)  # plot the difference in variogram scores; a negative value means the VAR1cor model is better, while a positive value means the VAR1 model is better diff_scores <- score(fcvarcor, score = \"variogram\")$all_series$score -   score(fcvar, score = \"variogram\")$all_series$score plot(diff_scores,   pch = 16, cex = 1.25, col = \"darkred\",   ylim = c(     -1 * max(abs(diff_scores), na.rm = TRUE),     max(abs(diff_scores), na.rm = TRUE)   ),   bty = \"l\",   xlab = \"Forecast horizon\",   ylab = expression(variogram[VAR1cor] ~ -~ variogram[VAR1]) ) abline(h = 0, lty = \"dashed\") # plot the difference in energy scores; a negative value means the VAR1cor model is better, while a positive value means the VAR1 model is better diff_scores <- score(fcvarcor, score = \"energy\")$all_series$score -   score(fcvar, score = \"energy\")$all_series$score plot(diff_scores,   pch = 16, cex = 1.25, col = \"darkred\",   ylim = c(     -1 * max(abs(diff_scores), na.rm = TRUE),     max(abs(diff_scores), na.rm = TRUE)   ),   bty = \"l\",   xlab = \"Forecast horizon\",   ylab = expression(energy[VAR1cor] ~ -~ energy[VAR1]) ) abline(h = 0, lty = \"dashed\") description <- how_to_cite(varcor_mod) description #> Methods text skeleton #> We used the R package mvgam (version 1.1.57; Clark & Wells, 2023) to #>   construct, fit and interrogate the model. mvgam fits Bayesian #>   State-Space models that can include flexible predictor effects in both #>   the process and observation components by incorporating functionalities #>   from the brms (Burkner 2017), mgcv (Wood 2017) and splines2 (Wang & Yan, #>   2023) packages. To encourage stability and prevent forecast variance #>   from increasing indefinitely, we enforced stationarity of the Vector #>   Autoregressive process following methods described by Heaps (2023) and #>   Clark et al. (2025). The mvgam-constructed model and observed data were #>   passed to the probabilistic programming environment Stan (version #>   2.36.0; Carpenter et al. 2017, Stan Development Team 2025), specifically #>   through the cmdstanr interface (Gabry & Cesnovar, 2021). We ran 4 #>   Hamiltonian Monte Carlo chains for 1000 warmup iterations and 500 #>   sampling iterations for joint posterior estimation. Rank normalized #>   split Rhat (Vehtari et al. 2021) and effective sample sizes were used to #>   monitor convergence. #>  #> Primary references #> Clark, NJ and Wells K (2023). Dynamic Generalized Additive Models #>   (DGAMs) for forecasting discrete ecological time series. Methods in #>   Ecology and Evolution, 14, 771-784. doi.org/10.1111/2041-210X.13974 #> Burkner, PC (2017). brms: An R Package for Bayesian Multilevel Models #>   Using Stan. Journal of Statistical Software, 80(1), 1-28. #>   doi:10.18637/jss.v080.i01 #> Wood, SN (2017). Generalized Additive Models: An Introduction with R #>   (2nd edition). Chapman and Hall/CRC. #> Wang W and Yan J (2021). Shape-Restricted Regression Splines with R #>   Package splines2. Journal of Data Science, 19(3), 498-517. #>   doi:10.6339/21-JDS1020 https://doi.org/10.6339/21-JDS1020. #> Heaps, SE (2023). Enforcing stationarity through the prior in vector #>   autoregressions. Journal of Computational and Graphical Statistics 32, #>   74-83. #> Clark NJ, Ernest SKM, Senyondo H, Simonis J, White EP, Yenni GM, #>   Karunarathna KANK (2025). Beyond single-species models: leveraging #>   multispecies forecasts to navigate the dynamics of ecological #>   predictability. PeerJ 13:e18929. #> Carpenter B, Gelman A, Hoffman MD, Lee D, Goodrich B, Betancourt M, #>   Brubaker M, Guo J, Li P and Riddell A (2017). Stan: A probabilistic #>   programming language. Journal of Statistical Software 76. #> Gabry J, Cesnovar R, Johnson A, and Bronder S (2025). cmdstanr: R #>   Interface to 'CmdStan'. https://mc-stan.org/cmdstanr/, #>   https://discourse.mc-stan.org. #> Vehtari A, Gelman A, Simpson D, Carpenter B, and Burkner P (2021). #>   Rank-normalization, folding, and localization: An improved Rhat for #>   assessing convergence of MCMC (with discussion). Bayesian Analysis 16(2) #>   667-718. https://doi.org/10.1214/20-BA1221. #>  #> Other useful references #> Arel-Bundock V, Greifer N, and Heiss A (2024). How to interpret #>   statistical models using marginaleffects for R and Python. Journal of #>   Statistical Software, 111(9), 1-32. #>   https://doi.org/10.18637/jss.v111.i09 #> Gabry J, Simpson D, Vehtari A, Betancourt M, and Gelman A (2019). #>   Visualization in Bayesian workflow. Journal of the Royal Statatistical #>   Society A, 182, 389-402. doi:10.1111/rssa.12378. #> Vehtari A, Gelman A, and Gabry J (2017). Practical Bayesian model #>   evaluation using leave-one-out cross-validation and WAIC. Statistics and #>   Computing, 27, 1413-1432. doi:10.1007/s11222-016-9696-4. #> Burkner PC, Gabry J, and Vehtari A. (2020). Approximate leave-future-out #>   cross-validation for Bayesian time series models. Journal of Statistical #>   Computation and Simulation, 90(14), 2499-2523. #>   https://doi.org/10.1080/00949655.2020.1783262"},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"State-Space models in mvgam","text":"following papers resources offer lot useful material multivariate State-Space models can applied practice: Auger‐Méthé, Marie, et al. guide state–space modeling ecological time series. Ecological Monographs 91.4 (2021): e01470. Clark, Nicholas J., et al. Beyond single-species models: leveraging multispecies forecasts navigate dynamics ecological predictability. PeerJ. (2025): 13:e18929 Heaps, Sarah E. Enforcing stationarity prior vector autoregressions. Journal Computational Graphical Statistics 32.1 (2023): 74-83. Hannaford, Naomi E., et al. sparse Bayesian hierarchical vector autoregressive model microbial dynamics wastewater treatment plant. Computational Statistics & Data Analysis 179 (2023): 107659. Holmes, Elizabeth E., Eric J. Ward, Wills Kellie. MARSS: multivariate autoregressive state-space models analyzing time-series data. R Journal. 4.1 (2012): 11. Karunarathna, K..N.K., et al. Modelling nonlinear responses desert rodent species environmental change hierarchical dynamic generalized additive models. Ecological Modelling (2024): 490, 110648. Ward, Eric J., et al. Inferring spatial structure time‐series data: using multivariate state‐space models detect metapopulation structure California sea lions Gulf California, Mexico. Journal Applied Ecology 47.1 (2010): 47-56.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/articles/trend_formulas.html","id":"interested-in-contributing","dir":"Articles","previous_headings":"","what":"Interested in contributing?","title":"State-Space models in mvgam","text":"’m actively seeking PhD students researchers work areas ecological forecasting, multivariate model evaluation development mvgam. Please see small list opportunities website reach interested (n.clark’’uq.edu.au)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Nicholas J Clark. Author, maintainer. Sarah Heaps. Contributor.            VARMA parameterisations Scott Pease. Contributor.            broom enhancements Matthijs Hollanders. Contributor.            ggplot visualizations","code":""},{"path":"https://nicholasjclark.github.io/mvgam/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Clark & Wells (2023). Dynamic Generalized Additive Models (DGAMs) forecasting discrete ecological time series. Methods Ecology Evolution, 14, 771-784. doi.org/10.1111/2041-210X.13974","code":"@Article{,   title = {Dynamic Generalized Additive Models (DGAMs) for forecasting discrete ecological time series},   author = {Nicholas J Clark and Konstans Wells},   journal = {Methods in Ecology and Evolution},   year = {2023},   volume = {14},   pages = {771-784},   doi = {10.18637/jss.v100.i05},   encoding = {UTF-8}, }"},{"path":"https://nicholasjclark.github.io/mvgam/index.html","id":"mvgam","dir":"","previous_headings":"","what":"Multivariate (Dynamic) Generalized Additive Models","title":"Multivariate (Dynamic) Generalized Additive Models","text":"MultiVariate (Dynamic) Generalized Additive Models goal mvgam 📦 fit Bayesian Dynamic Generalized Additive Models (DGAMs) can include highly flexible nonlinear predictor effects process observation components. package relying functionalities impressive brms mgcv packages. Parameters estimated using probabilistic programming language Stan, giving users access advanced Bayesian inference algorithms available. allows mvgam fit wide range models, including: Multivariate State-Space Time Series Models Hierarchical N-mixture Models Hierarchical Generalized Additive Models Joint Species Distribution Models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Multivariate (Dynamic) Generalized Additive Models","text":"Install stable version CRAN using: install.packages('mvgam'), install development version GitHub using: devtools::install_github(\"nicholasjclark/mvgam\"). also need working version Stan installed (along either rstan /cmdstanr). Please refer installation links Stan rstan , Stan cmdstandr .","code":""},{"path":[]},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting started","title":"Multivariate (Dynamic) Generalized Additive Models","text":"mvgam originally designed analyse forecast non-negative integer-valued data (counts). data traditionally challenging analyse existing time-series analysis packages. development mvgam resulted support growing number observation families extend types data. Currently, package can handle data following families: gaussian() real-valued data student_t() heavy-tailed real-valued data lognormal() non-negative real-valued data Gamma() non-negative real-valued data betar() proportional data (0,1) bernoulli() binary data poisson() count data nb() overdispersed count data binomial() count data known number trials beta_binomial() overdispersed count data known number trials nmix() count data imperfect detection (unknown number trials) See ?mvgam_families information. simple example simulating modelling proportional data Beta observations set seasonal series independent Gaussian Process dynamic trends: Plot series see evolve time Visualizing multivariate proportional time series using mvgam R package #rstats Fit State-Space GAM series uses hierarchical cyclic seasonal smooth term capture variation seasonality among series. model also includes series-specific latent Gaussian Processes squared exponential covariance functions capture temporal dynamics Plot estimated posterior hindcast forecast distributions series Forecasting multivariate time series Dynamic Generalized Additive Models Various S3 functions can used inspect parameter estimates, plot smooth functions residuals, evaluate models posterior predictive checks forecast comparisons. Please see package documentation detailed examples.","code":"set.seed(100) data <- sim_mvgam(   family = betar(),   T = 80,   trend_model = GP(),   prop_trend = 0.5,    seasonality = 'shared' ) plot_mvgam_series(   data = data$data_train,    series = 'all' ) mod <- mvgam(   y ~ s(season, bs = 'cc', k = 7) +     s(season, by = series, m = 1, k = 5),   trend_model = GP(),   data = data$data_train,   newdata = data$data_test,   family = betar() ) library(patchwork) fc <- forecast(mod) wrap_plots(   plot(fc, series = 1),    plot(fc, series = 2),    plot(fc, series = 3),    ncol = 2 )"},{"path":"https://nicholasjclark.github.io/mvgam/index.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"Multivariate (Dynamic) Generalized Additive Models","text":"can set build_vignettes = TRUE installing aware slow installation drastically. Instead, can always access vignette htmls online https://nicholasjclark.github.io/mvgam/articles/","code":""},{"path":"https://nicholasjclark.github.io/mvgam/index.html","id":"citing-mvgam-and-related-software","dir":"","previous_headings":"","what":"Citing mvgam and related software","title":"Multivariate (Dynamic) Generalized Additive Models","text":"using software please make sure appropriately acknowledge hard work developers maintainers put making packages available. Citations currently best way formally acknowledge work (feel free ⭐ repo well), highly encourage cite packages rely research. using mvgam, please cite following: Clark, N.J. Wells, K. (2023). Dynamic Generalized Additive Models (DGAMs) forecasting discrete ecological time series. Methods Ecology Evolution. DOI: https://doi.org/10.1111/2041-210X.13974 mvgam acts interface Stan, please additionally cite: Carpenter B., Gelman ., Hoffman M. D., Lee D., Goodrich B., Betancourt M., Brubaker M., Guo J., Li P., Riddell . (2017). Stan: probabilistic programming language. Journal Statistical Software. 76(1). DOI: https://doi.org/10.18637/jss.v076.i01 mvgam relies several R packages , course, R . find cite R packages, use citation(). features mvgam specifically rely certain packages. important generation data necessary estimate smoothing splines Gaussian Processes, rely mgcv, brms splines2 packages. rstan cmdstanr packages together Rcpp makes Stan conveniently accessible R. use features, please also consider citing related packages.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/index.html","id":"other-resources","dir":"","previous_headings":"","what":"Other resources","title":"Multivariate (Dynamic) Generalized Additive Models","text":"number case studies step--step webinars compiled highlight GAMs DGAMs can useful analysing multivariate data: Time series R Stan using mvgam package Ecological Forecasting Dynamic Generalized Additive Models State-Space Vector Autoregressions mvgam interpret report nonlinear effects Generalized Additive Models Phylogenetic smoothing using mgcv Distributed lags (hierarchical distributed lags) using mgcv mvgam Incorporating time-varying seasonality forecast models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/index.html","id":"getting-help","dir":"","previous_headings":"","what":"Getting help","title":"Multivariate (Dynamic) Generalized Additive Models","text":"encounter clear bug, please file issue minimal reproducible example GitHub. Please also feel free use mvgam Discussion Board hunt post discussion topics related package, check mvgam changelog updates recent upgrades package incorporated.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/index.html","id":"interested-in-contributing","dir":"","previous_headings":"","what":"Interested in contributing?","title":"Multivariate (Dynamic) Generalized Additive Models","text":"’m actively seeking PhD students researchers work areas ecological forecasting, multivariate model evaluation development mvgam. Please reach interested (n.clark’’uq.edu.au). contributions also welcome, please see Contributor Instructions general guidelines. Note participating project agree abide terms Contributor Code Conduct.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/GP.html","id":null,"dir":"Reference","previous_headings":"","what":"Specify dynamic Gaussian process trends in mvgam models — GP","title":"Specify dynamic Gaussian process trends in mvgam models — GP","text":"Set low-rank approximate Gaussian Process trend models using Hilbert basis expansions mvgam. function evaluate arguments – exists purely help set model particular GP trend models.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/GP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specify dynamic Gaussian process trends in mvgam models — GP","text":"","code":"GP(...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/GP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specify dynamic Gaussian process trends in mvgam models — GP","text":"... unused","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/GP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specify dynamic Gaussian process trends in mvgam models — GP","text":"object class mvgam_trend, contains list arguments interpreted parsing functions mvgam","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/GP.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Specify dynamic Gaussian process trends in mvgam models — GP","text":"GP trend estimated series using Hilbert space approximate Gaussian Processes. mvgam, latent squared exponential GP trends approximated using default 20 basis functions using multiplicative factor c = 5/4, saves computational costs compared fitting full GPs adequately estimating GP alpha rho parameters.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/GP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Specify dynamic Gaussian process trends in mvgam models — GP","text":"Riutort-Mayol G, Burkner PC, Andersen MR, Solin Vehtari (2023). Practical Hilbert space approximate Bayesian Gaussian processes probabilistic programming. Statistics Computing 33, 1. https://doi.org/10.1007/s11222-022-10167-2","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/GP.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Specify dynamic Gaussian process trends in mvgam models — GP","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/RW.html","id":null,"dir":"Reference","previous_headings":"","what":"Specify autoregressive dynamic processes in mvgam — RW","title":"Specify autoregressive dynamic processes in mvgam — RW","text":"Set autoregressive autoregressive moving average trend models mvgam. functions evaluate arguments – exist purely help set model particular autoregressive trend models.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/RW.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specify autoregressive dynamic processes in mvgam — RW","text":"","code":"RW(ma = FALSE, cor = FALSE, gr = NA, subgr = NA)  AR(p = 1, ma = FALSE, cor = FALSE, gr = NA, subgr = NA)  CAR(p = 1)  VAR(ma = FALSE, cor = FALSE, gr = NA, subgr = NA)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/RW.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specify autoregressive dynamic processes in mvgam — RW","text":"ma Logical Include moving average terms order 1? Default FALSE. cor Logical Include correlated process errors part multivariate normal process model? TRUE n_series > 1 supplied data, fully structured covariance matrix estimated process errors. Default FALSE. gr optional grouping variable, must factor supplied data, setting hierarchical residual correlation structures. specified, automatically set cor = TRUE set model residual correlations specific level gr modelled hierarchically: \\(\\Omega_{group} = \\alpha_{cor}\\Omega_{global} + (1 - \\alpha_{cor})\\Omega_{group, local}\\), \\(\\Omega_{global}\\) global correlation matrix, \\(\\Omega_{group, local}\\) local deviation correlation matrix \\(\\alpha_{cor}\\) weighting parameter controlling strongly local correlation matrix \\(\\Omega_{group}\\) shrunk towards global correlation matrix \\(\\Omega_{global}\\) (larger values \\(\\alpha_{cor}\\) indicate greater degree shrinkage, .e. greater degree partial pooling). used within VAR() model, essentially sets hierarchical panel vector autoregression autoregressive correlation matrices learned hierarchically. gr supplied subgr must also supplied subgr subgrouping factor variable specifying element data represents different time series. Defaults series, note models use hierarchical correlations, subgr time series measured level gr, include series element data. Rather, element created internally based supplied variables gr subgr. example, modelling temporal counts group species (labelled species data) across three different geographical regions (labelled region), like residuals correlated within regions, specify gr = region subgr = species. Internally, mvgam() create series element data using: series = interaction(group, subgroup, drop = TRUE)) p non-negative integer specifying autoregressive (AR) order. Default 1. currently larger 3 AR terms, anything 1 continuous time AR (CAR) terms","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/RW.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specify autoregressive dynamic processes in mvgam — RW","text":"object class mvgam_trend, contains list arguments interpreted parsing functions mvgam","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/RW.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Specify autoregressive dynamic processes in mvgam — RW","text":"Use vignette(\"mvgam_overview\") see full details available stochastic trend types mvgam, view rendered version package website : https://nicholasjclark.github.io/mvgam/articles/mvgam_overview.html","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/RW.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Specify autoregressive dynamic processes in mvgam — RW","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/RW.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Specify autoregressive dynamic processes in mvgam — RW","text":"","code":"# \\donttest{ # A short example to illustrate CAR(1) models # Function to simulate CAR1 data with seasonality sim_corcar1 = function(n = 125,                       phi = 0.5,                       sigma = 2,                       sigma_obs = 0.75){ # Sample irregularly spaced time intervals time_dis <- c(1, runif(n - 1, 0, 5))  # Set up the latent dynamic process x <- vector(length = n); x[1] <- -0.3 for(i in 2:n){  # zero-distances will cause problems in sampling, so mvgam uses a  # minimum threshold; this simulation function emulates that process  if(time_dis[i] == 0){    x[i] <- rnorm(      1, mean = (phi^1e-3) * x[i - 1],      sd = sigma * (1 - phi^(2*1e-3)) / (1 - phi^2)    )   } else {    x[i] <- rnorm(      1,      mean = (phi^time_dis[i]) * x[i - 1],      sd = sigma * (1 - phi^(2*time_dis[i])) / (1 - phi^2)    )   } }  # Add 12-month seasonality cov1 <- sin(2 * pi * (1 : n) / 12); cov2 <- cos(2 * pi * (1 : n) / 12) beta1 <- runif(1, 0.3, 0.7); beta2 <- runif(1, 0.2, 0.5) seasonality <- beta1 * cov1 + beta2 * cov2  # Take Gaussian observations with error and return data.frame(y = rnorm(n, mean = x + seasonality, sd = sigma_obs),            season = rep(1:12, 20)[1:n],            time = cumsum(time_dis)) }  # Sample two time series dat <- rbind(dplyr::bind_cols(sim_corcar1(phi = 0.65,                                          sigma_obs = 0.55),                              data.frame(series = 'series1')),             dplyr::bind_cols(sim_corcar1(phi = 0.8,                              sigma_obs = 0.35),                              data.frame(series = 'series2'))) %>%       dplyr::mutate(series = as.factor(series))  # mvgam with CAR(1) trends and series-level seasonal smooths; the # State-Space representation (using trend_formula) will be more efficient; # using informative priors on the sigmas often helps with convergence mod <- mvgam(formula = y ~ -1,             trend_formula = ~ s(season, bs = 'cc',                                 k = 5, by = trend),             trend_model = CAR(),             priors = c(prior(exponential(3),                            class = sigma),                        prior(beta(4, 4),                            class = sigma_obs)),             data = dat,             family = gaussian(),             chains = 2,             silent = 2)  # View usual summaries and plots summary(mod) #> GAM observation formula: #> y ~ 1 #> <environment: 0x5578e326c778> #>  #> GAM process formula: #> ~s(season, bs = \"cc\", k = 5, by = trend) #> <environment: 0x5578e326c778> #>  #> Family: #> gaussian #>  #> Link function: #> identity #>  #> Trend model: #> CAR() #>  #>  #> N process models: #> 2  #>  #> N series: #> 2  #>  #> N timepoints: #> 125  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> Observation error parameter estimates: #>              2.5%  50% 97.5% Rhat n_eff #> sigma_obs[1] 0.21 0.49  0.82 1.05    24 #> sigma_obs[2] 0.29 0.53  0.82 1.06    31 #>  #> GAM observation model coefficient (beta) estimates: #>             2.5% 50% 97.5% Rhat n_eff #> (Intercept)    0   0     0  NaN   NaN #>  #> Process model AR parameter estimates: #>        2.5%  50% 97.5% Rhat n_eff #> ar1[1] 0.58 0.73  0.83    1   969 #> ar1[2] 0.71 0.80  0.88    1   888 #>  #> Process error parameter estimates: #>          2.5% 50% 97.5% Rhat n_eff #> sigma[1]  2.4 2.7   3.2    1   509 #> sigma[2]  3.0 3.4   3.9    1  1116 #>  #> GAM process model coefficient (beta) estimates: #>                                2.5%      50% 97.5% Rhat n_eff #> (Intercept)_trend             -0.72  0.00066  0.75 1.00  1292 #> s(season):trendtrend1.1_trend -0.67  0.06400  0.80 1.00   792 #> s(season):trendtrend1.2_trend -1.20 -0.21000  0.57 1.00   841 #> s(season):trendtrend1.3_trend -1.90 -0.34000  0.30 1.00   294 #> s(season):trendtrend2.1_trend -0.57  0.11000  1.00 1.00   866 #> s(season):trendtrend2.2_trend -1.20 -0.15000  0.65 1.00   668 #> s(season):trendtrend2.3_trend -1.40 -0.21000  0.39 1.01   543 #>  #> Approximate significance of GAM process smooths: #>                         edf Ref.df Chi.sq p-value #> s(season):seriestrend1 1.84      3   6.55    0.85 #> s(season):seriestrend2 1.36      3   3.75    0.94 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model conditional_effects(mod, type = 'expected')  plot(mod, type = 'trend', series = 1)  plot(mod, type = 'trend', series = 2)  plot(mod, type = 'residuals', series = 1)  plot(mod, type = 'residuals', series = 2)  mcmc_plot(mod,          variable = 'ar1',          regex = TRUE,          type = 'hist') #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # Now an example illustrating hierarchical dynamics set.seed(123) # Simulate three species monitored in three different # regions, where dynamics can potentially vary across regions simdat1 <- sim_mvgam(trend_model = VAR(cor = TRUE),                     prop_trend = 0.95,                     n_series = 3,                     mu = c(1, 2, 3)) simdat2 <- sim_mvgam(trend_model = VAR(cor = TRUE),                     prop_trend = 0.95,                     n_series = 3,                     mu = c(1, 2, 3)) simdat3 <- sim_mvgam(trend_model = VAR(cor = TRUE),                     prop_trend = 0.95,                     n_series = 3,                     mu = c(1, 2, 3))  # Set up the data but DO NOT include 'series' all_dat <- rbind(simdat1$data_train %>%                   dplyr::mutate(region = 'qld'),                 simdat2$data_train %>%                   dplyr::mutate(region = 'nsw'),                 simdat3$data_train %>%                   dplyr::mutate(region = 'vic')) %>% dplyr::mutate(species = gsub('series', 'species', series),               species = as.factor(species),               region = as.factor(region)) %>% dplyr::arrange(series, time) %>% dplyr::select(-series)  # Check priors for a hierarchical AR1 model get_mvgam_priors(formula = y ~ species,                 trend_model = AR(gr = region, subgr = species),                 data = all_dat) #>                                param_name param_length #> 1                             (Intercept)            1 #> 2                        speciesspecies_2            1 #> 3                        speciesspecies_3            1 #> 4 vector<lower=-1,upper=1>[n_series] ar1;            9 #> 5        vector<lower=0>[n_series] sigma;            9 #>                      param_info                                  prior #> 1                   (Intercept)  (Intercept) ~ student_t(3, 1.9, 2.5); #> 2 speciesspecies_2 fixed effect speciesspecies_2 ~ student_t(3, 0, 2); #> 3 speciesspecies_3 fixed effect speciesspecies_3 ~ student_t(3, 0, 2); #> 4         trend AR1 coefficient                    ar1 ~ std_normal(); #> 5                      trend sd       sigma ~ inv_gamma(1.418, 0.452); #>                     example_change new_lowerbound new_upperbound #> 1      (Intercept) ~ normal(0, 1);             NA             NA #> 2 speciesspecies_2 ~ normal(0, 1);             NA             NA #> 3 speciesspecies_3 ~ normal(0, 1);             NA             NA #> 4       ar1 ~ normal(-0.79, 0.86);             NA             NA #> 5       sigma ~ exponential(0.37);             NA             NA  # Fit the model mod <- mvgam(formula = y ~ species,             trend_model = AR(gr = region, subgr = species),             data = all_dat,             chains = 2,             silent = 2)  # Check standard outputs summary(mod) #> GAM formula: #> y ~ species #> <environment: 0x5578e326c778> #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> AR(gr = region, subgr = species) #>  #>  #> N series: #> 9  #>  #> N timepoints: #> 75  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM coefficient (beta) estimates: #>                  2.5%  50% 97.5% Rhat n_eff #> (Intercept)      0.92 1.10   1.3 1.00   216 #> speciesspecies_2 0.79 0.99   1.2 1.00   268 #> speciesspecies_3 1.60 1.80   2.1 1.02   107 #>  #> Latent trend parameter AR estimates: #>             2.5%    50%  97.5% Rhat n_eff #> ar1[1]    0.2700  0.500 0.7200 1.01    98 #> ar1[2]    0.1300  0.340 0.5200 1.01   146 #> ar1[3]    0.0570  0.360 0.6800 1.05    46 #> ar1[4]    0.3000  0.630 0.8500 1.01   132 #> ar1[5]   -0.1400  0.086 0.3000 1.03   117 #> ar1[6]   -0.4200 -0.230 0.0021 1.03   114 #> ar1[7]    0.0015  0.320 0.6500 1.00   154 #> ar1[8]    0.5700  0.750 0.9100 1.01   223 #> ar1[9]    0.3300  0.540 0.8000 1.02    87 #> sigma[1]  0.7900  0.990 1.3000 1.00   447 #> sigma[2]  0.6400  0.780 0.9500 1.00   381 #> sigma[3]  0.8000  0.960 1.2000 1.00   817 #> sigma[4]  0.3100  0.470 0.6900 1.02    86 #> sigma[5]  0.6100  0.730 0.8800 1.00   564 #> sigma[6]  0.6700  0.780 0.9300 1.00   753 #> sigma[7]  0.5900  0.760 0.9800 1.00   294 #> sigma[8]  0.5400  0.690 0.8800 1.00   382 #> sigma[9]  0.6800  0.820 0.9800 1.00   762 #>  #> Hierarchical correlation weighting parameter (alpha_cor) estimates: #>            2.5%   50% 97.5% Rhat n_eff #> alpha_cor 0.014 0.072   0.2    1   519 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model conditional_effects(mod, type = 'link')   # Inspect posterior estimates for the correlation weighting parameter mcmc_plot(mod, variable = 'alpha_cor', type = 'hist') #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/ZMVN.html","id":null,"dir":"Reference","previous_headings":"","what":"Specify correlated residual processes in mvgam — ZMVN","title":"Specify correlated residual processes in mvgam — ZMVN","text":"Set latent correlated multivariate Gaussian residual processes mvgam. function evaluate arguments – exists purely help set model particular error processes.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ZMVN.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specify correlated residual processes in mvgam — ZMVN","text":"","code":"ZMVN(unit = time, gr = NA, subgr = series)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/ZMVN.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specify correlated residual processes in mvgam — ZMVN","text":"unit unquoted name variable represents unit analysis data latent residuals correlated. variable either numeric integer variable supplied data. Defaults time consistent functionalities mvgam, though note data need time series case. See examples details explanations gr optional grouping variable, must factor supplied data, setting hierarchical residual correlation structures. specified, automatically set model residual correlations specific level gr modelled hierarchically: \\(\\Omega_{group} = p\\Omega_{global} + (1 - p)\\Omega_{group, local}\\), \\(\\Omega_{global}\\) global correlation matrix, \\(\\Omega_{group, local}\\) local deviation correlation matrix \\(p\\) weighting parameter controlling strongly local correlation matrix \\(\\Omega_{group}\\) shrunk towards global correlation matrix \\(\\Omega_{global}\\). gr supplied subgr must also supplied subgr subgrouping factor variable specifying element data represents different observational units. Defaults series consistent functionalities mvgam, though note data need time series case. note models use hierarchical correlations (supplying value gr) include series element data. Rather, element created internally based supplied variables gr subgr. example, modelling counts group species (labelled species data) across sampling sites (labelled site data) three different geographical regions (labelled region), like residuals correlated within regions, specify unit = site,  gr = region, subgr = species. Internally, mvgam() appropriately order data unit (case, site) create series element data using something like: series = .factor(paste0(group, '_', subgroup))","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ZMVN.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specify correlated residual processes in mvgam — ZMVN","text":"object class mvgam_trend, contains list arguments interpreted parsing functions mvgam","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ZMVN.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Specify correlated residual processes in mvgam — ZMVN","text":"","code":"# \\donttest{ # Simulate counts of four species over ten sampling locations site_dat <- data.frame(site = rep(1:10, 4),                       species = as.factor(sort(rep(letters[1:4], 10))),                       y = c(NA, rpois(39, 3))) head(site_dat) #>   site species  y #> 1    1       a NA #> 2    2       a  1 #> 3    3       a  3 #> 4    4       a  4 #> 5    5       a  4 #> 6    6       a  1  # Set up a correlated residual (i.e. Joint Species Distribution) model, # where 'site' represents the unit of analysis trend_model <- ZMVN(unit = site, subgr = species) mod <- mvgam(y ~ species,             trend_model = ZMVN(unit = site,                                subgr = species),             data = site_dat,             chains = 2,             silent = 2)  # Inspect the estimated species-species residual covariances mcmc_plot(mod, variable = 'Sigma', regex = TRUE, type = 'hist') #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # A hierarchical correlation example; set up correlated counts # for three species across two sampling locations Sigma <- matrix(c(1, -0.4, 0.5,                  -0.4, 1, 0.3,                  0.5, 0.3, 1),                byrow = TRUE,                nrow = 3) Sigma #>      [,1] [,2] [,3] #> [1,]  1.0 -0.4  0.5 #> [2,] -0.4  1.0  0.3 #> [3,]  0.5  0.3  1.0  make_site_dat = function(...){  errors <- mgcv::rmvn(n = 30,                       mu = c(0.6, 0.8, 1.8),                       V = Sigma)  site_dat <- do.call(rbind, lapply(1:3, function(spec){    data.frame(y = rpois(30,                         lambda = exp(errors[, spec])),               species = paste0('species',                                spec),               site = 1:30) })) site_dat }  site_dat <- rbind(make_site_dat() %>%                    dplyr::mutate(group = 'group1'),                  make_site_dat() %>%                    dplyr::mutate(group = 'group2')) %>%    dplyr::mutate(species = as.factor(species),                  group = as.factor(group))  # Fit the hierarchical correlated residual model mod <- mvgam(y ~ species,             trend_model = ZMVN(unit = site,                                gr = group,                                subgr = species),             data = site_dat) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 4 chains, at most 3 in parallel... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 3 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 3 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 3 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 3 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 3 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 2.6 seconds. #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 4 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 3 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 3 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 3 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 3 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 4 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 3 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 finished in 4.1 seconds. #> Chain 3 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 4 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 3 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 3 finished in 4.5 seconds. #> Chain 4 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 4 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 4 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 4 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 4 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 4 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 4 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 4 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 4 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 4 finished in 3.4 seconds. #>  #> All 4 chains finished successfully. #> Mean chain execution time: 3.7 seconds. #> Total execution time: 6.3 seconds. #>   # Inspect the estimated species-species residual covariances mcmc_plot(mod, variable = 'Sigma', regex = TRUE, type = 'hist') #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/add_residuals.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate randomized quantile residuals for mvgam objects — add_residuals.mvgam","title":"Calculate randomized quantile residuals for mvgam objects — add_residuals.mvgam","text":"Calculate randomized quantile residuals mvgam objects","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/add_residuals.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate randomized quantile residuals for mvgam objects — add_residuals.mvgam","text":"","code":"add_residuals(object, ...)  # S3 method for mvgam add_residuals(object, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/add_residuals.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate randomized quantile residuals for mvgam objects — add_residuals.mvgam","text":"object list object class mvgam. See mvgam() ... unused","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/add_residuals.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate randomized quantile residuals for mvgam objects — add_residuals.mvgam","text":"list object class mvgam residuals included 'resids' slot","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/add_residuals.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate randomized quantile residuals for mvgam objects — add_residuals.mvgam","text":"series, randomized quantile (.e. Dunn-Smyth) residuals calculated inspecting model diagnostics fitted model appropriate Dunn-Smyth residuals standard normal distribution autocorrelation evident. particular observation missing, residual calculated comparing independent draws model's posterior distribution","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/all_neon_tick_data.html","id":null,"dir":"Reference","previous_headings":"","what":"NEON Amblyomma and Ixodes tick abundance survey data — all_neon_tick_data","title":"NEON Amblyomma and Ixodes tick abundance survey data — all_neon_tick_data","text":"dataset containing timeseries Amblyomma americanum Ixodes scapularis nymph abundances NEON sites","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/all_neon_tick_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"NEON Amblyomma and Ixodes tick abundance survey data — all_neon_tick_data","text":"","code":"all_neon_tick_data"},{"path":"https://nicholasjclark.github.io/mvgam/reference/all_neon_tick_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"NEON Amblyomma and Ixodes tick abundance survey data — all_neon_tick_data","text":"tibble/dataframe containing covariate information alongside main fields : Year Year sampling epiWeek Epidemiological week sampling plot_ID NEON plot ID survey location siteID NEON site ID survey location amblyomma_americanum Counts . americanum nymphs ixodes_scapularis Counts . scapularis nymphs","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/all_neon_tick_data.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"NEON Amblyomma and Ixodes tick abundance survey data — all_neon_tick_data","text":"https://www.neonscience.org/data","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/augment.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Augment an mvgam object's data — augment.mvgam","title":"Augment an mvgam object's data — augment.mvgam","text":"Add fits residuals data, implementing generic augment package broom.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/augment.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Augment an mvgam object's data — augment.mvgam","text":"","code":"# S3 method for mvgam augment(x, robust = FALSE, probs = c(0.025, 0.975), ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/augment.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Augment an mvgam object's data — augment.mvgam","text":"x object class mvgam. robust FALSE (default) mean used measure central tendency standard deviation measure variability. TRUE, median median absolute deviation (MAD) applied instead. probs percentiles computed quantile function. ... Unused, included generic consistency .","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/augment.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Augment an mvgam object's data — augment.mvgam","text":"list tibble (see details) combining: data supplied mvgam(). outcome variable, named .observed. fitted backcasts, along variability credible bounds. residuals, along variability credible bounds.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/augment.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Augment an mvgam object's data — augment.mvgam","text":"list returned class(x$obs_data) == 'list', otherwise tibble returned, contents either object . arguments robust probs applied fit residuals calls (see fitted.mvgam() residuals.mvgam() details).","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/augment.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Augment an mvgam object's data — augment.mvgam","text":"","code":"# \\donttest{ set.seed(0) dat <- sim_mvgam(T = 80,                  n_series = 3,                  mu = 2,                  trend_model = AR(p = 1),                  prop_missing = 0.1,                  prop_trend = 0.6)  mod1 <- mvgam(formula = y ~ s(season, bs = 'cc', k = 6),               data = dat$data_train,               trend_model = AR(),               family = poisson(),               noncentred = TRUE,               chains = 2,               silent = 2)  augment(mod1, robust = TRUE, probs = c(0.25, 0.75)) #> # A tibble: 180 × 14 #>        y season  year series    time .observed .fitted .fit.variability #>    <int>  <int> <int> <fct>    <int>     <int>   <dbl>            <dbl> #>  1     4      1     1 series_1     1         4    4.51             3.08 #>  2    NA      1     1 series_2     1        NA    6.10             3.52 #>  3     4      1     1 series_3     1         4    4.48             2.94 #>  4     5      2     1 series_1     2         5    4.53             3.04 #>  5     2      2     1 series_2     2         2    3.65             3.40 #>  6    NA      2     1 series_3     2        NA    4.56             3.41 #>  7     7      3     1 series_1     3         7    8.22             3.84 #>  8    12      3     1 series_2     3        12   11.4              5.73 #>  9     4      3     1 series_3     3         4    5.06             3.09 #> 10    39      4     1 series_1     4        39   35.6             28.1  #> # ℹ 170 more rows #> # ℹ 6 more variables: .fit.cred.low <dbl>, .fit.cred.high <dbl>, .resid <dbl>, #> #   .resid.variability <dbl>, .resid.cred.low <dbl>, .resid.cred.high <dbl> # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/code.html","id":null,"dir":"Reference","previous_headings":"","what":"Stan code and data objects for mvgam models — code","title":"Stan code and data objects for mvgam models — code","text":"Generate Stan code data objects mvgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stan code and data objects for mvgam models — code","text":"","code":"code(object)  # S3 method for mvgam_prefit stancode(object, ...)  # S3 method for mvgam stancode(object, ...)  # S3 method for mvgam_prefit standata(object, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/code.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stan code and data objects for mvgam models — code","text":"object object class mvgam mvgam_prefit, returned call mvgam ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/code.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stan code and data objects for mvgam models — code","text":"Either character string containing fully commented Stan code fit mvgam model named list containing data objects needed fit model Stan.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/code.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stan code and data objects for mvgam models — code","text":"","code":"# \\donttest{ simdat <- sim_mvgam() mod <- mvgam(y ~ s(season) +                s(time, by = series),              family = poisson(),              data = simdat$data_train,              run_model = FALSE)  # View Stan model code stancode(mod) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[9, 18] S1; // mgcv smooth penalty matrix S1 #>   matrix[9, 18] S2; // mgcv smooth penalty matrix S2 #>   matrix[9, 18] S3; // mgcv smooth penalty matrix S3 #>   matrix[9, 18] S4; // mgcv smooth penalty matrix S4 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : num_basis] = b_raw[1 : num_basis]; #> } #> model { #>   // prior for (Intercept)... #>   b_raw[1] ~ student_t(3, 0, 2.5); #>    #>   // prior for s(season)... #>   b_raw[2 : 10] ~ multi_normal_prec(zero[2 : 10], #>                                     S1[1 : 9, 1 : 9] * lambda[1] #>                                     + S1[1 : 9, 10 : 18] * lambda[2]); #>    #>   // prior for s(time):seriesseries_1... #>   b_raw[11 : 19] ~ multi_normal_prec(zero[11 : 19], #>                                      S2[1 : 9, 1 : 9] * lambda[3] #>                                      + S2[1 : 9, 10 : 18] * lambda[4]); #>    #>   // prior for s(time):seriesseries_2... #>   b_raw[20 : 28] ~ multi_normal_prec(zero[20 : 28], #>                                      S3[1 : 9, 1 : 9] * lambda[5] #>                                      + S3[1 : 9, 10 : 18] * lambda[6]); #>    #>   // prior for s(time):seriesseries_3... #>   b_raw[29 : 37] ~ multi_normal_prec(zero[29 : 37], #>                                      S4[1 : 9, 1 : 9] * lambda[7] #>                                      + S4[1 : 9, 10 : 18] * lambda[8]); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>   { #>     // likelihood functions #>     flat_ys ~ poisson_log_glm(flat_xs, 0.0, b); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   array[n, n_series] int ypred; #>   rho = log(lambda); #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> } #>  #>   # View Stan model data sdata <- standata(mod) str(sdata) #> List of 21 #>  $ y           : num [1:75, 1:3] 3 7 2 1 0 0 0 1 1 0 ... #>  $ n           : int 75 #>  $ X           : num [1:225, 1:37] 1 1 1 1 1 1 1 1 1 1 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:37] \"X.Intercept.\" \"V2\" \"V3\" \"V4\" ... #>  $ S1          : num [1:9, 1:18] 3.758 -0.625 1.816 0.191 2.357 ... #>  $ zero        : num [1:37] 0 0 0 0 0 0 0 0 0 0 ... #>  $ S2          : num [1:9, 1:18] 8.555 1.22 4.352 -0.822 -5.594 ... #>  $ S3          : num [1:9, 1:18] 8.555 1.22 4.352 -0.822 -5.594 ... #>  $ S4          : num [1:9, 1:18] 8.555 1.22 4.352 -0.822 -5.594 ... #>  $ p_coefs     : Named num 0 #>   ..- attr(*, \"names\")= chr \"(Intercept)\" #>  $ p_taus      : num 1.04 #>  $ ytimes      : int [1:75, 1:3] 1 4 7 10 13 16 19 22 25 28 ... #>  $ n_series    : int 3 #>  $ sp          : Named num [1:8] 0.368 0.368 0.368 0.368 0.368 ... #>   ..- attr(*, \"names\")= chr [1:8] \"s(season)1\" \"s(season)2\" \"s(time):seriesseries_11\" \"s(time):seriesseries_12\" ... #>  $ y_observed  : num [1:75, 1:3] 1 1 1 1 1 1 1 1 1 1 ... #>  $ total_obs   : int 225 #>  $ num_basis   : int 37 #>  $ n_sp        : num 8 #>  $ n_nonmissing: int 225 #>  $ obs_ind     : int [1:225] 1 2 3 4 5 6 7 8 9 10 ... #>  $ flat_ys     : num [1:225] 3 7 2 1 0 0 0 1 1 0 ... #>  $ flat_xs     : num [1:225, 1:37] 1 1 1 1 1 1 1 1 1 1 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:37] \"X.Intercept.\" \"V2\" \"V3\" \"V4\" ... #>  - attr(*, \"trend_model\")= chr \"None\" # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/conditional_effects.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Display conditional effects of predictors for mvgam models — conditional_effects.mvgam","title":"Display conditional effects of predictors for mvgam models — conditional_effects.mvgam","text":"Display conditional effects one numeric /categorical predictors models class mvgam jsdgam, including two-way interaction effects.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/conditional_effects.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Display conditional effects of predictors for mvgam models — conditional_effects.mvgam","text":"","code":"# S3 method for mvgam conditional_effects(   x,   effects = NULL,   type = \"expected\",   points = FALSE,   rug = FALSE,   ... )  # S3 method for mvgam_conditional_effects plot(x, plot = TRUE, ask = FALSE, ...)  # S3 method for mvgam_conditional_effects print(x, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/conditional_effects.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Display conditional effects of predictors for mvgam models — conditional_effects.mvgam","text":"x Object class mvgam, jsdgam mvgam_conditional_effects effects optional character vector naming effects (main effects interactions) compute conditional plots. Interactions specified : variable names. NULL (default), plots generated main effects two-way interactions estimated model. specifying effects manually, two-way interactions (including grouping variables) may plotted even originally modeled. type character specifying scale predictions. value link linear predictor calculated link scale. expected used (default), predictions reflect expectation response (mean) ignore uncertainty observation process. response used, predictions take uncertainty observation process account return predictions outcome scale. Two special cases also allowed: type latent_N return estimated latent abundances N-mixture distribution, type detection return estimated detection probability N-mixture distribution points Logical. Indicates original data points added, type == 'response'. Default TRUE. rug Logical. Indicates displays tick marks plotted axes mark distribution raw data, type == 'response'. Default TRUE. ... arguments pass plot_predictions plot Logical; indicates plots plotted directly active graphic device. Defaults TRUE. ask Logical. Indicates user prompted new page plotted. used plot TRUE. Default FALSE.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/conditional_effects.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Display conditional effects of predictors for mvgam models — conditional_effects.mvgam","text":"conditional_effects returns object class mvgam_conditional_effects named list one slot per effect containing ggplot object, can customized using ggplot2 package. corresponding plot method draw plots active graphic device","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/conditional_effects.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Display conditional effects of predictors for mvgam models — conditional_effects.mvgam","text":"function acts wrapper flexible plot_predictions. creating conditional_effects particular predictor (interaction two predictors), one choose values predictors condition . default, mean used continuous variables reference category used factors. Use plot_predictions change create bespoke conditional effects plots.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/conditional_effects.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Display conditional effects of predictors for mvgam models — conditional_effects.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/conditional_effects.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Display conditional effects of predictors for mvgam models — conditional_effects.mvgam","text":"","code":"# \\donttest{ # Simulate some data simdat <- sim_mvgam(family = poisson(),                     seasonality = 'hierarchical')  # Fit a model mod <- mvgam(y ~ s(season, by = series, k = 5) + year:series,              family = poisson(),              data = simdat$data_train,              chains = 2) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 0.7 seconds. #> Chain 2 finished in 0.6 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 0.7 seconds. #> Total execution time: 0.8 seconds. #>   # Plot all main effects on the response scale conditional_effects(mod)    # Change the prediction interval to 70% using plot_predictions() argument # 'conf_level' conditional_effects(mod, conf_level = 0.7)    # Plot all main effects on the link scale conditional_effects(mod, type = 'link')    # Works the same for smooth terms, including smooth interactions set.seed(0) dat <- mgcv::gamSim(1, n = 200, scale = 2) #> Gu & Wahba 4 term additive model mod <- mvgam(y ~ te(x0, x1, k = 5) + s(x2, k = 6) + s(x3, k = 6),             data = dat,             family = gaussian(),             chains = 2) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 finished in 3.6 seconds. #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 3.8 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 3.7 seconds. #> Total execution time: 3.9 seconds. #>  conditional_effects(mod)    conditional_effects(mod, conf_level = 0.5, type = 'link')     # ggplot objects can be modified and combined with the help of many # additional packages. Here is an example using the patchwork package  # Simulate some nonlinear data dat <- mgcv::gamSim(1, n = 200, scale = 2) #> Gu & Wahba 4 term additive model mod <- mvgam(y ~ s(x1, bs = 'moi') +                te(x0, x2),              data = dat,              family = gaussian(),              chains = 2,              silent = 2)  # Extract the list of ggplot conditional_effect plots m <- plot(conditional_effects(mod), plot = FALSE)  # Add custom labels and arrange plots together using patchwork::wrap_plots() library(patchwork) library(ggplot2) wrap_plots(m[[1]] + labs(title = 's(x1, bs = \"moi\")'),            m[[2]] + labs(title = 'te(x0, x2)'))  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/dynamic.html","id":null,"dir":"Reference","previous_headings":"","what":"Defining dynamic coefficients in mvgam formulae — dynamic","title":"Defining dynamic coefficients in mvgam formulae — dynamic","text":"Set time-varying (dynamic) coefficients use mvgam models. Currently, low-rank Gaussian Process smooths available estimating dynamics time-varying coefficient.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/dynamic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Defining dynamic coefficients in mvgam formulae — dynamic","text":"","code":"dynamic(variable, k, rho = 5, stationary = TRUE, scale = TRUE)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/dynamic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Defining dynamic coefficients in mvgam formulae — dynamic","text":"variable variable dynamic smooth function k Optional number basis functions computing approximate GPs. missing, k set large possible accurately estimate nonlinear function rho Either positive numeric stating length scale used approximating squared exponential Gaussian Process smooth (see gp.smooth details) missing, case length scale estimated setting Hilbert space approximate GP stationary Logical. TRUE (default) rho supplied, latent Gaussian Process smooth linear trend component. FALSE, linear trend covariate added Gaussian Process smooth. Leave TRUE believe coefficient evolving much trend, linear component basis functions can hard penalize zero. sometimes causes divergence issues Stan. See gp.smooth details. Ignored rho missing (case Hilbert space approximate GP used) scale Logical; TRUE (default) rho missing, predictors scaled maximum Euclidean distance two points 1. often improves sampling speed convergence. Scaling also affects estimated length-scale parameters resemble scaled predictors (original predictors) scale TRUE.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/dynamic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Defining dynamic coefficients in mvgam formulae — dynamic","text":"list object internal usage 'mvgam'","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/dynamic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Defining dynamic coefficients in mvgam formulae — dynamic","text":"mvgam currently sets dynamic coefficients low-rank squared exponential Gaussian Process smooths via call s(time, = variable, bs = \"gp\", m = c(2, rho, 2)). smooths, specified reasonable values length scale parameter, give realistic sample forecasts standard splines thin plate cubic. user must set value rho, currently support estimating value mgcv. may big problem, estimating latent length scales often difficult anyway. rho parameter thought prior smoothness latent dynamic coefficient function (higher values rho lead smoother functions temporal covariance structure. Values k set automatically ensure enough basis functions used approximate expected wiggliness underlying dynamic function (k increase rho decreases)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/dynamic.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Defining dynamic coefficients in mvgam formulae — dynamic","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/dynamic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Defining dynamic coefficients in mvgam formulae — dynamic","text":"","code":"# \\donttest{ # Simulate a time-varying coefficient # (as a Gaussian Process with length scale = 10) set.seed(1111) N <- 200  # A function to simulate from a squared exponential Gaussian Process sim_gp = function(N, c, alpha, rho){  Sigma <- alpha ^ 2 *           exp(-0.5 * ((outer(1:N, 1:N, \"-\") / rho) ^ 2)) +           diag(1e-9, N) c + mgcv::rmvn(1,                mu = rep(0, N),                V = Sigma) }  beta <- sim_gp(alpha = 0.75,               rho = 10,               c = 0.5,               N = N) plot(beta, type = 'l', lwd = 3,     bty = 'l', xlab = 'Time',     ylab = 'Coefficient',     col = 'darkred')   # Simulate the predictor as a standard normal predictor <- rnorm(N, sd = 1)  # Simulate a Gaussian outcome variable out <- rnorm(N, mean = 4 + beta * predictor,             sd = 0.25) time <- seq_along(predictor) plot(out,  type = 'l', lwd = 3,     bty = 'l', xlab = 'Time', ylab = 'Outcome',     col = 'darkred')   # Gather into a data.frame and fit a dynamic coefficient model data <- data.frame(out, predictor, time)  # Split into training and testing data_train <- data[1:190,] data_test <- data[191:200,]  # Fit a model using the dynamic function mod <- mvgam(out ~              # mis-specify the length scale slightly as this              # won't be known in practice              dynamic(predictor, rho = 8, stationary = TRUE),             family = gaussian(),             data = data_train,             chains = 2,             silent = 2)  # Inspect the summary summary(mod) #> GAM formula: #> out ~ s(time, by = predictor, bs = \"gp\", m = c(-2, 8, 2), k = 27) #> <environment: 0x5578ecf5c740> #>  #> Family: #> gaussian #>  #> Link function: #> identity #>  #> Trend model: #> None #>  #> N series: #> 1  #>  #> N timepoints: #> 190  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> Observation error parameter estimates: #>              2.5%  50% 97.5% Rhat n_eff #> sigma_obs[1] 0.22 0.24  0.27    1   736 #>  #> GAM coefficient (beta) estimates: #>                         2.5%    50%  97.5% Rhat n_eff #> (Intercept)           3.9000  4.000  4.000    1   864 #> s(time):predictor.1  -0.4900 -0.020  0.480    1   146 #> s(time):predictor.2   0.6200  0.660  0.710    1   915 #> s(time):predictor.3   0.1700  0.330  0.500    1   146 #> s(time):predictor.4  -0.3400 -0.290 -0.240    1   939 #> s(time):predictor.5   0.0200  0.140  0.250    1   150 #> s(time):predictor.6  -0.7100 -0.670 -0.620    1   857 #> s(time):predictor.7  -0.3600 -0.270 -0.170    1   177 #> s(time):predictor.8  -0.2300 -0.180 -0.130    1   811 #> s(time):predictor.9   0.1500  0.250  0.330    1   199 #> s(time):predictor.10 -0.2000 -0.130 -0.065    1   789 #> s(time):predictor.11  0.0140  0.100  0.200    1   230 #> s(time):predictor.12 -0.4400 -0.360 -0.280    1   567 #> s(time):predictor.13 -0.0035  0.110  0.220    1   324 #> s(time):predictor.14 -0.2500 -0.160 -0.074    1   962 #> s(time):predictor.15 -0.0650  0.046  0.170    1   373 #> s(time):predictor.16 -0.0026  0.120  0.240    1   734 #> s(time):predictor.17  0.0780  0.220  0.380    1   384 #> s(time):predictor.18 -0.1100  0.043  0.190    1   714 #> s(time):predictor.19  0.0250  0.200  0.370    1   490 #> s(time):predictor.20 -0.2300 -0.035  0.170    1  1068 #> s(time):predictor.21 -0.2000  0.039  0.290    1   653 #> s(time):predictor.22 -0.4400 -0.170  0.120    1   899 #> s(time):predictor.23 -0.0280  0.250  0.570    1   957 #> s(time):predictor.24 -0.4300 -0.110  0.240    1   781 #> s(time):predictor.25 -0.2500  0.160  0.550    1   718 #> s(time):predictor.26 -0.6300 -0.220  0.190    1   821 #> s(time):predictor.27 -0.0840  0.460  1.000    1   146 #>  #> Approximate significance of GAM smooths: #>                    edf Ref.df Chi.sq p-value     #> s(time):predictor 12.4     27    294  <2e-16 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model  # Plot the time-varying coefficient estimates plot(mod, type = 'smooths')   # Extrapolate the coefficient forward in time plot_mvgam_smooth(mod, smooth = 1, newdata = data) abline(v = 190, lty = 'dashed', lwd = 2)  # Overlay the true simulated time-varying coefficient lines(beta, lwd = 2.5, col = 'white') lines(beta, lwd = 2)  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/ensemble.mvgam_forecast.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine forecasts from mvgam models into evenly weighted ensembles — ensemble.mvgam_forecast","title":"Combine forecasts from mvgam models into evenly weighted ensembles — ensemble.mvgam_forecast","text":"Generate evenly weighted ensemble forecast distributions mvgam_forecast objects","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ensemble.mvgam_forecast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine forecasts from mvgam models into evenly weighted ensembles — ensemble.mvgam_forecast","text":"","code":"ensemble(object, ...)  # S3 method for mvgam_forecast ensemble(object, ..., ndraws = 5000)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/ensemble.mvgam_forecast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine forecasts from mvgam models into evenly weighted ensembles — ensemble.mvgam_forecast","text":"object list object class mvgam_forecast. See forecast.mvgam() ... mvgam_forecast objects. ndraws Positive integer specifying number draws use forecast distribution creating ensemble. ensemble members fewer draws ndraws, forecast distributions resampled replacement achieve correct number draws","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ensemble.mvgam_forecast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine forecasts from mvgam models into evenly weighted ensembles — ensemble.mvgam_forecast","text":"object class mvgam_forecast containing ensemble predictions. object can readily used supplied S3 functions plot score","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ensemble.mvgam_forecast.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Combine forecasts from mvgam models into evenly weighted ensembles — ensemble.mvgam_forecast","text":"widely recognised forecasting literature combining forecasts different models often results improved forecast accuracy. simplest way create ensemble use evenly weighted combinations forecasts different models. straightforward Bayesian setting mvgam posterior MCMC draws contained mvgam_forecast object already implicitly capture correlations among temporal posterior predictions.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/ensemble.mvgam_forecast.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Combine forecasts from mvgam models into evenly weighted ensembles — ensemble.mvgam_forecast","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ensemble.mvgam_forecast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine forecasts from mvgam models into evenly weighted ensembles — ensemble.mvgam_forecast","text":"","code":"# \\donttest{ # Simulate some series and fit a few competing dynamic models set.seed(1) simdat <- sim_mvgam(n_series = 1,                     prop_trend = 0.6,                     mu = 1)  plot_mvgam_series(data = simdat$data_train,                  newdata = simdat$data_test)   m1 <- mvgam(y ~ 1,             trend_formula = ~ time +               s(season, bs = 'cc', k = 9),             trend_model = AR(p = 1),             noncentred = TRUE,             data = simdat$data_train,             newdata = simdat$data_test,             chains = 2,             silent = 2)  m2 <- mvgam(y ~ time,             trend_model = RW(),             noncentred = TRUE,             data = simdat$data_train,             newdata = simdat$data_test,             chains = 2,             silent = 2)  # Calculate forecast distributions for each model fc1 <- forecast(m1) fc2 <- forecast(m2)  # Generate the ensemble forecast ensemble_fc <- ensemble(fc1, fc2)  # Plot forecasts plot(fc1) #> Out of sample DRPS: #> 42.585823  plot(fc2) #> Out of sample DRPS: #> 47.85449  plot(ensemble_fc) #> Out of sample DRPS: #> 43.86142888   # Score forecasts score(fc1) #> $series_1 #>       score in_interval interval_width eval_horizon score_type #> 1  3.523114           1            0.9            1       crps #> 2  0.932258           1            0.9            2       crps #> 3  2.073581           1            0.9            3       crps #> 4  1.414644           1            0.9            4       crps #> 5  0.996667           1            0.9            5       crps #> 6  1.419490           1            0.9            6       crps #> 7  2.097700           1            0.9            7       crps #> 8  1.610148           1            0.9            8       crps #> 9  1.232733           1            0.9            9       crps #> 10 1.225088           1            0.9           10       crps #> 11 1.347235           1            0.9           11       crps #> 12 1.604694           1            0.9           12       crps #> 13 0.990786           1            0.9           13       crps #> 14 0.966188           1            0.9           14       crps #> 15 0.843817           1            0.9           15       crps #> 16 1.019958           1            0.9           16       crps #> 17 0.899248           1            0.9           17       crps #> 18 4.163395           1            0.9           18       crps #> 19 1.249429           1            0.9           19       crps #> 20 2.031900           1            0.9           20       crps #> 21 1.496291           1            0.9           21       crps #> 22 1.579091           1            0.9           22       crps #> 23 5.095182           0            0.9           23       crps #> 24 1.374279           1            0.9           24       crps #> 25 1.398907           1            0.9           25       crps #>  #> $all_series #>       score eval_horizon score_type #> 1  3.523114            1   sum_crps #> 2  0.932258            2   sum_crps #> 3  2.073581            3   sum_crps #> 4  1.414644            4   sum_crps #> 5  0.996667            5   sum_crps #> 6  1.419490            6   sum_crps #> 7  2.097700            7   sum_crps #> 8  1.610148            8   sum_crps #> 9  1.232733            9   sum_crps #> 10 1.225088           10   sum_crps #> 11 1.347235           11   sum_crps #> 12 1.604694           12   sum_crps #> 13 0.990786           13   sum_crps #> 14 0.966188           14   sum_crps #> 15 0.843817           15   sum_crps #> 16 1.019958           16   sum_crps #> 17 0.899248           17   sum_crps #> 18 4.163395           18   sum_crps #> 19 1.249429           19   sum_crps #> 20 2.031900           20   sum_crps #> 21 1.496291           21   sum_crps #> 22 1.579091           22   sum_crps #> 23 5.095182           23   sum_crps #> 24 1.374279           24   sum_crps #> 25 1.398907           25   sum_crps #>  score(fc2) #> $series_1 #>       score in_interval interval_width eval_horizon score_type #> 1  4.027405           1            0.9            1       crps #> 2  1.601258           1            0.9            2       crps #> 3  3.404619           0            0.9            3       crps #> 4  0.890614           1            0.9            4       crps #> 5  2.538261           1            0.9            5       crps #> 6  2.620900           1            0.9            6       crps #> 7  1.892433           1            0.9            7       crps #> 8  0.918860           1            0.9            8       crps #> 9  1.634557           1            0.9            9       crps #> 10 1.184265           1            0.9           10       crps #> 11 1.456347           1            0.9           11       crps #> 12 1.715809           1            0.9           12       crps #> 13 1.019106           1            0.9           13       crps #> 14 0.957008           1            0.9           14       crps #> 15 1.113928           1            0.9           15       crps #> 16 1.133647           1            0.9           16       crps #> 17 1.101088           1            0.9           17       crps #> 18 2.844725           1            0.9           18       crps #> 19 1.189586           1            0.9           19       crps #> 20 3.681478           1            0.9           20       crps #> 21 2.473191           1            0.9           21       crps #> 22 1.617496           1            0.9           22       crps #> 23 4.132401           1            0.9           23       crps #> 24 1.385082           1            0.9           24       crps #> 25 1.320426           1            0.9           25       crps #>  #> $all_series #>       score eval_horizon score_type #> 1  4.027405            1   sum_crps #> 2  1.601258            2   sum_crps #> 3  3.404619            3   sum_crps #> 4  0.890614            4   sum_crps #> 5  2.538261            5   sum_crps #> 6  2.620900            6   sum_crps #> 7  1.892433            7   sum_crps #> 8  0.918860            8   sum_crps #> 9  1.634557            9   sum_crps #> 10 1.184265           10   sum_crps #> 11 1.456347           11   sum_crps #> 12 1.715809           12   sum_crps #> 13 1.019106           13   sum_crps #> 14 0.957008           14   sum_crps #> 15 1.113928           15   sum_crps #> 16 1.133647           16   sum_crps #> 17 1.101088           17   sum_crps #> 18 2.844725           18   sum_crps #> 19 1.189586           19   sum_crps #> 20 3.681478           20   sum_crps #> 21 2.473191           21   sum_crps #> 22 1.617496           22   sum_crps #> 23 4.132401           23   sum_crps #> 24 1.385082           24   sum_crps #> 25 1.320426           25   sum_crps #>  score(ensemble_fc) #> $series_1 #>        score in_interval interval_width eval_horizon score_type #> 1  3.7299148           1            0.9            1       crps #> 2  1.2316120           1            0.9            2       crps #> 3  2.7003969           1            0.9            3       crps #> 4  1.0364422           1            0.9            4       crps #> 5  1.6282435           1            0.9            5       crps #> 6  1.9640138           1            0.9            6       crps #> 7  1.9986178           1            0.9            7       crps #> 8  1.1822043           1            0.9            8       crps #> 9  1.3414211           1            0.9            9       crps #> 10 1.1086363           1            0.9           10       crps #> 11 1.3902652           1            0.9           11       crps #> 12 1.6876465           1            0.9           12       crps #> 13 0.9988800           1            0.9           13       crps #> 14 0.9523391           1            0.9           14       crps #> 15 0.9620027           1            0.9           15       crps #> 16 0.9439318           1            0.9           16       crps #> 17 0.9259058           1            0.9           17       crps #> 18 3.3335444           1            0.9           18       crps #> 19 1.2523325           1            0.9           19       crps #> 20 2.6802113           1            0.9           20       crps #> 21 1.8845088           1            0.9           21       crps #> 22 1.4960584           1            0.9           22       crps #> 23 4.7087855           1            0.9           23       crps #> 24 1.3857925           1            0.9           24       crps #> 25 1.3377218           1            0.9           25       crps #>  #> $all_series #>        score eval_horizon score_type #> 1  3.7299148            1   sum_crps #> 2  1.2316120            2   sum_crps #> 3  2.7003969            3   sum_crps #> 4  1.0364422            4   sum_crps #> 5  1.6282435            5   sum_crps #> 6  1.9640138            6   sum_crps #> 7  1.9986178            7   sum_crps #> 8  1.1822043            8   sum_crps #> 9  1.3414211            9   sum_crps #> 10 1.1086363           10   sum_crps #> 11 1.3902652           11   sum_crps #> 12 1.6876465           12   sum_crps #> 13 0.9988800           13   sum_crps #> 14 0.9523391           14   sum_crps #> 15 0.9620027           15   sum_crps #> 16 0.9439318           16   sum_crps #> 17 0.9259058           17   sum_crps #> 18 3.3335444           18   sum_crps #> 19 1.2523325           19   sum_crps #> 20 2.6802113           20   sum_crps #> 21 1.8845088           21   sum_crps #> 22 1.4960584           22   sum_crps #> 23 4.7087855           23   sum_crps #> 24 1.3857925           24   sum_crps #> 25 1.3377218           25   sum_crps #>  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/evaluate_mvgams.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate forecasts from fitted mvgam objects — evaluate_mvgams","title":"Evaluate forecasts from fitted mvgam objects — evaluate_mvgams","text":"Evaluate forecasts fitted mvgam objects","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/evaluate_mvgams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate forecasts from fitted mvgam objects — evaluate_mvgams","text":"","code":"eval_mvgam(   object,   n_samples = 5000,   eval_timepoint = 3,   fc_horizon = 3,   n_cores = 1,   score = \"drps\",   log = FALSE,   weights )  roll_eval_mvgam(   object,   n_evaluations = 5,   evaluation_seq,   n_samples = 5000,   fc_horizon = 3,   n_cores = 1,   score = \"drps\",   log = FALSE,   weights )  compare_mvgams(   model1,   model2,   n_samples = 1000,   fc_horizon = 3,   n_evaluations = 10,   n_cores = 1,   score = \"drps\",   log = FALSE,   weights )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/evaluate_mvgams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate forecasts from fitted mvgam objects — evaluate_mvgams","text":"object list object returned mvgam n_samples integer specifying number samples generate model's posterior distribution eval_timepoint integer indexing timepoint represents last 'observed' set outcome data fc_horizon integer specifying length forecast horizon evaluating forecasts n_cores Deprecated. Parallel processing longer supported score character specifying type ranked probability score use evaluation. Options : variogram, drps crps log logical. forecasts truths logged prior scoring? often appropriate comparing performance models series vary observation ranges weights optional vector weights (length(weights) == n_series) weighting pairwise correlations evaluating variogram score multivariate forecasts. Useful -weighting series larger magnitude observations less interest forecasting. Ignored score != 'variogram' n_evaluations integer specifying total number evaluations perform evaluation_seq Optional integer sequence specifying exact set timepoints evaluating model's forecasts. sequence values <3 > max(training timepoints) - fc_horizon model1 list object returned mvgam representing first model evaluated model2 list object returned mvgam representing second model evaluated","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/evaluate_mvgams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate forecasts from fitted mvgam objects — evaluate_mvgams","text":"eval_mvgam, list object containing information specific evaluations series (using drps crps score) vector scores using variogram. roll_eval_mvgam, list object containing information specific evaluations series well total evaluation summary (taken summing forecast score series evaluation averaging coverages evaluation) compare_mvgams, series plots comparing forecast Rank Probability Scores competing model. lower score preferred. Note however possible select model ultimately perform poorly true --sample forecasting. example wiggly smooth function 'year' included model function learned prior evaluating rolling window forecasts, model generate tight predictions result. forecasting ahead timepoints model seen (.e. next year), smooth function end extrapolating, sometimes strange unexpected ways. therefore recommended use smooth functions covariates adequately measured data (.e. 'seasonality', example) reduce possible extrapolation smooths let latent trends mvgam model capture temporal dependencies data. trends time series models provide much stable forecasts","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/evaluate_mvgams.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluate forecasts from fitted mvgam objects — evaluate_mvgams","text":"eval_mvgam may useful repeated fitting model using update.mvgam exact leave-future-cross-validation approximate leave-future-cross-validation using lfo_cv impractical. function generates set samples representing fixed parameters estimated full mvgam model latent trend states given point time. trends rolled forward total fc_horizon timesteps according estimated state space dynamics generate '--sample' forecast evaluated true observations horizon window. function therefore simulates situation model's parameters already estimated observed data evaluation timepoint like generate forecasts latent trends observed timepoint. Evaluation involves calculating appropriate Rank Probability Score binary indicator whether true value lies within forecast's 90% prediction interval roll_eval_mvgam sets sequence evaluation timepoints along rolling window iteratively calls eval_mvgam evaluate '--sample' forecasts. Evaluation involves calculating Rank Probability Scores binary indicator whether true value lies within forecast's 90% prediction interval compare_mvgams automates evaluation compare two fitted models using rolling window forecast evaluation provides series summary plots facilitate model selection. essentially wrapper roll_eval_mvgam","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/evaluate_mvgams.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate forecasts from fitted mvgam objects — evaluate_mvgams","text":"","code":"# \\donttest{ # Simulate from a Poisson-AR2 model with a seasonal smooth set.seed(1) dat <- sim_mvgam(T = 75,                 n_series = 1,                 prop_trend = 0.75,                 trend_model = AR(p = 2),                 family = poisson())   # Fit an appropriate model mod_ar2 <- mvgam(y ~ s(season, bs = 'cc'),                 trend_model = AR(p = 2),                 family = poisson(),                 data = dat$data_train,                 newdata = dat$data_test,                 chains = 2,                 silent = 2)  # Fit a less appropriate model mod_rw <- mvgam(y ~ 1,                trend_model = RW(),                family = poisson(),                data = dat$data_train,                newdata = dat$data_test,                chains = 2,                silent = 2)  # Compare Discrete Ranked Probability Scores for the testing period fc_ar2 <- forecast(mod_ar2) fc_rw <- forecast(mod_rw) score_ar2 <- score(fc_ar2, score = 'drps') score_rw <- score(fc_rw, score = 'drps') sum(score_ar2$series_1$score) #> [1] 39.48068 sum(score_rw$series_1$score) #> [1] 39.65344  # Use rolling evaluation for approximate comparisons of 3-step ahead # forecasts across the training period compare_mvgams(mod_ar2,               mod_rw,               fc_horizon = 3,               n_samples = 1000,               n_evaluations = 5) #> RPS summaries per model (lower is better) #>             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. #> Model 1 0.816925 1.411382 2.005840 2.367583 3.142912 4.279985 #> Model 2 1.842947 2.168967 2.494988 2.523765 2.864174 3.233360 #>  #> 90% interval coverages per model (closer to 0.9 is better) #> Model 1 0.9333333  #> Model 2 1     # Now use approximate leave-future-out CV to compare # rolling forecasts; start at time point 40 to reduce # computational time and to ensure enough data is available # for estimating model parameters lfo_ar2 <- lfo_cv(mod_ar2,                  min_t = 40,                  fc_horizon = 3,                  silent = 2) lfo_rw <- lfo_cv(mod_rw,                 min_t = 40,                 fc_horizon = 3,                 silent = 2)  # Plot Pareto-K values and ELPD estimates plot(lfo_ar2)  plot(lfo_rw)   # Proportion of timepoints in which AR2 model gives # better forecasts length(which((lfo_ar2$elpds - lfo_rw$elpds) > 0)) /       length(lfo_ar2$elpds) #> [1] 0.7692308  # A higher total ELPD is preferred lfo_ar2$sum_ELPD #> [1] -58.77854 lfo_rw$sum_ELPD #> [1] -75.98128 # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/fevd.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate latent VAR forecast error variance decompositions — fevd.mvgam","title":"Calculate latent VAR forecast error variance decompositions — fevd.mvgam","text":"Compute forecast error variance decompositions mvgam models Vector Autoregressive dynamics","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/fevd.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate latent VAR forecast error variance decompositions — fevd.mvgam","text":"","code":"fevd(object, ...)  # S3 method for mvgam fevd(object, h = 10, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/fevd.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate latent VAR forecast error variance decompositions — fevd.mvgam","text":"object list object class mvgam resulting call mvgam() used Vector Autoregressive latent process model (either VAR(cor = FALSE) VAR(cor = TRUE); see VAR() details) ... ignored h Positive integer specifying forecast horizon calculate IRF","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/fevd.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate latent VAR forecast error variance decompositions — fevd.mvgam","text":"See mvgam_fevd-class full description quantities computed returned function, along key references.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/fevd.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate latent VAR forecast error variance decompositions — fevd.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/fevd.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate latent VAR forecast error variance decompositions — fevd.mvgam","text":"","code":"# \\donttest{ # Simulate some time series that follow a latent VAR(1) process simdat <- sim_mvgam(   family = gaussian(),   n_series = 4,   trend_model = VAR(cor = TRUE),   prop_trend = 1 ) plot_mvgam_series(data = simdat$data_train, series = \"all\")   # Fit a model that uses a latent VAR(1) mod <- mvgam(   formula = y ~ -1,   trend_formula = ~ 1,   trend_model = VAR(cor = TRUE),   family = gaussian(),   data = simdat$data_train,   chains = 2,   silent = 2 )  # Plot the autoregressive coefficient distributions; # use 'dir = \"v\"' to arrange the order of facets # correctly mcmc_plot(   mod,   variable = 'A',   regex = TRUE,   type = 'hist',   facet_args = list(dir = 'v') ) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # Calulate forecast error variance decompositions for each series fevds <- fevd(mod, h = 12)  # Plot median contributions to forecast error variance plot(fevds)   # View a summary of the error variance decompositions summary(fevds) #> # A tibble: 192 × 5 #>    shock                  horizon fevdQ50 fevdQ2.5 fevdQ97.5 #>    <chr>                    <int>   <dbl>    <dbl>     <dbl> #>  1 Process_1 -> Process_1       1   1        1         1     #>  2 Process_1 -> Process_1       2   0.861    0.500     0.986 #>  3 Process_1 -> Process_1       3   0.763    0.376     0.969 #>  4 Process_1 -> Process_1       4   0.708    0.313     0.957 #>  5 Process_1 -> Process_1       5   0.686    0.279     0.950 #>  6 Process_1 -> Process_1       6   0.677    0.271     0.946 #>  7 Process_1 -> Process_1       7   0.672    0.271     0.944 #>  8 Process_1 -> Process_1       8   0.669    0.273     0.942 #>  9 Process_1 -> Process_1       9   0.668    0.269     0.942 #> 10 Process_1 -> Process_1      10   0.668    0.268     0.941 #> # ℹ 182 more rows # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/fitted.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Expected values of the posterior predictive distribution for mvgam objects — fitted.mvgam","title":"Expected values of the posterior predictive distribution for mvgam objects — fitted.mvgam","text":"method extracts posterior estimates fitted values (.e. actual predictions, included estimates trend states, obtained fitting model). also includes option obtaining summaries computed draws.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/fitted.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expected values of the posterior predictive distribution for mvgam objects — fitted.mvgam","text":"","code":"# S3 method for mvgam fitted(   object,   process_error = TRUE,   scale = c(\"response\", \"linear\"),   summary = TRUE,   robust = FALSE,   probs = c(0.025, 0.975),   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/fitted.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expected values of the posterior predictive distribution for mvgam objects — fitted.mvgam","text":"object object class mvgam process_error Logical. TRUE dynamic trend model fit, expected uncertainty process model accounted using draws latent trend SD parameters. FALSE, uncertainty latent trend component ignored calculating predictions scale Either \"response\" \"linear\". \"response\", results returned scale response variable. \"linear\", results returned scale linear predictor term, without applying inverse link function transformations. summary summary statistics returned instead raw values? Default TRUE.. robust FALSE (default) mean used measure central tendency standard deviation measure variability. TRUE, median median absolute deviation (MAD) applied instead. used summary TRUE. probs percentiles computed quantile function. used summary TRUE. ... arguments passed prepare_predictions control several aspects data validation prediction.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/fitted.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expected values of the posterior predictive distribution for mvgam objects — fitted.mvgam","text":"array predicted mean response values. summary = FALSE output resembles posterior_epred.mvgam predict.mvgam. summary = TRUE output n_observations x E matrix. number summary statistics E equal 2 +   length(probs): Estimate column contains point estimates (either mean median depending argument robust), Est.Error column contains uncertainty estimates (either standard deviation median absolute deviation depending argument robust). remaining columns starting Q contain quantile estimates specified via argument probs.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/fitted.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Expected values of the posterior predictive distribution for mvgam objects — fitted.mvgam","text":"method gives actual fitted values model (.e. see generate hindcasts fitted model using hindcast.mvgam type = 'expected'). predictions can overly precise flexible dynamic trend component included model. contrast set predict functions (.e. posterior_epred.mvgam predict.mvgam), assume dynamic trend component reached stationarity returning hypothetical predictions","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/fitted.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expected values of the posterior predictive distribution for mvgam objects — fitted.mvgam","text":"","code":"# \\donttest{ # Simulate some data and fit a model simdat <- sim_mvgam(n_series = 1, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc'),             trend_model = AR(),             data = simdat$data_train,             chains = 2,             silent = 2)  # Extract fitted values (posterior expectations) expectations <- fitted(mod) str(expectations) #>  num [1:75, 1:4] 2.372 3.965 2.663 1.385 0.362 ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : NULL #>   ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\" # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/forecast.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract or compute hindcasts and forecasts for a fitted mvgam object — forecast.mvgam","title":"Extract or compute hindcasts and forecasts for a fitted mvgam object — forecast.mvgam","text":"Extract compute hindcasts forecasts fitted mvgam object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/forecast.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract or compute hindcasts and forecasts for a fitted mvgam object — forecast.mvgam","text":"","code":"# S3 method for mvgam forecast(object, newdata, data_test, n_cores = 1, type = \"response\", ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/forecast.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract or compute hindcasts and forecasts for a fitted mvgam object — forecast.mvgam","text":"object list object class mvgam jsdgam. See mvgam() newdata Optional dataframe list test data containing variables included original data used fit model. included, covariate information newdata used generate forecasts fitted model equations. newdata originally included call mvgam, forecasts already produced generative model simply extracted plotted. However newdata supplied original model call, assumption made newdata supplied comes sequentially data supplied original model (.e. assume time gap last observation series 1 original data first observation series 1 newdata) data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows n_cores Deprecated. Parallel processing longer supported type value link (default) linear predictor calculated link scale. expected used, predictions reflect expectation response (mean) ignore uncertainty observation process. response used, predictions take uncertainty observation process account return predictions outcome scale. variance used, variance response respect mean (mean-variance relationship) returned. type = \"terms\", component linear predictor returned separately form list (possibly standard errors, summary = TRUE): includes parametric model components, followed smooth component, excludes offset intercept. Two special cases also allowed: type latent_N return estimated latent abundances N-mixture distribution, type detection return estimated detection probability N-mixture distribution ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/forecast.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract or compute hindcasts and forecasts for a fitted mvgam object — forecast.mvgam","text":"object class mvgam_forecast containing hindcast forecast distributions. See mvgam_forecast-class details.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/forecast.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract or compute hindcasts and forecasts for a fitted mvgam object — forecast.mvgam","text":"Posterior predictions drawn fitted mvgam used simulate forecast distribution","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/forecast.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract or compute hindcasts and forecasts for a fitted mvgam object — forecast.mvgam","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 3, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),              trend_model = AR(),              noncentred = TRUE,              data = simdat$data_train,              chains = 2,              silent = 2)  # Hindcasts on response scale hc <- hindcast(mod) str(hc) #> List of 15 #>  $ call              :Class 'formula'  language y ~ s(season, bs = \"cc\", k = 6) #>   .. ..- attr(*, \".Environment\")=<environment: 0x5578dd77b470>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ trend_model       :List of 7 #>   ..$ trend_model: chr \"AR1\" #>   ..$ ma         : logi FALSE #>   ..$ cor        : logi FALSE #>   ..$ unit       : chr \"time\" #>   ..$ gr         : chr \"NA\" #>   ..$ subgr      : chr \"series\" #>   ..$ label      : language AR() #>   ..- attr(*, \"class\")= chr \"mvgam_trend\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : chr [1:3] \"series_1\" \"series_2\" \"series_3\" #>  $ train_observations:List of 3 #>   ..$ series_1: int [1:75] 0 0 1 0 1 0 1 0 3 2 ... #>   ..$ series_2: int [1:75] 0 1 1 0 0 1 3 0 2 2 ... #>   ..$ series_3: int [1:75] 1 1 0 0 0 0 1 9 3 5 ... #>  $ train_times       :List of 3 #>   ..$ series_1: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_2: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_3: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations : NULL #>  $ test_times        : NULL #>  $ hindcasts         :List of 3 #>   ..$ series_1: num [1:1000, 1:75] 0 0 0 0 0 0 1 0 4 2 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>   ..$ series_2: num [1:1000, 1:75] 1 0 0 2 1 1 1 1 2 1 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,2]\" \"ypred[2,2]\" \"ypred[3,2]\" \"ypred[4,2]\" ... #>   ..$ series_3: num [1:1000, 1:75] 0 2 3 1 1 0 0 2 0 5 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,3]\" \"ypred[2,3]\" \"ypred[3,3]\" \"ypred[4,3]\" ... #>  $ forecasts         : NULL #>  - attr(*, \"class\")= chr \"mvgam_forecast\"  # Use summary() to extract hindcasts / forecasts for custom plotting head(summary(hc), 12) #> # A tibble: 12 × 7 #>    series    time predQ50 predQ2.5 predQ97.5 truth type     #>    <fct>    <int>   <dbl>    <dbl>     <dbl> <int> <chr>    #>  1 series_1     1       1        0         3     0 response #>  2 series_1     2       0        0         3     0 response #>  3 series_1     3       0        0         2     1 response #>  4 series_1     4       0        0         2     0 response #>  5 series_1     5       0        0         2     1 response #>  6 series_1     6       0        0         2     0 response #>  7 series_1     7       1        0         4     1 response #>  8 series_1     8       1        0         5     0 response #>  9 series_1     9       2        0         6     3 response #> 10 series_1    10       2        0         6     2 response #> 11 series_1    11       1        0         4     1 response #> 12 series_1    12       1        0         3     0 response  # Or just use the plot() function for quick plots plot(hc, series = 1) #> No non-missing values in test_observations; cannot calculate forecast score  plot(hc, series = 2) #> No non-missing values in test_observations; cannot calculate forecast score  plot(hc, series = 3) #> No non-missing values in test_observations; cannot calculate forecast score   # Forecasts on response scale fc <- forecast(mod,                newdata = simdat$data_test) str(fc) #> List of 16 #>  $ call              :Class 'formula'  language y ~ s(season, bs = \"cc\", k = 6) #>   .. ..- attr(*, \".Environment\")=<environment: 0x5578dd77b470>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ family_pars       : NULL #>  $ trend_model       :List of 7 #>   ..$ trend_model: chr \"AR1\" #>   ..$ ma         : logi FALSE #>   ..$ cor        : logi FALSE #>   ..$ unit       : chr \"time\" #>   ..$ gr         : chr \"NA\" #>   ..$ subgr      : chr \"series\" #>   ..$ label      : language AR() #>   ..- attr(*, \"class\")= chr \"mvgam_trend\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : Factor w/ 3 levels \"series_1\",\"series_2\",..: 1 2 3 #>  $ train_observations:List of 3 #>   ..$ series_1: int [1:75] 0 0 1 0 1 0 1 0 3 2 ... #>   ..$ series_2: int [1:75] 0 1 1 0 0 1 3 0 2 2 ... #>   ..$ series_3: int [1:75] 1 1 0 0 0 0 1 9 3 5 ... #>  $ train_times       :List of 3 #>   ..$ series_1: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_2: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_3: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations :List of 3 #>   ..$ series_1: int [1:25] 0 0 0 2 4 1 1 0 1 1 ... #>   ..$ series_2: int [1:25] 0 0 0 1 4 4 3 0 1 1 ... #>   ..$ series_3: int [1:25] 0 1 0 2 8 2 6 1 3 2 ... #>  $ test_times        :List of 3 #>   ..$ series_1: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>   ..$ series_2: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>   ..$ series_3: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>  $ hindcasts         :List of 3 #>   ..$ series_1: num [1:1000, 1:75] 0 0 0 0 0 0 1 0 4 2 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>   ..$ series_2: num [1:1000, 1:75] 1 0 0 2 1 1 1 1 2 1 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,2]\" \"ypred[2,2]\" \"ypred[3,2]\" \"ypred[4,2]\" ... #>   ..$ series_3: num [1:1000, 1:75] 0 2 3 1 1 0 0 2 0 5 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,3]\" \"ypred[2,3]\" \"ypred[3,3]\" \"ypred[4,3]\" ... #>  $ forecasts         :List of 3 #>   ..$ series_1: int [1:1000, 1:25] 0 0 0 0 0 1 0 0 1 1 ... #>   ..$ series_2: int [1:1000, 1:25] 0 0 0 0 0 0 0 1 0 1 ... #>   ..$ series_3: int [1:1000, 1:25] 0 0 0 1 0 0 0 0 0 0 ... #>  - attr(*, \"class\")= chr \"mvgam_forecast\" head(summary(fc), 12) #> # A tibble: 12 × 7 #>    series    time predQ50 predQ2.5 predQ97.5 truth type     #>    <fct>    <int>   <dbl>    <dbl>     <dbl> <int> <chr>    #>  1 series_1     1       1        0         3     0 response #>  2 series_1     2       0        0         3     0 response #>  3 series_1     3       0        0         2     1 response #>  4 series_1     4       0        0         2     0 response #>  5 series_1     5       0        0         2     1 response #>  6 series_1     6       0        0         2     0 response #>  7 series_1     7       1        0         4     1 response #>  8 series_1     8       1        0         5     0 response #>  9 series_1     9       2        0         6     3 response #> 10 series_1    10       2        0         6     2 response #> 11 series_1    11       1        0         4     1 response #> 12 series_1    12       1        0         3     0 response plot(fc, series = 1) #> Out of sample DRPS: #> 9.69664  plot(fc, series = 2) #> Out of sample DRPS: #> 9.70004  plot(fc, series = 3) #> Out of sample DRPS: #> 25.049484   # Forecasts as expectations fc <- forecast(mod,                newdata = simdat$data_test,                type = 'expected') head(summary(fc), 12) #> # A tibble: 12 × 6 #>    series    time predQ50 predQ2.5 predQ97.5 type     #>    <fct>    <int>   <dbl>    <dbl>     <dbl> <chr>    #>  1 series_1     1   0.900    0.437     1.50  expected #>  2 series_1     2   0.577    0.271     1.12  expected #>  3 series_1     3   0.391    0.183     0.864 expected #>  4 series_1     4   0.260    0.125     0.547 expected #>  5 series_1     5   0.261    0.117     0.622 expected #>  6 series_1     6   0.406    0.175     0.850 expected #>  7 series_1     7   0.915    0.451     1.60  expected #>  8 series_1     8   1.59     0.793     2.70  expected #>  9 series_1     9   2.19     1.27      3.98  expected #> 10 series_1    10   1.90     1.08      3.35  expected #> 11 series_1    11   1.37     0.734     2.44  expected #> 12 series_1    12   0.893    0.442     1.45  expected plot(fc, series = 1)  plot(fc, series = 2)  plot(fc, series = 3)   # Dynamic trend extrapolations fc <- forecast(mod,                newdata = simdat$data_test,                type = 'trend') head(summary(fc), 12) #> # A tibble: 12 × 6 #>    series    time  predQ50 predQ2.5 predQ97.5 type  #>    <fct>    <int>    <dbl>    <dbl>     <dbl> <chr> #>  1 series_1     1 -0.0557    -0.718     0.388 trend #>  2 series_1     2 -0.0380    -0.748     0.565 trend #>  3 series_1     3  0.0277    -0.533     0.697 trend #>  4 series_1     4  0.00146   -0.668     0.547 trend #>  5 series_1     5  0.0364    -0.629     0.768 trend #>  6 series_1     6 -0.0133    -0.688     0.610 trend #>  7 series_1     7 -0.0209    -0.670     0.539 trend #>  8 series_1     8 -0.0963    -0.749     0.398 trend #>  9 series_1     9  0.0315    -0.537     0.626 trend #> 10 series_1    10 -0.00243   -0.583     0.548 trend #> 11 series_1    11 -0.0352    -0.688     0.472 trend #> 12 series_1    12 -0.0572    -0.788     0.400 trend plot(fc, series = 1)  plot(fc, series = 2)  plot(fc, series = 3)  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/formula.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract formulae from mvgam objects — formula.mvgam","title":"Extract formulae from mvgam objects — formula.mvgam","text":"Extract formulae mvgam objects","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/formula.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract formulae from mvgam objects — formula.mvgam","text":"","code":"# S3 method for mvgam formula(x, trend_effects = FALSE, ...)  # S3 method for mvgam_prefit formula(x, trend_effects = FALSE, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/formula.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract formulae from mvgam objects — formula.mvgam","text":"x mvgam, jsdgam mvgam_prefit object trend_effects logical, return formula observation model (FALSE) underlying process model (ifTRUE) ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/formula.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract formulae from mvgam objects — formula.mvgam","text":"formula object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/formula.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract formulae from mvgam objects — formula.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/get_mvgam_priors.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","title":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","text":"function lists parameters can prior distributions changed given model, well listing default distributions","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/get_mvgam_priors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","text":"","code":"get_mvgam_priors(   formula,   trend_formula,   factor_formula,   knots,   trend_knots,   trend_model = \"None\",   family = poisson(),   data,   unit = time,   species = series,   use_lv = FALSE,   n_lv,   trend_map,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/get_mvgam_priors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","text":"formula formula object specifying GAM observation model formula. exactly like formula GLM except smooth terms, s(), te(), ti(), t2(), well time-varying dynamic() terms, nonparametric gp() terms offsets using offset(), can added right hand side specify linear predictor depends smooth functions predictors (linear functionals ). nmix() family models, formula used set linear predictor detection probability. Details formula syntax used mvgam can found mvgam_formulae trend_formula optional formula object specifying GAM process model formula. supplied, linear predictor modelled latent trends capture process model evolution separately observation model. response variable specified left-hand side formula (.e. valid option ~ season + s(year)). Also note use identifier series formula specify effects vary across time series. Instead use trend. ensure models trend_map supplied still work consistently (.e. allowing effects vary across process models, even time series share underlying process model). feature currently available RW(), AR() VAR() trend models. nmix() family models, trend_formula used set linear predictor underlying latent abundance. aware can challenging simultaneously estimate intercept parameters observation mode (captured formula) process model (captured trend_formula). Users recommended drop one using - 1 convention formula right hand side. factor_formula Can supplied instead trend_formula match syntax jsdgam knots optional list containing user specified knot values used basis construction. bases user simply supplies knots used, must match k value supplied (note number knots always just k). Different terms can use different numbers knots, unless share covariate trend_knots knots , optional list knot values smooth functions within trend_formula trend_model character  function specifying time series dynamics latent trend. Options : None (latent trend component; .e. GAM component contributes linear predictor, observation process source error; similarly estimated gam) ZMVN ZMVN() (Zero-Mean Multivariate Normal; available Stan) 'RW' RW() 'AR1' AR(p = 1) 'AR2' AR(p = 2) 'AR3' AR(p = 3) 'CAR1' CAR(p = 1) (also known Ornstein–Uhlenbeck process) 'VAR1'  VAR()(available Stan) 'PWlogistic, 'PWlinear' PW() (available Stan) 'GP' GP() (Gaussian Process squared exponential kernel; available Stan) trend types apart ZMVN(), GP(), CAR() PW(), moving average /correlated process error terms can also estimated (example, RW(cor = TRUE) set multivariate Random Walk n_series > 1). also possible many multivariate trends estimate hierarchical correlations data structured among levels relevant grouping factor. See mvgam_trends details see ZMVN() example. family family specifying exponential observation family series. Currently supported families : gaussian() real-valued data betar() proportional data (0,1) lognormal() non-negative real-valued data student_t() real-valued data Gamma() non-negative real-valued data bernoulli() binary data poisson() count data nb() overdispersed count data binomial() count data imperfect detection number trials known; note cbind() function must used bind discrete observations discrete number trials beta_binomial() binomial() allows overdispersion nmix() count data imperfect detection number trials unknown modeled via State-Space N-Mixture model. latent states Poisson, capturing 'true' latent abundance, observation process Binomial account imperfect detection. See mvgam_families example use family Default poisson(). See mvgam_families details data dataframe list containing model response variable covariates required GAM formula optional trend_formula. models include columns: series (factor index series IDs; number levels identical number unique series labels (.e. n_series = length(levels(data$series)))) time (numeric integer index time point observation). dynamic trend types available mvgam (see argument trend_model), time measured discrete, regularly spaced intervals (.e. c(1, 2, 3, ...)). However can use irregularly spaced intervals using trend_model = CAR(1), though note temporal intervals exactly 0 adjusted small number (1e-12) prevent sampling errors. See example CAR() trends CAR() Note however special cases identifiers needed. example, models hierarchical temporal correlation processes (e.g. AR(gr = region, subgr = species)) include series identifier, constructed internally (see mvgam_trends AR() details). mvgam() can also fit models include time variable temporal dynamic structures included (.e. trend_model = 'None' trend_model = ZMVN()). data also include variables included linear predictor formula unit unquoted name variable represents unit analysis data latent residuals correlated. variable either numeric integer variable supplied data. Defaults time consistent functionalities mvgam, though note data need time series case. See examples details explanations species unquoted name factor variable indexes different response units data (usually 'species' JSDM). Defaults series consistent mvgam models use_lv logical. TRUE, use dynamic factors estimate series' latent trends reduced dimension format. available RW(), AR() GP() trend models. Defaults FALSE. See lv_correlations worked example n_lv integer number latent dynamic factors use use_lv == TRUE. > n_series. Defaults arbitrarily min(2, floor(n_series / 2)) trend_map Optional data.frame specifying series depend latent trends. Useful allowing multiple series depend latent trend process, different observation processes. supplied, latent factor model set setting use_lv = TRUE using mapping set shared trends. Needs column names series trend, integer values trend column state trend series depend . series column single unique entry series data (names perfectly match factor levels series variable data). Note supplied, intercept parameter process model automatically suppressed. yet supported models latent factors evolve continuous time (CAR()). See examples details ... currently used","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/get_mvgam_priors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","text":"either data.frame containing prior definitions (suitable priors can altered user) NULL, indicating priors model can modified","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/get_mvgam_priors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","text":"Users can supply model formula, prior fitting model, default priors can inspected altered. make alterations, change contents prior column supplying data.frame mvgam jsdgam functions using argument priors. using Stan backend, users can also modify parameter bounds modifying new_lowerbound /new_upperbound columns. necessary using restrictive distributions parameters, Beta distribution trend sd parameters example (Beta support  (0,1)), upperbound 1. Another option make use prior modification functions brms (.e. prior) change prior distributions bounds (just use name parameter like change class argument; see examples )","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/get_mvgam_priors.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","text":"prior, new_lowerbound /new_upperbound columns output altered defining user-defined priors model. Use familiar underlying probabilistic programming language. sanity checks done ensure code legal (.e. check lower bounds smaller upper bounds, example)","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/get_mvgam_priors.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/get_mvgam_priors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract information on default prior distributions for an mvgam model — get_mvgam_priors","text":"","code":"# \\donttest{ # Simulate three integer-valued time series library(mvgam) dat <- sim_mvgam(trend_rel = 0.5)  # Get a model file that uses default mvgam priors for inspection (not always necessary, # but this can be useful for testing whether your updated priors are written correctly) mod_default <- mvgam(y ~ s(series, bs = 're') +               s(season, bs = 'cc') - 1,               family = nb(),               data = dat$data_train,               trend_model = AR(p = 2),               run_model = FALSE) #> Your model may benefit from using \"noncentred = TRUE\"  # Inspect the model file with default mvgam priors stancode(mod_default) #> // Stan model code generated by package mvgam #> functions { #>   vector rep_each(vector x, int K) { #>     int N = rows(x); #>     vector[N * K] y; #>     int pos = 1; #>     for (n in 1 : N) { #>       for (k in 1 : K) { #>         y[pos] = x[n]; #>         pos += 1; #>       } #>     } #>     return y; #>   } #> } #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[8, 8] S1; // mgcv smooth penalty matrix S1 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // random effect variances #>   vector<lower=0>[1] sigma_raw; #>    #>   // random effect means #>   vector[1] mu_raw; #>    #>   // negative binomial overdispersion #>   vector<lower=0>[n_series] phi_inv; #>    #>   // latent trend AR1 terms #>   vector<lower=-1, upper=1>[n_series] ar1; #>    #>   // latent trend AR2 terms #>   vector<lower=-1, upper=1>[n_series] ar2; #>    #>   // latent trend variance parameters #>   vector<lower=0>[n_series] sigma; #>    #>   // latent trends #>   matrix[n, n_series] trend; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : 8] = b_raw[1 : 8]; #>   b[9 : 11] = mu_raw[1] + b_raw[9 : 11] * sigma_raw[1]; #> } #> model { #>   // prior for random effect population variances #>   sigma_raw ~ inv_gamma(1.418, 0.452); #>    #>   // prior for random effect population means #>   mu_raw ~ std_normal(); #>    #>   // prior for s(season)... #>   b_raw[1 : 8] ~ multi_normal_prec(zero[1 : 8], S1[1 : 8, 1 : 8] * lambda[1]); #>    #>   // prior (non-centred) for s(series)... #>   b_raw[9 : 11] ~ std_normal(); #>    #>   // priors for AR parameters #>   ar1 ~ std_normal(); #>   ar2 ~ std_normal(); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for overdispersion parameters #>   phi_inv ~ student_t(3, 0, 0.1); #>    #>   // priors for latent trend variance parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>    #>   // trend estimates #>   trend[1, 1 : n_series] ~ normal(0, sigma); #>   trend[2, 1 : n_series] ~ normal(trend[1, 1 : n_series] * ar1, sigma); #>   for (s in 1 : n_series) { #>     trend[3 : n, s] ~ normal(ar1[s] * trend[2 : (n - 1), s] #>                              + ar2[s] * trend[1 : (n - 2), s], sigma[s]); #>   } #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     array[n_nonmissing] real flat_phis; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_phis = to_array_1d(rep_each(phi_inv, n)[obs_ind]); #>     flat_ys ~ neg_binomial_2(exp(append_col(flat_xs, flat_trends) #>                                  * append_row(b, 1.0)), #>                              inv(flat_phis)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   vector[n_series] tau; #>   array[n, n_series] int ypred; #>   matrix[n, n_series] phi_vec; #>   vector[n_series] phi; #>   phi = inv(phi_inv); #>   for (s in 1 : n_series) { #>     phi_vec[1 : n, s] = rep_vector(phi[s], n); #>   } #>   rho = log(lambda); #>   for (s in 1 : n_series) { #>     tau[s] = pow(sigma[s], -2.0); #>   } #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = neg_binomial_2_rng(exp(mus[1 : n, s]), phi_vec[1 : n, s]); #>   } #> } #>  #>   # Look at which priors can be updated in mvgam test_priors <- get_mvgam_priors(y ~ s(series, bs = 're') +                               s(season, bs = 'cc') - 1,                               family = nb(),                               data = dat$data_train,                               trend_model = AR(p = 2)) test_priors #>                                param_name param_length #> 1           vector<lower=0>[n_sp] lambda;            2 #> 2                       vector[1] mu_raw;            1 #> 3           vector<lower=0>[1] sigma_raw;            1 #> 4 vector<lower=-1,upper=1>[n_series] ar1;            3 #> 5 vector<lower=-1,upper=1>[n_series] ar2;            3 #> 6        vector<lower=0>[n_series] sigma;            3 #> 7      vector<lower=0>[n_series] phi_inv;            3 #>                    param_info                                prior #> 1 s(season) smooth parameters              lambda ~ normal(5, 30); #> 2          s(series) pop mean               mu_raw ~ std_normal(); #> 3            s(series) pop sd sigma_raw ~ inv_gamma(1.418, 0.452); #> 4       trend AR1 coefficient                  ar1 ~ std_normal(); #> 5       trend AR2 coefficient                  ar2 ~ std_normal(); #> 6                    trend sd     sigma ~ inv_gamma(1.418, 0.452); #> 7   inverse of NB dispsersion      phi_inv ~ student_t(3, 0, 0.1); #>                   example_change new_lowerbound new_upperbound #> 1    lambda ~ exponential(0.46);             NA             NA #> 2    mu_raw ~ normal(0.5, 0.21);             NA             NA #> 3 sigma_raw ~ exponential(0.93);             NA             NA #> 4      ar1 ~ normal(0.98, 0.41);             NA             NA #> 5     ar2 ~ normal(-0.75, 0.87);             NA             NA #> 6     sigma ~ exponential(0.44);             NA             NA #> 7  phi_inv ~ normal(0.59, 0.87);             NA             NA  # Make a few changes; first, change the population mean for the series-level # random intercepts test_priors$prior[2] <- 'mu_raw ~ normal(0.2, 0.5);'  # Now use stronger regularisation for the series-level AR2 coefficients test_priors$prior[5] <- 'ar2 ~ normal(0, 0.25);'  # Check that the changes are made to the model file without any warnings by # setting 'run_model = FALSE' mod <- mvgam(y ~ s(series, bs = 're') +             s(season, bs = 'cc') - 1,             family = nb(),             data = dat$data_train,             trend_model = AR(p = 2),             priors = test_priors,             run_model = FALSE) #> Your model may benefit from using \"noncentred = TRUE\" stancode(mod) #> // Stan model code generated by package mvgam #> functions { #>   vector rep_each(vector x, int K) { #>     int N = rows(x); #>     vector[N * K] y; #>     int pos = 1; #>     for (n in 1 : N) { #>       for (k in 1 : K) { #>         y[pos] = x[n]; #>         pos += 1; #>       } #>     } #>     return y; #>   } #> } #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[8, 8] S1; // mgcv smooth penalty matrix S1 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // random effect variances #>   vector<lower=0>[1] sigma_raw; #>    #>   // random effect means #>   vector[1] mu_raw; #>    #>   // negative binomial overdispersion #>   vector<lower=0>[n_series] phi_inv; #>    #>   // latent trend AR1 terms #>   vector<lower=-1, upper=1>[n_series] ar1; #>    #>   // latent trend AR2 terms #>   vector<lower=-1, upper=1>[n_series] ar2; #>    #>   // latent trend variance parameters #>   vector<lower=0>[n_series] sigma; #>    #>   // latent trends #>   matrix[n, n_series] trend; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : 8] = b_raw[1 : 8]; #>   b[9 : 11] = mu_raw[1] + b_raw[9 : 11] * sigma_raw[1]; #> } #> model { #>   // prior for random effect population variances #>   sigma_raw ~ inv_gamma(1.418, 0.452); #>    #>   // prior for random effect population means #>   mu_raw ~ normal(0.2, 0.5); #>    #>   // prior for s(season)... #>   b_raw[1 : 8] ~ multi_normal_prec(zero[1 : 8], S1[1 : 8, 1 : 8] * lambda[1]); #>    #>   // prior (non-centred) for s(series)... #>   b_raw[9 : 11] ~ std_normal(); #>    #>   // priors for AR parameters #>   ar1 ~ std_normal(); #>   ar2 ~ normal(0, 0.25); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for overdispersion parameters #>   phi_inv ~ student_t(3, 0, 0.1); #>    #>   // priors for latent trend variance parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>    #>   // trend estimates #>   trend[1, 1 : n_series] ~ normal(0, sigma); #>   trend[2, 1 : n_series] ~ normal(trend[1, 1 : n_series] * ar1, sigma); #>   for (s in 1 : n_series) { #>     trend[3 : n, s] ~ normal(ar1[s] * trend[2 : (n - 1), s] #>                              + ar2[s] * trend[1 : (n - 2), s], sigma[s]); #>   } #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     array[n_nonmissing] real flat_phis; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_phis = to_array_1d(rep_each(phi_inv, n)[obs_ind]); #>     flat_ys ~ neg_binomial_2(exp(append_col(flat_xs, flat_trends) #>                                  * append_row(b, 1.0)), #>                              inv(flat_phis)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   vector[n_series] tau; #>   array[n, n_series] int ypred; #>   matrix[n, n_series] phi_vec; #>   vector[n_series] phi; #>   phi = inv(phi_inv); #>   for (s in 1 : n_series) { #>     phi_vec[1 : n, s] = rep_vector(phi[s], n); #>   } #>   rho = log(lambda); #>   for (s in 1 : n_series) { #>     tau[s] = pow(sigma[s], -2.0); #>   } #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = neg_binomial_2_rng(exp(mus[1 : n, s]), phi_vec[1 : n, s]); #>   } #> } #>  #>   # No warnings, the model is ready for fitting now in the usual way with the addition # of the 'priors' argument  # The same can be done using 'brms' functions; here we will also change the ar1 prior # and put some bounds on the ar coefficients to enforce stationarity; we set the # prior using the 'class' argument in all brms prior functions brmsprior <- c(prior(normal(0.2, 0.5), class = mu_raw),               prior(normal(0, 0.25), class = ar1, lb = -1, ub = 1),               prior(normal(0, 0.25), class = ar2, lb = -1, ub = 1)) brmsprior #>             prior  class coef group resp dpar nlpar   lb   ub tag source #>  normal(0.2, 0.5) mu_raw                            <NA> <NA>       user #>   normal(0, 0.25)    ar1                              -1    1       user #>   normal(0, 0.25)    ar2                              -1    1       user  mod <- mvgam(y ~ s(series, bs = 're') +              s(season, bs = 'cc') - 1,            family = nb(),            data = dat$data_train,            trend_model = AR(p = 2),            priors = brmsprior,            run_model = FALSE) #> Your model may benefit from using \"noncentred = TRUE\" stancode(mod) #> // Stan model code generated by package mvgam #> functions { #>   vector rep_each(vector x, int K) { #>     int N = rows(x); #>     vector[N * K] y; #>     int pos = 1; #>     for (n in 1 : N) { #>       for (k in 1 : K) { #>         y[pos] = x[n]; #>         pos += 1; #>       } #>     } #>     return y; #>   } #> } #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[8, 8] S1; // mgcv smooth penalty matrix S1 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // random effect variances #>   vector<lower=0>[1] sigma_raw; #>    #>   // random effect means #>   vector[1] mu_raw; #>    #>   // negative binomial overdispersion #>   vector<lower=0>[n_series] phi_inv; #>    #>   // latent trend AR1 terms #>   vector<lower=-1, upper=1>[n_series] ar1; #>    #>   // latent trend AR2 terms #>   vector<lower=-1, upper=1>[n_series] ar2; #>    #>   // latent trend variance parameters #>   vector<lower=0>[n_series] sigma; #>    #>   // latent trends #>   matrix[n, n_series] trend; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : 8] = b_raw[1 : 8]; #>   b[9 : 11] = mu_raw[1] + b_raw[9 : 11] * sigma_raw[1]; #> } #> model { #>   // prior for random effect population variances #>   sigma_raw ~ inv_gamma(1.418, 0.452); #>    #>   // prior for random effect population means #>   mu_raw ~ normal(0.2, 0.5); #>    #>   // prior for s(season)... #>   b_raw[1 : 8] ~ multi_normal_prec(zero[1 : 8], S1[1 : 8, 1 : 8] * lambda[1]); #>    #>   // prior (non-centred) for s(series)... #>   b_raw[9 : 11] ~ std_normal(); #>    #>   // priors for AR parameters #>   ar1 ~ normal(0, 0.25); #>   ar2 ~ normal(0, 0.25); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for overdispersion parameters #>   phi_inv ~ student_t(3, 0, 0.1); #>    #>   // priors for latent trend variance parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>    #>   // trend estimates #>   trend[1, 1 : n_series] ~ normal(0, sigma); #>   trend[2, 1 : n_series] ~ normal(trend[1, 1 : n_series] * ar1, sigma); #>   for (s in 1 : n_series) { #>     trend[3 : n, s] ~ normal(ar1[s] * trend[2 : (n - 1), s] #>                              + ar2[s] * trend[1 : (n - 2), s], sigma[s]); #>   } #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     array[n_nonmissing] real flat_phis; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_phis = to_array_1d(rep_each(phi_inv, n)[obs_ind]); #>     flat_ys ~ neg_binomial_2(exp(append_col(flat_xs, flat_trends) #>                                  * append_row(b, 1.0)), #>                              inv(flat_phis)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   vector[n_series] tau; #>   array[n, n_series] int ypred; #>   matrix[n, n_series] phi_vec; #>   vector[n_series] phi; #>   phi = inv(phi_inv); #>   for (s in 1 : n_series) { #>     phi_vec[1 : n, s] = rep_vector(phi[s], n); #>   } #>   rho = log(lambda); #>   for (s in 1 : n_series) { #>     tau[s] = pow(sigma[s], -2.0); #>   } #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = neg_binomial_2_rng(exp(mus[1 : n, s]), phi_vec[1 : n, s]); #>   } #> } #>  #>   # Look at what is returned when an incorrect spelling is used test_priors$prior[5] <- 'ar2_bananas ~ normal(0, 0.25);' mod <- mvgam(y ~ s(series, bs = 're') +              s(season, bs = 'cc') - 1,             family = nb(),             data = dat$data_train,             trend_model = AR(p = 2),             priors = test_priors,             run_model = FALSE) #> Warning: no match found in model_file for parameter: ar2_bananas #> Your model may benefit from using \"noncentred = TRUE\" stancode(mod) #> // Stan model code generated by package mvgam #> functions { #>   vector rep_each(vector x, int K) { #>     int N = rows(x); #>     vector[N * K] y; #>     int pos = 1; #>     for (n in 1 : N) { #>       for (k in 1 : K) { #>         y[pos] = x[n]; #>         pos += 1; #>       } #>     } #>     return y; #>   } #> } #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[8, 8] S1; // mgcv smooth penalty matrix S1 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // random effect variances #>   vector<lower=0>[1] sigma_raw; #>    #>   // random effect means #>   vector[1] mu_raw; #>    #>   // negative binomial overdispersion #>   vector<lower=0>[n_series] phi_inv; #>    #>   // latent trend AR1 terms #>   vector<lower=-1, upper=1>[n_series] ar1; #>    #>   // latent trend AR2 terms #>   vector<lower=-1, upper=1>[n_series] ar2; #>    #>   // latent trend variance parameters #>   vector<lower=0>[n_series] sigma; #>    #>   // latent trends #>   matrix[n, n_series] trend; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : 8] = b_raw[1 : 8]; #>   b[9 : 11] = mu_raw[1] + b_raw[9 : 11] * sigma_raw[1]; #> } #> model { #>   // prior for random effect population variances #>   sigma_raw ~ inv_gamma(1.418, 0.452); #>    #>   // prior for random effect population means #>   mu_raw ~ normal(0.2, 0.5); #>    #>   // prior for s(season)... #>   b_raw[1 : 8] ~ multi_normal_prec(zero[1 : 8], S1[1 : 8, 1 : 8] * lambda[1]); #>    #>   // prior (non-centred) for s(series)... #>   b_raw[9 : 11] ~ std_normal(); #>    #>   // priors for AR parameters #>   ar1 ~ std_normal(); #>   ar2 ~ std_normal(); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for overdispersion parameters #>   phi_inv ~ student_t(3, 0, 0.1); #>    #>   // priors for latent trend variance parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>    #>   // trend estimates #>   trend[1, 1 : n_series] ~ normal(0, sigma); #>   trend[2, 1 : n_series] ~ normal(trend[1, 1 : n_series] * ar1, sigma); #>   for (s in 1 : n_series) { #>     trend[3 : n, s] ~ normal(ar1[s] * trend[2 : (n - 1), s] #>                              + ar2[s] * trend[1 : (n - 2), s], sigma[s]); #>   } #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     array[n_nonmissing] real flat_phis; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_phis = to_array_1d(rep_each(phi_inv, n)[obs_ind]); #>     flat_ys ~ neg_binomial_2(exp(append_col(flat_xs, flat_trends) #>                                  * append_row(b, 1.0)), #>                              inv(flat_phis)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   vector[n_series] tau; #>   array[n, n_series] int ypred; #>   matrix[n, n_series] phi_vec; #>   vector[n_series] phi; #>   phi = inv(phi_inv); #>   for (s in 1 : n_series) { #>     phi_vec[1 : n, s] = rep_vector(phi[s], n); #>   } #>   rho = log(lambda); #>   for (s in 1 : n_series) { #>     tau[s] = pow(sigma[s], -2.0); #>   } #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = neg_binomial_2_rng(exp(mus[1 : n, s]), phi_vec[1 : n, s]); #>   } #> } #>  #>   # Example of changing parametric (fixed effect) priors simdat <- sim_mvgam()  # Add a fake covariate simdat$data_train$cov <- rnorm(NROW(simdat$data_train))  priors <- get_mvgam_priors(y ~ cov + s(season),                           data = simdat$data_train,                           family = poisson(),                           trend_model = AR())  # Change priors for the intercept and fake covariate effects priors$prior[1] <- '(Intercept) ~ normal(0, 1);' priors$prior[2] <- 'cov ~ normal(0, 0.1);'  mod2 <- mvgam(y ~ cov + s(season),              data = simdat$data_train,              trend_model = AR(),              family = poisson(),              priors = priors,              run_model = FALSE) #> Your model may benefit from using \"noncentred = TRUE\" stancode(mod2) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[9, 18] S1; // mgcv smooth penalty matrix S1 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // latent trend AR1 terms #>   vector<lower=-1, upper=1>[n_series] ar1; #>    #>   // latent trend variance parameters #>   vector<lower=0>[n_series] sigma; #>    #>   // latent trends #>   matrix[n, n_series] trend; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : num_basis] = b_raw[1 : num_basis]; #> } #> model { #>   // prior for (Intercept)... #>   b_raw[1] ~ normal(0, 1); #>    #>   // prior for cov... #>   b_raw[2] ~ normal(0, 0.1); #>    #>   // prior for s(season)... #>   b_raw[3 : 11] ~ multi_normal_prec(zero[3 : 11], #>                                     S1[1 : 9, 1 : 9] * lambda[1] #>                                     + S1[1 : 9, 10 : 18] * lambda[2]); #>    #>   // priors for AR parameters #>   ar1 ~ std_normal(); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for latent trend variance parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>    #>   // trend estimates #>   trend[1, 1 : n_series] ~ normal(0, sigma); #>   for (s in 1 : n_series) { #>     trend[2 : n, s] ~ normal(ar1[s] * trend[1 : (n - 1), s], sigma[s]); #>   } #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_ys ~ poisson_log_glm(append_col(flat_xs, flat_trends), 0.0, #>                               append_row(b, 1.0)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   vector[n_series] tau; #>   array[n, n_series] int ypred; #>   rho = log(lambda); #>   for (s in 1 : n_series) { #>     tau[s] = pow(sigma[s], -2.0); #>   } #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> } #>  #>   # Likewise using 'brms' utilities (note that you can use # Intercept rather than `(Intercept)`) to change priors on the intercept brmsprior <- c(prior(normal(0.2, 0.5), class = cov),               prior(normal(0, 0.25), class = Intercept)) brmsprior #>             prior     class coef group resp dpar nlpar   lb   ub tag source #>  normal(0.2, 0.5)       cov                            <NA> <NA>       user #>   normal(0, 0.25) Intercept                            <NA> <NA>       user  mod2 <- mvgam(y ~ cov + s(season),              data = simdat$data_train,              trend_model = AR(),              family = poisson(),              priors = brmsprior,              run_model = FALSE) #> Your model may benefit from using \"noncentred = TRUE\" stancode(mod2) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[9, 18] S1; // mgcv smooth penalty matrix S1 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // latent trend AR1 terms #>   vector<lower=-1, upper=1>[n_series] ar1; #>    #>   // latent trend variance parameters #>   vector<lower=0>[n_series] sigma; #>    #>   // latent trends #>   matrix[n, n_series] trend; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : num_basis] = b_raw[1 : num_basis]; #> } #> model { #>   // prior for (Intercept)... #>   b_raw[1] ~ normal(0, 0.25); #>    #>   // prior for cov... #>   b_raw[2] ~ normal(0.2, 0.5); #>    #>   // prior for s(season)... #>   b_raw[3 : 11] ~ multi_normal_prec(zero[3 : 11], #>                                     S1[1 : 9, 1 : 9] * lambda[1] #>                                     + S1[1 : 9, 10 : 18] * lambda[2]); #>    #>   // priors for AR parameters #>   ar1 ~ std_normal(); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for latent trend variance parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>    #>   // trend estimates #>   trend[1, 1 : n_series] ~ normal(0, sigma); #>   for (s in 1 : n_series) { #>     trend[2 : n, s] ~ normal(ar1[s] * trend[1 : (n - 1), s], sigma[s]); #>   } #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_ys ~ poisson_log_glm(append_col(flat_xs, flat_trends), 0.0, #>                               append_row(b, 1.0)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   vector[n_series] tau; #>   array[n, n_series] int ypred; #>   rho = log(lambda); #>   for (s in 1 : n_series) { #>     tau[s] = pow(sigma[s], -2.0); #>   } #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> } #>  #>   # The \"class = 'b'\" shortcut can be used to put the same prior on all # 'fixed' effect coefficients (apart from any intercepts) set.seed(0) dat <- mgcv::gamSim(1, n = 200, scale = 2) #> Gu & Wahba 4 term additive model dat$time <- 1:NROW(dat) mod <- mvgam(y ~ x0 + x1 + s(x2) + s(x3),             priors = prior(normal(0, 0.75), class = 'b'),             data = dat,             family = gaussian(),             run_model = FALSE) stancode(mod) #> // Stan model code generated by package mvgam #> functions { #>   vector rep_each(vector x, int K) { #>     int N = rows(x); #>     vector[N * K] y; #>     int pos = 1; #>     for (n in 1 : N) { #>       for (k in 1 : K) { #>         y[pos] = x[n]; #>         pos += 1; #>       } #>     } #>     return y; #>   } #> } #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[9, 18] S1; // mgcv smooth penalty matrix S1 #>   matrix[9, 18] S2; // mgcv smooth penalty matrix S2 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   vector[n_nonmissing] flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // gaussian observation error #>   vector<lower=0>[n_series] sigma_obs; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : num_basis] = b_raw[1 : num_basis]; #> } #> model { #>   // prior for (Intercept)... #>   b_raw[1] ~ student_t(3, 7.4, 3.7); #>    #>   // prior for x0... #>   b_raw[2] ~ normal(0, 0.75); #>    #>   // prior for x1... #>   b_raw[3] ~ normal(0, 0.75); #>    #>   // prior for s(x2)... #>   b_raw[4 : 12] ~ multi_normal_prec(zero[4 : 12], #>                                     S1[1 : 9, 1 : 9] * lambda[1] #>                                     + S1[1 : 9, 10 : 18] * lambda[2]); #>    #>   // prior for s(x3)... #>   b_raw[13 : 21] ~ multi_normal_prec(zero[13 : 21], #>                                      S2[1 : 9, 1 : 9] * lambda[3] #>                                      + S2[1 : 9, 10 : 18] * lambda[4]); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for observation error parameters #>   sigma_obs ~ inv_gamma(1.418, 0.452); #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_sigma_obs; #>     flat_sigma_obs = rep_each(sigma_obs, n)[obs_ind]; #>     flat_ys ~ normal_id_glm(flat_xs, 0.0, b, flat_sigma_obs); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] sigma_obs_vec; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   array[n, n_series] real ypred; #>   rho = log(lambda); #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     sigma_obs_vec[1 : n, s] = rep_vector(sigma_obs[s], n); #>   } #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]]; #>     ypred[1 : n, s] = normal_rng(mus[1 : n, s], sigma_obs_vec[1 : n, s]); #>   } #> } #>  #>  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/gratia_mvgam_enhancements.html","id":null,"dir":"Reference","previous_headings":"","what":"Enhance post-processing of mvgam models using gratia functionality — gratia_mvgam_enhancements","title":"Enhance post-processing of mvgam models using gratia functionality — gratia_mvgam_enhancements","text":"evaluation plotting functions exist allow popular gratia methods work mvgam jsdgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/gratia_mvgam_enhancements.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enhance post-processing of mvgam models using gratia functionality — gratia_mvgam_enhancements","text":"","code":"drawDotmvgam(   object,   trend_effects = FALSE,   data = NULL,   select = NULL,   parametric = FALSE,   terms = NULL,   residuals = FALSE,   scales = c(\"free\", \"fixed\"),   ci_level = 0.95,   n = 100,   n_3d = 16,   n_4d = 4,   unconditional = FALSE,   overall_uncertainty = TRUE,   constant = NULL,   fun = NULL,   dist = 0.1,   rug = TRUE,   contour = TRUE,   grouped_by = FALSE,   ci_alpha = 0.2,   ci_col = \"black\",   smooth_col = \"black\",   resid_col = \"steelblue3\",   contour_col = \"black\",   n_contour = NULL,   partial_match = FALSE,   discrete_colour = NULL,   discrete_fill = NULL,   continuous_colour = NULL,   continuous_fill = NULL,   position = \"identity\",   angle = NULL,   ncol = NULL,   nrow = NULL,   guides = \"keep\",   widths = NULL,   heights = NULL,   crs = NULL,   default_crs = NULL,   lims_method = \"cross\",   wrap = TRUE,   envir = environment(formula(object)),   ... )  eval_smoothDothilbertDotsmooth(   smooth,   model,   n = 100,   n_3d = NULL,   n_4d = NULL,   data = NULL,   unconditional = FALSE,   overall_uncertainty = TRUE,   dist = NULL,   ... )  eval_smoothDotmodDotsmooth(   smooth,   model,   n = 100,   n_3d = NULL,   n_4d = NULL,   data = NULL,   unconditional = FALSE,   overall_uncertainty = TRUE,   dist = NULL,   ... )  eval_smoothDotmoiDotsmooth(   smooth,   model,   n = 100,   n_3d = NULL,   n_4d = NULL,   data = NULL,   unconditional = FALSE,   overall_uncertainty = TRUE,   dist = NULL,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/gratia_mvgam_enhancements.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enhance post-processing of mvgam models using gratia functionality — gratia_mvgam_enhancements","text":"object fitted mvgam, result call mvgam(). trend_effects logical specifying whether smooth terms trend_formula drawn. FALSE, terms observation formula drawn. TRUE, terms trend_formula drawn. data data frame covariate values evaluate model's smooth functions. select character, logical, numeric; smooths plot. NULL, default, model smooths drawn. Character select matches labels smooths shown example output summary(object). Logical select operates per numeric select order smooths stored. parametric logical; plot parametric terms also? Note select used selecting smooths plot. terms argument used select parametric effects plotted. default, mgcv::plot.gam(), draw parametric effects. terms character; model parametric terms drawn? Default NULL plot parametric terms can drawn. residuals currently ignored mvgam models. scales character; univariate smooths plotted y-axis scale? scales = \"free\", default, univariate smooth y-axis scale. scales = \"fixed\", common y axis scale used univariate smooths. Currently affect y-axis scale plots parametric terms. ci_level numeric 0 1; coverage credible interval. n numeric; number points range covariate evaluate smooth. n_3d, n_4d numeric; number points range last covariate 3D 4D smooth. default NULL achieves standard behaviour using n points range covariate, resulting n^d evaluation points, d dimension smooth. d > 2 can result many evaluation points slow performance. smooths d > 4, value n_4d used dimensions > 4, unless NULL, case default behaviour (using n dimensions) observed. unconditional ignored mvgam models appropriate uncertainties already included posterior estimates. overall_uncertainty ignored mvgam models appropriate uncertainties already included posterior estimates. constant numeric; constant add estimated values smooth. constant, supplied, added estimated value confidence band computed. fun function; function applied estimated values confidence interval plotting. Can function name function. Function fun applied adding constant, provided. dist numeric; greater 0, used determine location far data plotted plotting 2-D smooths. data scaled unit square deciding exclude, dist distance within unit square. See mgcv::exclude..far() details. rug logical; draw rug plot bottom plot 1-D smooths plot locations data higher dimensions. contour logical; contours draw plot using ggplot2::geom_contour(). grouped_by logical; factor smooths drawn one panel per level factor (FALSE, default), individual smooths combined single panel containing levels (TRUE)? ci_alpha numeric; alpha transparency confidence simultaneous interval. ci_col colour specification confidence/credible intervals band. Affects fill interval. smooth_col colour specification smooth line. resid_col colour specification residual points. Ignored. contour_col colour specification contour lines. n_contour numeric; number contour bins. result n_contour - 1 contour lines drawn. See ggplot2::geom_contour(). partial_match logical; smooths selected partial matches select? TRUE, select can single string match . discrete_colour suitable colour scale used plotting discrete variables. discrete_fill suitable fill scale used plotting discrete variables. continuous_colour suitable colour scale used plotting continuous variables. continuous_fill suitable fill scale used plotting continuous variables. position Position adjustment, either string, result call position adjustment function. angle numeric; angle x axis tick labels drawn passed angle argument ggplot2::guide_axis(). ncol, nrow numeric; numbers rows columns spread plots guides character; one \"keep\" (default), \"collect\", \"auto\". Passed patchwork::plot_layout() widths, heights relative widths heights column row grid. get repeated match dimensions grid. 1 plot widths = NULL, value widths set internally widths = 1 accommodate plots smooths use fixed aspect ratio. crs coordinate reference system (CRS) use plot. data projected CRS. See ggplot2::coord_sf() details. default_crs coordinate reference system (CRS) use non-sf layers plot. left default NULL, CRS used 4326 (WGS84), appropriate spline---sphere smooths, parameterized terms latitude longitude coordinates. See ggplot2::coord_sf() details. lims_method character; affects axis limits determined. See ggplot2::coord_sf(). careful; testing examples, changing \"orthogonal\" example chlorophyll-example Simon Wood's GAM book quickly used RAM test system OS killed R. incorrect usage part; right now grid points SOS smooths evaluated (supplied user) can produce invalid coordinates corners tiles grid generated tile centres without respect spacing tiles. wrap logical; wrap plots patchwork? FALSE, list ggplot objects returned, 1 per term plotted. envir environment look data within. ... additional arguments passed methods. smooth smooth object class \"gp.smooth\" (returned model using either dynamic() function gp() function) class \"moi.smooth\" \"mod.smooth\" (returned model using 'moi' 'mod' basis). model fitted mgcv model clas gam bam.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/gratia_mvgam_enhancements.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Enhance post-processing of mvgam models using gratia functionality — gratia_mvgam_enhancements","text":"methods allow mvgam models Enhanced users gratia package installed, making available popular draw() function plot partial effects mvgam smooth functions using ggplot2::ggplot() utilities","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/gratia_mvgam_enhancements.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Enhance post-processing of mvgam models using gratia functionality — gratia_mvgam_enhancements","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/gratia_mvgam_enhancements.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Enhance post-processing of mvgam models using gratia functionality — gratia_mvgam_enhancements","text":"","code":"# \\donttest{ # Fit a simple GAM and draw partial effects of smooths using gratia set.seed(0) dat <- mgcv::gamSim(1, n = 200, scale = 2) #> Gu & Wahba 4 term additive model mod <- mvgam(y ~ s(x1, bs = 'moi') +               te(x0, x2),              data = dat,              family = gaussian(),              chains = 2,              silent = 2)  if(require(\"gratia\")){  gratia::draw(mod) } #> Loading required package: gratia #>  #> Attaching package: ‘gratia’ #> The following object is masked from ‘package:mvgam’: #>  #>     add_residuals   # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/hindcast.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract hindcasts for a fitted mvgam object — hindcast.mvgam","title":"Extract hindcasts for a fitted mvgam object — hindcast.mvgam","text":"Extract hindcasts fitted mvgam object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/hindcast.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract hindcasts for a fitted mvgam object — hindcast.mvgam","text":"","code":"hindcast(object, ...)  # S3 method for mvgam hindcast(object, type = \"response\", ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/hindcast.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract hindcasts for a fitted mvgam object — hindcast.mvgam","text":"object list object class mvgam jsdgam. See mvgam() ... Ignored type value link (default) linear predictor calculated link scale. expected used, predictions reflect expectation response (mean) ignore uncertainty observation process. response used, predictions take uncertainty observation process account return predictions outcome scale. variance used, variance response respect mean (mean-variance relationship) returned. type = \"terms\", component linear predictor returned separately form list (possibly standard errors, summary = TRUE): includes parametric model components, followed smooth component, excludes offset intercept. Two special cases also allowed: type latent_N return estimated latent abundances N-mixture distribution, type detection return estimated detection probability N-mixture distribution","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/hindcast.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract hindcasts for a fitted mvgam object — hindcast.mvgam","text":"object class mvgam_forecast containing hindcast distributions. See mvgam_forecast-class details.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/hindcast.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract hindcasts for a fitted mvgam object — hindcast.mvgam","text":"Posterior retrodictions drawn fitted mvgam organized convenient format","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/hindcast.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract hindcasts for a fitted mvgam object — hindcast.mvgam","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 3, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc'),              trend_model = AR(),              noncentred = TRUE,              data = simdat$data_train,              chains = 2,              silent = 2)  # Hindcasts on response scale hc <- hindcast(mod) str(hc) #> List of 15 #>  $ call              :Class 'formula'  language y ~ s(season, bs = \"cc\") #>   .. ..- attr(*, \".Environment\")=<environment: 0x5578f32bffa8>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ trend_model       :List of 7 #>   ..$ trend_model: chr \"AR1\" #>   ..$ ma         : logi FALSE #>   ..$ cor        : logi FALSE #>   ..$ unit       : chr \"time\" #>   ..$ gr         : chr \"NA\" #>   ..$ subgr      : chr \"series\" #>   ..$ label      : language AR() #>   ..- attr(*, \"class\")= chr \"mvgam_trend\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : chr [1:3] \"series_1\" \"series_2\" \"series_3\" #>  $ train_observations:List of 3 #>   ..$ series_1: int [1:75] 1 2 1 0 0 0 1 0 2 2 ... #>   ..$ series_2: int [1:75] 3 0 3 0 3 2 0 4 2 1 ... #>   ..$ series_3: int [1:75] 3 0 3 1 1 1 0 2 1 1 ... #>  $ train_times       :List of 3 #>   ..$ series_1: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_2: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_3: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations : NULL #>  $ test_times        : NULL #>  $ hindcasts         :List of 3 #>   ..$ series_1: num [1:1000, 1:75] 1 3 1 1 1 0 1 3 1 3 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>   ..$ series_2: num [1:1000, 1:75] 2 6 2 3 4 3 10 2 5 3 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,2]\" \"ypred[2,2]\" \"ypred[3,2]\" \"ypred[4,2]\" ... #>   ..$ series_3: num [1:1000, 1:75] 7 5 7 6 3 6 3 6 2 3 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,3]\" \"ypred[2,3]\" \"ypred[3,3]\" \"ypred[4,3]\" ... #>  $ forecasts         : NULL #>  - attr(*, \"class\")= chr \"mvgam_forecast\" head(summary(hc), 12) #> # A tibble: 12 × 7 #>    series    time predQ50 predQ2.5 predQ97.5 truth type     #>    <fct>    <int>   <dbl>    <dbl>     <dbl> <int> <chr>    #>  1 series_1     1       2        0         7     1 response #>  2 series_1     2       1        0         5     2 response #>  3 series_1     3       1        0         4     1 response #>  4 series_1     4       0        0         2     0 response #>  5 series_1     5       0        0         2     0 response #>  6 series_1     6       0        0         3     0 response #>  7 series_1     7       0        0         3     1 response #>  8 series_1     8       1        0         4     0 response #>  9 series_1     9       1        0         5     2 response #> 10 series_1    10       2        0         6     2 response #> 11 series_1    11       2        0         6     2 response #> 12 series_1    12       1        0         5     0 response plot(hc, series = 1) #> No non-missing values in test_observations; cannot calculate forecast score  plot(hc, series = 2) #> No non-missing values in test_observations; cannot calculate forecast score  plot(hc, series = 3) #> No non-missing values in test_observations; cannot calculate forecast score   # Hindcasts as expectations hc <- hindcast(mod, type = 'expected') head(summary(hc), 12) #> # A tibble: 12 × 6 #>    series    time predQ50 predQ2.5 predQ97.5 type     #>    <fct>    <int>   <dbl>    <dbl>     <dbl> <chr>    #>  1 series_1     1   2.28    0.758       5.18 expected #>  2 series_1     2   1.51    0.497       3.87 expected #>  3 series_1     3   0.829   0.218       2.35 expected #>  4 series_1     4   0.313   0.0500      1.20 expected #>  5 series_1     5   0.272   0.0367      1.11 expected #>  6 series_1     6   0.466   0.0942      1.70 expected #>  7 series_1     7   0.697   0.152       2.25 expected #>  8 series_1     8   0.797   0.179       2.63 expected #>  9 series_1     9   1.27    0.356       3.65 expected #> 10 series_1    10   1.75    0.570       4.45 expected #> 11 series_1    11   1.84    0.624       4.66 expected #> 12 series_1    12   1.35    0.252       3.64 expected plot(hc, series = 1)  plot(hc, series = 2)  plot(hc, series = 3)   # Estimated latent trends hc <- hindcast(mod, type = 'trend') head(summary(hc), 12) #> # A tibble: 12 × 6 #>    series    time predQ50 predQ2.5 predQ97.5 type  #>    <fct>    <int>   <dbl>    <dbl>     <dbl> <chr> #>  1 series_1     1 -0.591    -1.82    0.282   trend #>  2 series_1     2 -0.179    -1.35    0.777   trend #>  3 series_1     3  0.0563   -1.37    1.13    trend #>  4 series_1     4 -0.423    -2.09    0.979   trend #>  5 series_1     5 -0.557    -2.54    0.724   trend #>  6 series_1     6 -0.440    -2.14    0.864   trend #>  7 series_1     7 -0.338    -1.98    0.826   trend #>  8 series_1     8 -0.418    -1.98    0.672   trend #>  9 series_1     9  0.0877   -1.16    1.14    trend #> 10 series_1    10  0.198    -0.924   1.13    trend #> 11 series_1    11 -0.452    -1.71    0.505   trend #> 12 series_1    12 -1.14     -2.87   -0.00818 trend plot(hc, series = 1)  plot(hc, series = 2)  plot(hc, series = 3)  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/how_to_cite.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a methods description for mvgam models — how_to_cite.mvgam","title":"Generate a methods description for mvgam models — how_to_cite.mvgam","text":"Create brief fully referenced methods description, along useful list references, fitted mvgam jsdgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/how_to_cite.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a methods description for mvgam models — how_to_cite.mvgam","text":"","code":"how_to_cite(object, ...)  # S3 method for mvgam how_to_cite(object, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/how_to_cite.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a methods description for mvgam models — how_to_cite.mvgam","text":"object list object class mvgam resulting call mvgam() jsdgam() ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/how_to_cite.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a methods description for mvgam models — how_to_cite.mvgam","text":"object class how_to_cite containing text description methods well lists primary additional references","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/how_to_cite.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a methods description for mvgam models — how_to_cite.mvgam","text":"function uses model's structure come basic hopefully useful methods description can help users appropriately acknowledge hard work developers champion open science. Please consider text returned function completely adequate methods section, meant get started.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/how_to_cite.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate a methods description for mvgam models — how_to_cite.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/how_to_cite.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a methods description for mvgam models — how_to_cite.mvgam","text":"","code":"# \\donttest{ # Simulate 4 time series with hierarchical seasonality # and a VAR(1) dynamic process set.seed(0) simdat <- sim_mvgam(seasonality = 'hierarchical',                     trend_model = VAR(cor = TRUE),                     family = gaussian())  # Fit an appropriate model mod1 <- mvgam(y ~ s(season, bs = 'cc', k = 6),               data = simdat$data_train,               family = gaussian(),               trend_model = VAR(cor = TRUE),               chains = 2,               silent = 2) how_to_cite(mod1) #> Methods text skeleton #> We used the R package mvgam (version 1.1.57; Clark & Wells, 2023) to #>   construct, fit and interrogate the model. mvgam fits Bayesian #>   State-Space models that can include flexible predictor effects in both #>   the process and observation components by incorporating functionalities #>   from the brms (Burkner 2017), mgcv (Wood 2017) and splines2 (Wang & Yan, #>   2023) packages. To encourage stability and prevent forecast variance #>   from increasing indefinitely, we enforced stationarity of the Vector #>   Autoregressive process following methods described by Heaps (2023) and #>   Clark et al. (2025). The mvgam-constructed model and observed data were #>   passed to the probabilistic programming environment Stan (version #>   2.36.0; Carpenter et al. 2017, Stan Development Team 2025), specifically #>   through the cmdstanr interface (Gabry & Cesnovar, 2021). We ran 2 #>   Hamiltonian Monte Carlo chains for 500 warmup iterations and 500 #>   sampling iterations for joint posterior estimation. Rank normalized #>   split Rhat (Vehtari et al. 2021) and effective sample sizes were used to #>   monitor convergence. #>  #> Primary references #> Clark, NJ and Wells K (2023). Dynamic Generalized Additive Models #>   (DGAMs) for forecasting discrete ecological time series. Methods in #>   Ecology and Evolution, 14, 771-784. doi.org/10.1111/2041-210X.13974 #> Burkner, PC (2017). brms: An R Package for Bayesian Multilevel Models #>   Using Stan. Journal of Statistical Software, 80(1), 1-28. #>   doi:10.18637/jss.v080.i01 #> Wood, SN (2017). Generalized Additive Models: An Introduction with R #>   (2nd edition). Chapman and Hall/CRC. #> Wang W and Yan J (2021). Shape-Restricted Regression Splines with R #>   Package splines2. Journal of Data Science, 19(3), 498-517. #>   doi:10.6339/21-JDS1020 https://doi.org/10.6339/21-JDS1020. #> Heaps, SE (2023). Enforcing stationarity through the prior in vector #>   autoregressions. Journal of Computational and Graphical Statistics 32, #>   74-83. #> Clark NJ, Ernest SKM, Senyondo H, Simonis J, White EP, Yenni GM, #>   Karunarathna KANK (2025). Beyond single-species models: leveraging #>   multispecies forecasts to navigate the dynamics of ecological #>   predictability. PeerJ 13:e18929. #> Carpenter B, Gelman A, Hoffman MD, Lee D, Goodrich B, Betancourt M, #>   Brubaker M, Guo J, Li P and Riddell A (2017). Stan: A probabilistic #>   programming language. Journal of Statistical Software 76. #> Gabry J, Cesnovar R, Johnson A, and Bronder S (2025). cmdstanr: R #>   Interface to 'CmdStan'. https://mc-stan.org/cmdstanr/, #>   https://discourse.mc-stan.org. #> Vehtari A, Gelman A, Simpson D, Carpenter B, and Burkner P (2021). #>   Rank-normalization, folding, and localization: An improved Rhat for #>   assessing convergence of MCMC (with discussion). Bayesian Analysis 16(2) #>   667-718. https://doi.org/10.1214/20-BA1221. #>  #> Other useful references #> Arel-Bundock V, Greifer N, and Heiss A (2024). How to interpret #>   statistical models using marginaleffects for R and Python. Journal of #>   Statistical Software, 111(9), 1-32. #>   https://doi.org/10.18637/jss.v111.i09 #> Gabry J, Simpson D, Vehtari A, Betancourt M, and Gelman A (2019). #>   Visualization in Bayesian workflow. Journal of the Royal Statatistical #>   Society A, 182, 389-402. doi:10.1111/rssa.12378. #> Vehtari A, Gelman A, and Gabry J (2017). Practical Bayesian model #>   evaluation using leave-one-out cross-validation and WAIC. Statistics and #>   Computing, 27, 1413-1432. doi:10.1007/s11222-016-9696-4. #> Burkner PC, Gabry J, and Vehtari A. (2020). Approximate leave-future-out #>   cross-validation for Bayesian time series models. Journal of Statistical #>   Computation and Simulation, 90(14), 2499-2523. #>   https://doi.org/10.1080/00949655.2020.1783262  # For a GP example, simulate data using the mgcv package dat <- mgcv::gamSim(1, n = 30, scale = 2) #> Gu & Wahba 4 term additive model  # Fit a model that uses an approximate GP from the brms package mod2 <- mvgam(y ~ gp(x2, k = 12),               data = dat,               family = gaussian(),               chains = 2,               silent = 2) #> Warning: gp effects in mvgam cannot yet handle autogrouping #> resetting all instances of 'gr = TRUE' to 'gr = FALSE' #> This warning is displayed once per session. how_to_cite(mod2) #> Methods text skeleton #> We used the R package mvgam (version 1.1.57; Clark & Wells, 2023) to #>   construct, fit and interrogate the model. mvgam fits Bayesian #>   State-Space models that can include flexible predictor effects in both #>   the process and observation components by incorporating functionalities #>   from the brms (Burkner 2017), mgcv (Wood 2017) and splines2 (Wang & Yan, #>   2023) packages. Gaussian Process functional effects were estimated using #>   a low-rank Hilbert space approximation following methods described by #>   Riutort-Mayol et al. (2023). The mvgam-constructed model and observed #>   data were passed to the probabilistic programming environment Stan #>   (version 2.36.0; Carpenter et al. 2017, Stan Development Team 2025), #>   specifically through the cmdstanr interface (Gabry & Cesnovar, 2021). We #>   ran 2 Hamiltonian Monte Carlo chains for 500 warmup iterations and 500 #>   sampling iterations for joint posterior estimation. Rank normalized #>   split Rhat (Vehtari et al. 2021) and effective sample sizes were used to #>   monitor convergence. #>  #> Primary references #> Clark, NJ and Wells K (2023). Dynamic Generalized Additive Models #>   (DGAMs) for forecasting discrete ecological time series. Methods in #>   Ecology and Evolution, 14, 771-784. doi.org/10.1111/2041-210X.13974 #> Burkner, PC (2017). brms: An R Package for Bayesian Multilevel Models #>   Using Stan. Journal of Statistical Software, 80(1), 1-28. #>   doi:10.18637/jss.v080.i01 #> Wood, SN (2017). Generalized Additive Models: An Introduction with R #>   (2nd edition). Chapman and Hall/CRC. #> Wang W and Yan J (2021). Shape-Restricted Regression Splines with R #>   Package splines2. Journal of Data Science, 19(3), 498-517. #>   doi:10.6339/21-JDS1020 https://doi.org/10.6339/21-JDS1020. #> Riutort-Mayol G, Burkner PC, Andersen MR, Solin A and Vehtari A (2023). #>   Practical Hilbert space approximate Bayesian Gaussian processes for #>   probabilistic programming. Statistics and Computing 33, 1. #>   https://doi.org/10.1007/s11222-022-10167-2 #> Carpenter B, Gelman A, Hoffman MD, Lee D, Goodrich B, Betancourt M, #>   Brubaker M, Guo J, Li P and Riddell A (2017). Stan: A probabilistic #>   programming language. Journal of Statistical Software 76. #> Gabry J, Cesnovar R, Johnson A, and Bronder S (2025). cmdstanr: R #>   Interface to 'CmdStan'. https://mc-stan.org/cmdstanr/, #>   https://discourse.mc-stan.org. #> Vehtari A, Gelman A, Simpson D, Carpenter B, and Burkner P (2021). #>   Rank-normalization, folding, and localization: An improved Rhat for #>   assessing convergence of MCMC (with discussion). Bayesian Analysis 16(2) #>   667-718. https://doi.org/10.1214/20-BA1221. #>  #> Other useful references #> Arel-Bundock V, Greifer N, and Heiss A (2024). How to interpret #>   statistical models using marginaleffects for R and Python. Journal of #>   Statistical Software, 111(9), 1-32. #>   https://doi.org/10.18637/jss.v111.i09 #> Gabry J, Simpson D, Vehtari A, Betancourt M, and Gelman A (2019). #>   Visualization in Bayesian workflow. Journal of the Royal Statatistical #>   Society A, 182, 389-402. doi:10.1111/rssa.12378. #> Vehtari A, Gelman A, and Gabry J (2017). Practical Bayesian model #>   evaluation using leave-one-out cross-validation and WAIC. Statistics and #>   Computing, 27, 1413-1432. doi:10.1007/s11222-016-9696-4. #> Burkner PC, Gabry J, and Vehtari A. (2020). Approximate leave-future-out #>   cross-validation for Bayesian time series models. Journal of Statistical #>   Computation and Simulation, 90(14), 2499-2523. #>   https://doi.org/10.1080/00949655.2020.1783262 # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/index-mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Index mvgam objects — index-mvgam","title":"Index mvgam objects — index-mvgam","text":"Index mvgam objects","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/index-mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Index mvgam objects — index-mvgam","text":"","code":"# S3 method for mvgam variables(x, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/index-mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Index mvgam objects — index-mvgam","text":"x list object returned mvgam. See mvgam() ... Arguments passed individual methods (applicable).","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/index-mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Index mvgam objects — index-mvgam","text":"list object variables can extracted, along aliases","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/index-mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Index mvgam objects — index-mvgam","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 1, trend_model = 'AR1') mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),              trend_model = AR(),              data = simdat$data_train,             chains = 2,             silent = 2) variables(mod) #> $observation_pars #> NULL #>  #> $observation_linpreds #>    orig_name alias #> 1   mus[1,1]    NA #> 2   mus[2,1]    NA #> 3   mus[3,1]    NA #> 4   mus[4,1]    NA #> 5   mus[5,1]    NA #> 6   mus[6,1]    NA #> 7   mus[7,1]    NA #> 8   mus[8,1]    NA #> 9   mus[9,1]    NA #> 10 mus[10,1]    NA #> 11 mus[11,1]    NA #> 12 mus[12,1]    NA #> 13 mus[13,1]    NA #> 14 mus[14,1]    NA #> 15 mus[15,1]    NA #> 16 mus[16,1]    NA #> 17 mus[17,1]    NA #> 18 mus[18,1]    NA #> 19 mus[19,1]    NA #> 20 mus[20,1]    NA #> 21 mus[21,1]    NA #> 22 mus[22,1]    NA #> 23 mus[23,1]    NA #> 24 mus[24,1]    NA #> 25 mus[25,1]    NA #> 26 mus[26,1]    NA #> 27 mus[27,1]    NA #> 28 mus[28,1]    NA #> 29 mus[29,1]    NA #> 30 mus[30,1]    NA #> 31 mus[31,1]    NA #> 32 mus[32,1]    NA #> 33 mus[33,1]    NA #> 34 mus[34,1]    NA #> 35 mus[35,1]    NA #> 36 mus[36,1]    NA #> 37 mus[37,1]    NA #> 38 mus[38,1]    NA #> 39 mus[39,1]    NA #> 40 mus[40,1]    NA #> 41 mus[41,1]    NA #> 42 mus[42,1]    NA #> 43 mus[43,1]    NA #> 44 mus[44,1]    NA #> 45 mus[45,1]    NA #> 46 mus[46,1]    NA #> 47 mus[47,1]    NA #> 48 mus[48,1]    NA #> 49 mus[49,1]    NA #> 50 mus[50,1]    NA #> 51 mus[51,1]    NA #> 52 mus[52,1]    NA #> 53 mus[53,1]    NA #> 54 mus[54,1]    NA #> 55 mus[55,1]    NA #> 56 mus[56,1]    NA #> 57 mus[57,1]    NA #> 58 mus[58,1]    NA #> 59 mus[59,1]    NA #> 60 mus[60,1]    NA #> 61 mus[61,1]    NA #> 62 mus[62,1]    NA #> 63 mus[63,1]    NA #> 64 mus[64,1]    NA #> 65 mus[65,1]    NA #> 66 mus[66,1]    NA #> 67 mus[67,1]    NA #> 68 mus[68,1]    NA #> 69 mus[69,1]    NA #> 70 mus[70,1]    NA #> 71 mus[71,1]    NA #> 72 mus[72,1]    NA #> 73 mus[73,1]    NA #> 74 mus[74,1]    NA #> 75 mus[75,1]    NA #>  #> $observation_betas #>   orig_name       alias #> 1      b[1] (Intercept) #> 2      b[2] s(season).1 #> 3      b[3] s(season).2 #> 4      b[4] s(season).3 #> 5      b[5] s(season).4 #>  #> $observation_smoothpars #>   orig_name         alias #> 1    rho[1] s(season)_rho #>  #> $observation_re_params #> NULL #>  #> $posterior_preds #>      orig_name alias #> 1   ypred[1,1]    NA #> 2   ypred[2,1]    NA #> 3   ypred[3,1]    NA #> 4   ypred[4,1]    NA #> 5   ypred[5,1]    NA #> 6   ypred[6,1]    NA #> 7   ypred[7,1]    NA #> 8   ypred[8,1]    NA #> 9   ypred[9,1]    NA #> 10 ypred[10,1]    NA #> 11 ypred[11,1]    NA #> 12 ypred[12,1]    NA #> 13 ypred[13,1]    NA #> 14 ypred[14,1]    NA #> 15 ypred[15,1]    NA #> 16 ypred[16,1]    NA #> 17 ypred[17,1]    NA #> 18 ypred[18,1]    NA #> 19 ypred[19,1]    NA #> 20 ypred[20,1]    NA #> 21 ypred[21,1]    NA #> 22 ypred[22,1]    NA #> 23 ypred[23,1]    NA #> 24 ypred[24,1]    NA #> 25 ypred[25,1]    NA #> 26 ypred[26,1]    NA #> 27 ypred[27,1]    NA #> 28 ypred[28,1]    NA #> 29 ypred[29,1]    NA #> 30 ypred[30,1]    NA #> 31 ypred[31,1]    NA #> 32 ypred[32,1]    NA #> 33 ypred[33,1]    NA #> 34 ypred[34,1]    NA #> 35 ypred[35,1]    NA #> 36 ypred[36,1]    NA #> 37 ypred[37,1]    NA #> 38 ypred[38,1]    NA #> 39 ypred[39,1]    NA #> 40 ypred[40,1]    NA #> 41 ypred[41,1]    NA #> 42 ypred[42,1]    NA #> 43 ypred[43,1]    NA #> 44 ypred[44,1]    NA #> 45 ypred[45,1]    NA #> 46 ypred[46,1]    NA #> 47 ypred[47,1]    NA #> 48 ypred[48,1]    NA #> 49 ypred[49,1]    NA #> 50 ypred[50,1]    NA #> 51 ypred[51,1]    NA #> 52 ypred[52,1]    NA #> 53 ypred[53,1]    NA #> 54 ypred[54,1]    NA #> 55 ypred[55,1]    NA #> 56 ypred[56,1]    NA #> 57 ypred[57,1]    NA #> 58 ypred[58,1]    NA #> 59 ypred[59,1]    NA #> 60 ypred[60,1]    NA #> 61 ypred[61,1]    NA #> 62 ypred[62,1]    NA #> 63 ypred[63,1]    NA #> 64 ypred[64,1]    NA #> 65 ypred[65,1]    NA #> 66 ypred[66,1]    NA #> 67 ypred[67,1]    NA #> 68 ypred[68,1]    NA #> 69 ypred[69,1]    NA #> 70 ypred[70,1]    NA #> 71 ypred[71,1]    NA #> 72 ypred[72,1]    NA #> 73 ypred[73,1]    NA #> 74 ypred[74,1]    NA #> 75 ypred[75,1]    NA #>  #> $trend_pars #>   orig_name alias #> 1    ar1[1]    NA #> 2  sigma[1]    NA #>  #> $trend_linpreds #> NULL #>  #> $trend_betas #> NULL #>  #> $trend_smoothpars #> NULL #>  #> $trend_re_params #> NULL #>  #> $trends #>      orig_name alias #> 1   trend[1,1]    NA #> 2   trend[2,1]    NA #> 3   trend[3,1]    NA #> 4   trend[4,1]    NA #> 5   trend[5,1]    NA #> 6   trend[6,1]    NA #> 7   trend[7,1]    NA #> 8   trend[8,1]    NA #> 9   trend[9,1]    NA #> 10 trend[10,1]    NA #> 11 trend[11,1]    NA #> 12 trend[12,1]    NA #> 13 trend[13,1]    NA #> 14 trend[14,1]    NA #> 15 trend[15,1]    NA #> 16 trend[16,1]    NA #> 17 trend[17,1]    NA #> 18 trend[18,1]    NA #> 19 trend[19,1]    NA #> 20 trend[20,1]    NA #> 21 trend[21,1]    NA #> 22 trend[22,1]    NA #> 23 trend[23,1]    NA #> 24 trend[24,1]    NA #> 25 trend[25,1]    NA #> 26 trend[26,1]    NA #> 27 trend[27,1]    NA #> 28 trend[28,1]    NA #> 29 trend[29,1]    NA #> 30 trend[30,1]    NA #> 31 trend[31,1]    NA #> 32 trend[32,1]    NA #> 33 trend[33,1]    NA #> 34 trend[34,1]    NA #> 35 trend[35,1]    NA #> 36 trend[36,1]    NA #> 37 trend[37,1]    NA #> 38 trend[38,1]    NA #> 39 trend[39,1]    NA #> 40 trend[40,1]    NA #> 41 trend[41,1]    NA #> 42 trend[42,1]    NA #> 43 trend[43,1]    NA #> 44 trend[44,1]    NA #> 45 trend[45,1]    NA #> 46 trend[46,1]    NA #> 47 trend[47,1]    NA #> 48 trend[48,1]    NA #> 49 trend[49,1]    NA #> 50 trend[50,1]    NA #> 51 trend[51,1]    NA #> 52 trend[52,1]    NA #> 53 trend[53,1]    NA #> 54 trend[54,1]    NA #> 55 trend[55,1]    NA #> 56 trend[56,1]    NA #> 57 trend[57,1]    NA #> 58 trend[58,1]    NA #> 59 trend[59,1]    NA #> 60 trend[60,1]    NA #> 61 trend[61,1]    NA #> 62 trend[62,1]    NA #> 63 trend[63,1]    NA #> 64 trend[64,1]    NA #> 65 trend[65,1]    NA #> 66 trend[66,1]    NA #> 67 trend[67,1]    NA #> 68 trend[68,1]    NA #> 69 trend[69,1]    NA #> 70 trend[70,1]    NA #> 71 trend[71,1]    NA #> 72 trend[72,1]    NA #> 73 trend[73,1]    NA #> 74 trend[74,1]    NA #> 75 trend[75,1]    NA #>  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/irf.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate latent VAR impulse response functions — irf.mvgam","title":"Calculate latent VAR impulse response functions — irf.mvgam","text":"Compute Generalized Orthogonalized Impulse Response Functions (IRFs) mvgam models Vector Autoregressive dynamics","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/irf.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate latent VAR impulse response functions — irf.mvgam","text":"","code":"irf(object, ...)  # S3 method for mvgam irf(object, h = 10, cumulative = FALSE, orthogonal = FALSE, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/irf.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate latent VAR impulse response functions — irf.mvgam","text":"object list object class mvgam resulting call mvgam() used Vector Autoregressive latent process model (either VAR(cor = FALSE) VAR(cor = TRUE); see VAR() details) ... ignored h Positive integer specifying forecast horizon calculate IRF cumulative Logical flag indicating whether IRF cumulative orthogonal Logical flag indicating whether orthogonalized IRFs calculated. Note order variables matters calculating ","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/irf.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate latent VAR impulse response functions — irf.mvgam","text":"object mvgam_irf-class containing posterior IRFs. object can used supplied S3 functions plot.mvgam_irf() summary.mvgam_irf()","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/irf.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate latent VAR impulse response functions — irf.mvgam","text":"See mvgam_irf-class full description quantities computed returned function, along key references.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/irf.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate latent VAR impulse response functions — irf.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/irf.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate latent VAR impulse response functions — irf.mvgam","text":"","code":"# \\donttest{ # Fit a model to the portal time series that uses a latent VAR(1) mod <- mvgam(   formula = captures ~ -1,   trend_formula = ~ trend,   trend_model = VAR(cor = TRUE),   family = poisson(),   data = portal_data,   chains = 2,   silent = 2 )  # Plot the autoregressive coefficient distributions; # use 'dir = \"v\"' to arrange the order of facets # correctly mcmc_plot(   mod,   variable = 'A',   regex = TRUE,   type = 'hist',   facet_args = list(dir = 'v') ) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # Calulate Generalized IRFs for each series irfs <- irf(   mod,   h = 12,   cumulative = FALSE )  # Plot them plot(irfs, series = 1)  plot(irfs, series = 2)  plot(irfs, series = 3)  plot(irfs, series = 4)   # Calculate posterior median, upper and lower 95th quantiles # of the impulse responses summary(irfs) #> # A tibble: 192 × 5 #>    shock                horizon irfQ50 irfQ2.5 irfQ97.5 #>    <chr>                  <int>  <dbl>   <dbl>    <dbl> #>  1 Process1 -> Process1       1 0.144  0.0916     0.211 #>  2 Process1 -> Process1       2 0.131  0.0871     0.192 #>  3 Process1 -> Process1       3 0.117  0.0784     0.172 #>  4 Process1 -> Process1       4 0.105  0.0662     0.157 #>  5 Process1 -> Process1       5 0.0926 0.0536     0.145 #>  6 Process1 -> Process1       6 0.0819 0.0416     0.137 #>  7 Process1 -> Process1       7 0.0716 0.0292     0.128 #>  8 Process1 -> Process1       8 0.0636 0.0190     0.120 #>  9 Process1 -> Process1       9 0.0554 0.0108     0.114 #> 10 Process1 -> Process1      10 0.0483 0.00346    0.108 #> # ℹ 182 more rows # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/jsdgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Joint Species Distribution Models in mvgam — jsdgam","title":"Fit Joint Species Distribution Models in mvgam — jsdgam","text":"function sets Joint Species Distribution Model whereby residual associations among species can modelled reduced-rank format using set latent factors. factor specification extremely flexible, allowing users include spatial, temporal type predictor effects efficiently capture unmodelled residual associations, observation model can also highly flexible (including smooth, GP effects mvgam can handle)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/jsdgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Joint Species Distribution Models in mvgam — jsdgam","text":"","code":"jsdgam(   formula,   factor_formula = ~-1,   knots,   factor_knots,   data,   newdata,   family = poisson(),   unit = time,   species = series,   share_obs_params = FALSE,   priors,   n_lv = 2,   backend = getOption(\"brms.backend\", \"cmdstanr\"),   algorithm = getOption(\"brms.algorithm\", \"sampling\"),   control = list(max_treedepth = 10, adapt_delta = 0.8),   chains = 4,   burnin = 500,   samples = 500,   thin = 1,   parallel = TRUE,   threads = 1,   silent = 1,   run_model = TRUE,   return_model_data = FALSE,   residuals = TRUE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/jsdgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Joint Species Distribution Models in mvgam — jsdgam","text":"formula formula object specifying GAM observation model formula. exactly like formula GLM except smooth terms, s(), te(), ti(), t2(), well time-varying dynamic() terms, nonparametric gp() terms offsets using offset(), can added right hand side specify linear predictor depends smooth functions predictors (linear functionals ). Details formula syntax used mvgam can found mvgam_formulae factor_formula formula object specifying linear predictor effects latent factors. Use = trend within calls functional terms (.e. s(), te(), ti(), t2(), dynamic(), gp()) ensure factor captures different axis variation. See example illustration knots optional list containing user specified knot values used basis construction. bases user simply supplies knots used, must match k value supplied (note number knots always just k). Different terms can use different numbers knots, unless share covariate factor_knots optional list containing user specified knot values used basis construction smooth terms factor_formula. bases user simply supplies knots used, must match k value supplied (note number knots always just k). Different terms can use different numbers knots, unless share covariate data dataframe list containing model response variable covariates required GAM formula factor_formula objects newdata Optional dataframe list test data containing variables data. included, observations variable y set NA fitting model posterior simulations can obtained family family specifying observation family outcomes. Currently supported families : gaussian() real-valued data betar() proportional data (0,1) lognormal() non-negative real-valued data student_t() real-valued data Gamma() non-negative real-valued data bernoulli() binary data poisson() count data nb() overdispersed count data binomial() count data imperfect detection number trials known; note cbind() function must used bind discrete observations discrete number trials beta_binomial() binomial() allows overdispersion Default poisson(). See mvgam_families details unit unquoted name variable represents unit analysis data latent residuals correlated. variable either numeric integer variable supplied data. Defaults time consistent functionalities mvgam, though note data need time series case. See examples details explanations species unquoted name factor variable indexes different response units data (usually 'species' JSDM). Defaults series consistent mvgam models share_obs_params logical. TRUE family additional family-specific observation parameters (e.g. variance components student_t() gaussian(), dispersion parameters nb() betar()), parameters shared across outcome variables. handy multiple outcomes (time series mvgam models) believe share properties, species different spatial units. Default FALSE. priors optional data.frame prior definitions (Stan syntax) , preferentially, vector containing objects class brmsprior (see. prior details). See get_mvgam_priors information changing default prior distributions n_lv integer number latent factors use modelling residual associations. > n_species. Defaults arbitrarily 2 backend Character string naming package use backend fitting Stan model. Options \"cmdstanr\" (default) \"rstan\". Can set globally current R session via \"brms.backend\" option (see options). Details rstan cmdstanr packages available https://mc-stan.org/rstan/ https://mc-stan.org/cmdstanr/, respectively algorithm Character string naming estimation approach use. Options \"sampling\" MCMC (default), \"meanfield\" variational inference factorized normal distributions, \"fullrank\" variational inference multivariate normal distribution, \"laplace\" Laplace approximation (available using cmdstanr backend) \"pathfinder\" pathfinder algorithm (currently available using cmdstanr backend). Can set globally current R session via \"brms.algorithm\" option (see options). Limited testing suggests \"meanfield\" performs best non-MCMC approximations dynamic GAMs, possibly difficulties estimating covariances among many spline parameters latent trend parameters. rigorous testing carried control named list controlling sampler's behaviour. Valid elements include max_treedepth, adapt_delta init chains integer specifying number parallel chains model. Ignored algorithm %% c('meanfield', 'fullrank', 'pathfinder', 'laplace') burnin integer specifying number warmup iterations Markov chain run tune sampling algorithms. Ignored algorithm %% c('meanfield', 'fullrank', 'pathfinder', 'laplace') samples integer specifying number post-warmup iterations Markov chain run sampling posterior distribution thin Thinning interval monitors.  Ignored algorithm %% c('meanfield', 'fullrank', 'pathfinder', 'laplace') parallel logical specifying whether multiple cores used generating MCMC simulations parallel. TRUE, number cores use min(c(chains, parallel::detectCores() - 1)) threads integer Experimental option use multithreading within-chain parallelisation Stan. recommend use experienced Stan's reduce_sum function slow running model sped means. Currently works families using Cmdstan backend silent Verbosity level 0 2. 1 (default), informational messages compiler sampler suppressed. 2, even messages suppressed. actual sampling progress still printed. Set refresh = 0 turn well. using backend = \"rstan\" can also set open_progress = FALSE prevent opening additional progress bars. run_model logical. FALSE, model fitted instead function return model file data / initial values needed fit model outside mvgam return_model_data logical. TRUE, list data needed fit model returned, along initial values smooth AR parameters, model fitted. helpful users wish modify model file add stochastic elements currently available mvgam. Default FALSE reduce size returned object, unless run_model == FALSE residuals Logical indicating whether compute series-level randomized quantile residuals include part returned object. Defaults TRUE, can set FALSE save computational time reduce size returned object (users can always add residuals object class mvgam using add_residuals) ... arguments pass mvgam","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/jsdgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Joint Species Distribution Models in mvgam — jsdgam","text":"list object class mvgam containing model output, text representation model file, mgcv model output (easily generating simulations unsampled covariate values), Dunn-Smyth residuals species key information needed functions package. See mvgam-class details. Use methods(class = \"mvgam\") overview available methods","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/jsdgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit Joint Species Distribution Models in mvgam — jsdgam","text":"Joint Species Distribution Models allow responses multiple species learned hierarchically, whereby responses environmental variables formula can partially pooled latent, unmodelled residual associations can also learned. mvgam, effects can modelled full power latent factor Hierarchical GAMs, providing unmatched flexibility model full communities species. calling jsdgam, initial State-Space model using trend = 'None' set modified include latent factors linear predictors. Consequently, can inspect priors models using get_mvgam_priors supplying relevant formula, factor_formula, data family arguments keeping default trend = 'None'. JSDGAM, expectation response \\(Y_{ij}\\) modelled $$g(\\mu_{ij}) = X_i\\beta + u_i\\theta_j,$$ \\(g(.)\\) known link function, \\(X\\) design matrix linear predictors (associated \\(\\beta\\) coefficients), \\(u\\) \\(n_{lv}\\)-variate latent factors (\\(n_{lv}\\)<<\\(n_{species}\\)) \\(\\theta_j\\) species-specific loadings latent factors, respectively. design matrix \\(X\\) \\(\\beta\\) coefficients constructed modelled using formula can contain mvgam's predictor effects, including random intercepts slopes, multidimensional penalized smooths, GP effects etc... factor loadings \\(\\theta_j\\) constrained identifiability can used reconstruct estimate species' residual variance-covariance matrix using \\(\\Theta \\Theta'\\) (see example residual_cor() details). latent factors modelled using: $$ u_i \\sim \\text{Normal}(Q_i\\beta_{factor}, 1) \\quad $$ second design matrix \\(Q\\) associated \\(\\beta_{factor}\\) coefficients constructed modelled using factor_formula. , effects make linear predictor can contain mvgam's allowed predictor effects, providing enormous flexibility modelling species' communities.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/jsdgam.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit Joint Species Distribution Models in mvgam — jsdgam","text":"Nicholas J Clark & Konstans Wells (2023). Dynamic generalised additive models (DGAMs) forecasting discrete ecological time series. Methods Ecology Evolution. 14:3, 771-784.  David Warton, F Guillaume Blanchet, Robert B O’Hara, Otso Ovaskainen, Sara Taskinen, Steven C Walker & Francis KC Hui (2015). many variables: joint modeling community ecology. Trends Ecology & Evolution 30:12, 766-779.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/jsdgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit Joint Species Distribution Models in mvgam — jsdgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/jsdgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Joint Species Distribution Models in mvgam — jsdgam","text":"","code":"# \\donttest{ # Fit a JSDGAM to the portal_data captures mod <- jsdgam(   formula = captures ~     # Fixed effects of NDVI and mintemp, row effect as a GP of time     ndvi_ma12:series + mintemp:series + gp(time, k = 15),   factor_formula = ~ -1,   data = portal_data,   unit = time,   species = series,   family = poisson(),   n_lv = 2,   silent = 2,   chains = 2 )  # Plot covariate effects library(ggplot2); theme_set(theme_bw()) plot_predictions(  mod,  condition = c('ndvi_ma12','series', 'series') )   plot_predictions(  mod,  condition = c('mintemp','series', 'series') )   # A residual correlation plot plot(   residual_cor(mod) )   # An ordination biplot can also be constructed # from the factor scores and their loadings if(requireNamespace('ggrepel', quietly = TRUE)){   ordinate(mod, alpha = 0.7) } #> Registered S3 methods overwritten by 'ggpp': #>   method                  from    #>   heightDetails.titleGrob ggplot2 #>   widthDetails.titleGrob  ggplot2 #> Warning: ggrepel: 12 unlabeled data points (too many overlaps). Consider increasing max.overlaps    # A more complicated example showing how to include predictors # in the factor_formula  # Simulate latent count data for 500 spatial locations and 10 species set.seed(0) N_points <- 500 N_species <- 10  # Species-level intercepts (on the log scale) alphas <- runif(N_species, 2, 2.25)  # Simulate a covariate and species-level responses to it temperature <- rnorm(N_points) betas <- runif(N_species, -0.5, 0.5)  # Simulate points uniformly over a space lon <- runif(N_points, min = 150, max = 155) lat <- runif(N_points, min = -20, max = -19)  # Set up spatial basis functions as a tensor product of lat and lon sm <- mgcv::smoothCon(mgcv::te(lon, lat, k = 5),                       data = data.frame(lon, lat),                       knots = NULL)[[1]]  # The design matrix for this smooth is in the 'X' slot des_mat <- sm$X dim(des_mat) #> [1] 500  25  # Function to generate a random covariance matrix where all variables # have unit variance (i.e. diagonals are all 1) random_Sigma = function(N){   L_Omega <- matrix(0, N, N);   L_Omega[1, 1] <- 1;   for (i in 2 : N) {     bound <- 1;     for (j in 1 : (i - 1)) {       L_Omega[i, j] <- runif(1, -sqrt(bound), sqrt(bound));       bound <- bound - L_Omega[i, j] ^ 2;     }     L_Omega[i, i] <- sqrt(bound);   }   Sigma <- L_Omega %*% t(L_Omega);   return(Sigma) }  # Simulate a variance-covariance matrix for the correlations among # basis coefficients Sigma <- random_Sigma(N = NCOL(des_mat))  # Now simulate the species-level basis coefficients hierarchically, where # spatial basis function correlations are a convex sum of a base correlation # matrix and a species-level correlation matrix basis_coefs <- matrix(NA, nrow = N_species, ncol = NCOL(Sigma)) base_field <- mgcv::rmvn(1, mu = rep(0, NCOL(Sigma)), V = Sigma) for(t in 1:N_species){   corOmega <- (cov2cor(Sigma) * 0.7) +                  (0.3 * cov2cor(random_Sigma(N = NCOL(des_mat))))   basis_coefs[t, ] <- mgcv::rmvn(1, mu = rep(0, NCOL(Sigma)), V = corOmega) }  # Simulate the latent spatial processes st_process <- do.call(rbind, lapply(seq_len(N_species), function(t){   data.frame(lat = lat,              lon = lon,              species = paste0('species_', t),              temperature = temperature,              process = alphas[t] +                betas[t] * temperature +                des_mat %*% basis_coefs[t,]) }))  # Now take noisy observations at some of the points (60) obs_points <- sample(1:N_points, size = 60, replace = FALSE) obs_points <- data.frame(lat = lat[obs_points],                          lon = lon[obs_points],                          site = 1:60)  # Keep only the process data at these points st_process %>%   dplyr::inner_join(obs_points, by = c('lat', 'lon')) %>%   # now take noisy Poisson observations of the process   dplyr::mutate(count = rpois(NROW(.), lambda = exp(process))) %>%   dplyr::mutate(species = factor(species,                                  levels = paste0('species_', 1:N_species))) %>%   dplyr::group_by(lat, lon) -> dat  # View the count distributions for each species ggplot(dat, aes(x = count)) +   geom_histogram() +   facet_wrap(~ species, scales = 'free') #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   ggplot(dat, aes(x = lon, y = lat, col = log(count + 1))) +   geom_point(size = 2.25) +   facet_wrap(~ species, scales = 'free') +   scale_color_viridis_c()   # Inspect default priors for a joint species model with three spatial factors priors <- get_mvgam_priors(formula = count ~                             # Environmental model includes random slopes for                             # a linear effect of temperature                             s(species, bs = 're', by = temperature),                            # Each factor estimates a different nonlinear spatial process, using                           # 'by = trend' as in other mvgam State-Space models                           factor_formula = ~ gp(lon, lat, k = 6, by = trend) - 1,                           n_lv = 3,                            # The data and grouping variables                           data = dat,                           unit = site,                           species = species,                            # Poisson observations                           family = poisson()) head(priors) #>                                            param_name param_length #> 1                                         (Intercept)            1 #> 2                                   vector[1] mu_raw;            1 #> 3                       vector<lower=0>[1] sigma_raw;            1 #> 4 real<lower=0> alpha_gp_trend(lon, lat):trendtrend1;            1 #> 5 real<lower=0> alpha_gp_trend(lon, lat):trendtrend2;            1 #> 6 real<lower=0> alpha_gp_trend(lon, lat):trendtrend3;            1 #>                                    param_info #> 1                                 (Intercept) #> 2             s(species):temperature pop mean #> 3               s(species):temperature pop sd #> 4 gp(lon, lat):trendtrend1 marginal deviation #> 5 gp(lon, lat):trendtrend2 marginal deviation #> 6 gp(lon, lat):trendtrend3 marginal deviation #>                                                          prior #> 1                        (Intercept) ~ student_t(3, 2.1, 2.5); #> 2                                       mu_raw ~ std_normal(); #> 3                         sigma_raw ~ inv_gamma(1.418, 0.452); #> 4 alpha_gp_trend(lon, lat):trendtrend1 ~ student_t(3, 0, 2.5); #> 5 alpha_gp_trend(lon, lat):trendtrend2 ~ student_t(3, 0, 2.5); #> 6 alpha_gp_trend(lon, lat):trendtrend3 ~ student_t(3, 0, 2.5); #>                                            example_change new_lowerbound #> 1                             (Intercept) ~ normal(0, 1);           <NA> #> 2                            mu_raw ~ normal(0.65, 0.15);           <NA> #> 3                          sigma_raw ~ exponential(0.27);           <NA> #> 4 alpha_gp_trend(lon, lat):trendtrend1 ~ normal(0, 0.86);           <NA> #> 5 alpha_gp_trend(lon, lat):trendtrend2 ~ normal(0, 0.84);           <NA> #> 6 alpha_gp_trend(lon, lat):trendtrend3 ~ normal(0, 0.78);           <NA> #>   new_upperbound #> 1           <NA> #> 2           <NA> #> 3           <NA> #> 4           <NA> #> 5           <NA> #> 6           <NA>  # Fit a JSDM that estimates hierarchical temperature responses # and that uses three latent spatial factors mod <- jsdgam(formula = count ~                 # Environmental model includes random slopes for a                 # linear effect of temperature                 s(species, bs = 're', by = temperature),                # Each factor estimates a different nonlinear spatial process, using               # 'by = trend' as in other mvgam State-Space models               factor_formula = ~ gp(lon, lat, k = 6, by = trend) - 1,               n_lv = 3,                # Change default priors for fixed random effect variances and               # factor GP marginal deviations to standard normal               priors = c(prior(std_normal(),                                class = sigma_raw),                          prior(std_normal(),                                class = `alpha_gp_trend(lon, lat):trendtrend1`),                          prior(std_normal(),                                class = `alpha_gp_trend(lon, lat):trendtrend2`),                          prior(std_normal(),                                class = `alpha_gp_trend(lon, lat):trendtrend3`)),                # The data and the grouping variables               data = dat,               unit = site,               species = species,                # Poisson observations               family = poisson(),               chains = 2,               silent = 2)  # Plot the implicit species-level intercept estimates plot_predictions(mod, condition = 'species',                  type = 'link')   # Plot species' hierarchical responses to temperature plot_predictions(mod, condition = c('temperature', 'species', 'species'),                  type = 'link')   # Plot posterior median estimates of the latent spatial factors plot(mod, type = 'smooths', trend_effects = TRUE)   # Or using gratia, if you have it installed if(requireNamespace('gratia', quietly = TRUE)){   gratia::draw(mod, trend_effects = TRUE, dist = 0) }   # Plot species' randomized quantile residual distributions # as a function of latitude pp_check(mod,          type = 'resid_ribbon_grouped',          group = 'species',          x = 'lat',          ndraws = 200)   # Calculate residual spatial correlations post_cors <- residual_cor(mod) names(post_cors) #>  [1] \"cor\"        \"cor_lower\"  \"cor_upper\"  \"sig_cor\"    \"cov\"        #>  [6] \"prec\"       \"prec_lower\" \"prec_upper\" \"sig_prec\"   \"trace\"      # Look at lower and upper credible interval estimates for # some of the estimated correlations post_cors$cor[1:5, 1:5] #>            species_1  species_2  species_3  species_4  species_5 #> species_1  1.0000000  0.7478131  0.7077590  0.7584940 -0.2226666 #> species_2  0.7478131  1.0000000  0.1847152  0.4121104 -0.4646831 #> species_3  0.7077590  0.1847152  1.0000000  0.8893192 -0.0605947 #> species_4  0.7584940  0.4121104  0.8893192  1.0000000 -0.4702592 #> species_5 -0.2226666 -0.4646831 -0.0605947 -0.4702592  1.0000000 post_cors$cor_upper[1:5, 1:5] #>           species_1  species_2 species_3  species_4  species_5 #> species_1 1.0000000  0.9490105 0.9502660  0.9530000  0.2554001 #> species_2 0.9490105  1.0000000 0.5245856  0.7150460 -0.1225558 #> species_3 0.9502660  0.5245856 1.0000000  0.9724250  0.3063924 #> species_4 0.9530000  0.7150460 0.9724250  1.0000000 -0.1192868 #> species_5 0.2554001 -0.1225558 0.3063924 -0.1192868  1.0000000 post_cors$cor_lower[1:5, 1:5] #>            species_1  species_2  species_3  species_4  species_5 #> species_1  1.0000000  0.4303950  0.2587196  0.2762048 -0.5832629 #> species_2  0.4303950  1.0000000 -0.2033176  0.0140300 -0.7246617 #> species_3  0.2587196 -0.2033176  1.0000000  0.7341198 -0.4171416 #> species_4  0.2762048  0.0140300  0.7341198  1.0000000 -0.7275820 #> species_5 -0.5832629 -0.7246617 -0.4171416 -0.7275820  1.0000000  # Plot of the posterior median correlations for those estimated # to be non-zero plot(post_cors, cluster = TRUE)   # An ordination biplot can also be constructed # from the factor scores and their loadings if(requireNamespace('ggrepel', quietly = TRUE)){   ordinate(mod) }   # Posterior predictive checks and ELPD-LOO can ascertain model fit pp_check(mod,          type = \"pit_ecdf_grouped\",          group = \"species\",          ndraws = 200)  loo(mod) #> Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. #>  #> Computed from 1000 by 600 log-likelihood matrix. #>  #>          Estimate    SE #> elpd_loo  -4466.9 231.6 #> p_loo      2267.8 191.4 #> looic      8933.8 463.3 #> ------ #> MCSE of elpd_loo is NA. #> MCSE and ESS estimates assume MCMC draws (r_eff in [0.0, 1.0]). #>  #> Pareto k diagnostic values: #>                           Count Pct.    Min. ESS #> (-Inf, 0.67]   (good)     322   53.7%   1        #>    (0.67, 1]   (bad)       88   14.7%   <NA>     #>     (1, Inf)   (very bad) 190   31.7%   <NA>     #> See help('pareto-k-diagnostic') for details.  # Forecast log(counts) for entire region (site value doesn't matter as long # as each spatial location has a different and unique site identifier); # note this calculation takes a few minutes because of the need to calculate # draws from the stochastic latent factors newdata <- st_process %>%  dplyr::mutate(species = factor(species,                                 levels = paste0('species_',                                                 1:N_species))) %>%  dplyr::group_by(lat, lon) %>%  dplyr::mutate(site = dplyr::cur_group_id()) %>%  dplyr::ungroup() preds <- predict(mod, newdata = newdata)  # Plot the median log(count) predictions on a grid newdata$log_count <- preds[,1] ggplot(newdata, aes(x = lon, y = lat, col = log_count)) +   geom_point(size = 1.5) +   facet_wrap(~ species, scales = 'free') +   scale_color_viridis_c() +   theme_classic()   # \\dontshow{ # For R CMD check: make sure any open connections are closed afterward  closeAllConnections() # } # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/lfo_cv.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","title":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","text":"Approximate leave-future-cross-validation fitted mvgam objects","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/lfo_cv.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","text":"","code":"lfo_cv(object, ...)  # S3 method for mvgam lfo_cv(   object,   data,   min_t,   fc_horizon = 1,   pareto_k_threshold = 0.7,   silent = 1,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/lfo_cv.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","text":"object list object class mvgam. See mvgam() ... Ignored data dataframe list containing model response variable covariates required GAM formula. include columns: 'series' (character factor index series IDs) 'time' (numeric index time point observation). variables included linear predictor formula must also present min_t Integer specifying minimum training time required making predictions data. Default either 30th timepoint observational data, whatever training time allows least 10 lfo-cv calculations, possible. value essentially arbitrary highly recommended change something suitable data models evaluated. fc_horizon Integer specifying number time steps ahead evaluating forecasts pareto_k_threshold Proportion specifying threshold Pareto shape parameter considered unstable, triggering model refit. Default 0.7 silent Verbosity level 0 2. 1 (default), informational messages compiler sampler suppressed. 2, even messages suppressed. actual sampling progress still printed. Set refresh = 0 turn well. using backend = \"rstan\" can also set open_progress = FALSE prevent opening additional progress bars.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/lfo_cv.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","text":"list class mvgam_lfo containing approximate ELPD scores, Pareto-k shape values 'specified pareto_k_threshold","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/lfo_cv.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","text":"Approximate leave-future-cross-validation uses expanding training window scheme evaluate model forecasting ability. steps used function mirror laid lfo vignette loo package, written Paul Bürkner, Jonah Gabry, Aki Vehtari. First, refit model using first min_t observations perform single exact fc_horizon-ahead forecast step. forecast evaluated min_t + fc_horizon sample observations using Expected Log Predictive Density (ELPD). Next, approximate successive round expanding window forecasts moving forward one step time 1:N_evaluations re-weighting draws model's posterior predictive distribution using Pareto Smoothed Importance Sampling (PSIS). iteration , PSIS weights obtained next observation included model re-fit (.e. last observation training data, min_t + ). importance ratios stable, consider approximation adequate use re-weighted posterior's forecast evaluating next holdout set testing observations ((min_t + + 1):(min_t + + fc_horizon)). point importance ratio variability become large importance sampling fail. indicated estimated shape parameter k generalized Pareto distribution crossing certain threshold pareto_k_threshold. refit model using observations time failure. restart process iterate forward next refit triggered (Bürkner et al. 2020).","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/lfo_cv.mvgam.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","text":"Paul-Christian Bürkner, Jonah Gabry & Aki Vehtari (2020). Approximate leave-future-cross-validation Bayesian time series models Journal Statistical Computation Simulation. 90:14, 2499-2523.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/lfo_cv.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/lfo_cv.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Approximate leave-future-out cross-validation of fitted mvgam objects — lfo_cv.mvgam","text":"","code":"# \\donttest{ # Simulate from a Poisson-AR2 model with a seasonal smooth set.seed(100) dat <- sim_mvgam(T = 75,                 n_series = 1,                 prop_trend = 0.75,                 trend_model = 'AR2',                 family = poisson())  # Plot the time series plot_mvgam_series(data = dat$data_train,                  newdata = dat$data_test,                  series = 1)   # Fit an appropriate model mod_ar2 <- mvgam(y ~ s(season, bs = 'cc', k = 6),                trend_model = AR(p = 2),                family = poisson(),                data = dat$data_train,                newdata = dat$data_test,                chains = 2,                silent = 2)  # Fit a less appropriate model mod_rw <- mvgam(y ~ s(season, bs = 'cc', k = 6),               trend_model = RW(),               family = poisson(),               data = dat$data_train,               newdata = dat$data_test,               chains = 2,               silent = 2)  # Compare Discrete Ranked Probability Scores for the testing period fc_ar2 <- forecast(mod_ar2) fc_rw <- forecast(mod_rw) score_ar2 <- score(fc_ar2, score = 'drps') score_rw <- score(fc_rw, score = 'drps') sum(score_ar2$series_1$score) #> [1] 22.9077 sum(score_rw$series_1$score) #> [1] 61.5392  # Now use approximate leave-future-out CV to compare # rolling forecasts; start at time point 40 to reduce # computational time and to ensure enough data is available # for estimating model parameters lfo_ar2 <- lfo_cv(mod_ar2,                  min_t = 40,                  fc_horizon = 3,                  silent = 2) lfo_rw <- lfo_cv(mod_rw,                 min_t = 40,                 fc_horizon = 3,                 silent = 2)  # Plot Pareto-K values and ELPD estimates plot(lfo_ar2)  plot(lfo_rw)   # Proportion of timepoints in which AR2 model gives better forecasts length(which((lfo_ar2$elpds - lfo_rw$elpds) > 0)) /       length(lfo_ar2$elpds) #> [1] 0.9230769  # A higher total ELPD is preferred lfo_ar2$sum_ELPD #> [1] -76.25362 lfo_rw$sum_ELPD #> [1] -95.04612 # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/logLik.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute pointwise Log-Likelihoods from fitted mvgam objects — logLik.mvgam","title":"Compute pointwise Log-Likelihoods from fitted mvgam objects — logLik.mvgam","text":"Compute pointwise Log-Likelihoods fitted mvgam objects","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/logLik.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute pointwise Log-Likelihoods from fitted mvgam objects — logLik.mvgam","text":"","code":"# S3 method for mvgam logLik(object, linpreds, newdata, family_pars, include_forecast = TRUE, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/logLik.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute pointwise Log-Likelihoods from fitted mvgam objects — logLik.mvgam","text":"object list object class mvgam jsdgam linpreds Optional matrix linear predictor draws use calculating pointwise log-likelihoods newdata Optional data.frame list object specifying series column linpreds belongs . linpreds supplied, newdata must also supplied family_pars Optional list containing posterior draws family-specific parameters (.e. shape, scale overdispersion parameters). Required linpreds newdata supplied include_forecast Logical. newdata fed model compute forecasts, log-likelihood draws observations also returned. Defaults TRUE ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/logLik.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute pointwise Log-Likelihoods from fitted mvgam objects — logLik.mvgam","text":"matrix dimension n_samples x n_observations containing pointwise log-likelihood draws observations newdata. newdata supplied, log-likelihood draws returned observations originally fed model (training observations , supplied original model via newdata argument mvgam, testing observations)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/logLik.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute pointwise Log-Likelihoods from fitted mvgam objects — logLik.mvgam","text":"","code":"# \\donttest{ # Simulate some data and fit a model simdat <- sim_mvgam(n_series = 1, trend_model = 'AR1') mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),              trend_model = AR(),              data = simdat$data_train,              chains = 2,              silent = 2)  # Extract logLikelihood values lls <- logLik(mod) str(lls) #>  num [1:1000, 1:75] -4.23 -4.88 -5.51 -5.44 -4.77 ... # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/loo.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"LOO information criteria for mvgam models — loo.mvgam","title":"LOO information criteria for mvgam models — loo.mvgam","text":"Extract LOOIC (leave-one-information criterion) using loo::loo()","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/loo.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LOO information criteria for mvgam models — loo.mvgam","text":"","code":"# S3 method for mvgam loo(x, incl_dynamics = FALSE, ...)  # S3 method for mvgam loo_compare(x, ..., model_names = NULL, incl_dynamics = FALSE)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/loo.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LOO information criteria for mvgam models — loo.mvgam","text":"x Object class mvgam incl_dynamics Deprecated currently ignored ... mvgam objects. model_names NULL (default) use model names derived deparsing call. Otherwise use passed values model names.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/loo.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"LOO information criteria for mvgam models — loo.mvgam","text":"loo.mvgam, object class psis_loo (see loo::loo() details). loo_compare.mvgam, object class compare.loo ( loo::loo_compare() details)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/loo.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"LOO information criteria for mvgam models — loo.mvgam","text":"comparing two () fitted mvgam models, can estimate difference -sample predictive accuracies using Expcted Log Predictive Density (ELPD). metric can approximated using Pareto Smoothed Importance Sampling (PSIS), method re-weight posterior draws approximate predictions models might made given datapoint datapoint included original model fit (.e. run leave-one-cross-validation made prediction held-datapoint). See details loo::loo() loo::loo_compare() information importance sampling works. Note -sample predictive metrics PSIS-LOO can sometimes overly optimistic models included process error components (trend_model, trend_formula factor_formula included). therefore recommended perhaps use --sample evaluations scrutiny models (see example forecast.mvgam, score.mvgam_forecast lfo_cv)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/loo.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"LOO information criteria for mvgam models — loo.mvgam","text":"","code":"# \\donttest{ # Simulate 4 time series with hierarchical seasonality # and independent AR1 dynamic processes set.seed(111) simdat <- sim_mvgam(seasonality = 'hierarchical',                    trend_model = AR(),                    family = gaussian())  # Fit a model with shared seasonality mod1 <- mvgam(y ~ s(season, bs = 'cc', k = 6),              data = rbind(simdat$data_train,              simdat$data_test),              family = gaussian(),              chains = 2,              silent = 2)  # Inspect the model and calculate LOO conditional_effects(mod1)  mc.cores.def <- getOption('mc.cores') options(mc.cores = 1) loo(mod1) #>  #> Computed from 1000 by 300 log-likelihood matrix. #>  #>          Estimate   SE #> elpd_loo   -364.2 11.3 #> p_loo         7.6  0.6 #> looic       728.4 22.5 #> ------ #> MCSE of elpd_loo is 0.1. #> MCSE and ESS estimates assume MCMC draws (r_eff in [0.8, 1.8]). #>  #> All Pareto k estimates are good (k < 0.67). #> See help('pareto-k-diagnostic') for details.  # Now fit a model with hierarchical seasonality mod2 <- update(mod1,               formula = y ~ s(season, bs = 'cc', k = 6) +               s(season, series, bs = 'fs',               xt = list(bs = 'cc'), k = 4),               chains = 2,               silent = 2) #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. conditional_effects(mod2)   loo(mod2) #>  #> Computed from 1000 by 300 log-likelihood matrix. #>  #>          Estimate   SE #> elpd_loo   -309.5 11.5 #> p_loo        13.3  1.1 #> looic       619.1 23.1 #> ------ #> MCSE of elpd_loo is 0.1. #> MCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.5]). #>  #> All Pareto k estimates are good (k < 0.67). #> See help('pareto-k-diagnostic') for details.  # Now add AR1 dynamic errors to mod2 mod3 <- update(mod2,               trend_model = AR(),               chains = 2,               silent = 2) #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. conditional_effects(mod3)   plot(mod3, type = 'trend')  loo(mod3) #> Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. #>  #> Computed from 1000 by 300 log-likelihood matrix. #>  #>          Estimate   SE #> elpd_loo   -236.6 10.2 #> p_loo       191.3  7.9 #> looic       473.3 20.4 #> ------ #> MCSE of elpd_loo is NA. #> MCSE and ESS estimates assume MCMC draws (r_eff in [0.0, 0.1]). #>  #> Pareto k diagnostic values: #>                           Count Pct.    Min. ESS #> (-Inf, 0.67]   (good)     168   56.0%   1        #>    (0.67, 1]   (bad)      121   40.3%   <NA>     #>     (1, Inf)   (very bad)  11    3.7%   <NA>     #> See help('pareto-k-diagnostic') for details.  # Compare models using LOO loo_compare(mod1, mod2, mod3) #> Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. #>      elpd_diff se_diff #> mod3    0.0       0.0  #> mod2  -72.9       5.6  #> mod1 -127.6       9.5  options(mc.cores = mc.cores.def)  # Compare forecast abilities using an expanding training window and # forecasting ahead 1 timepoint from each window; the first window by includes # the first 92 timepoints (of the 100 that were simulated) max(mod2$obs_data$time) #> [1] 100 lfo_mod2 <- lfo_cv(mod2, min_t = 92) #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 finished in 1.3 seconds. #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 1.4 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 1.3 seconds. #> Total execution time: 1.5 seconds. #>  lfo_mod3 <- lfo_cv(mod3, min_t = 92) #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Your model may benefit from using \"noncentred = TRUE\" #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 14.0 seconds. #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 16.1 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 15.0 seconds. #> Total execution time: 16.2 seconds. #>  #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Your model may benefit from using \"noncentred = TRUE\" #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 16.9 seconds. #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 23.3 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 20.1 seconds. #> Total execution time: 23.4 seconds. #>  #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Your model may benefit from using \"noncentred = TRUE\" #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 15.0 seconds. #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 31.5 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 23.2 seconds. #> Total execution time: 31.6 seconds. #>  #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Your model may benefit from using \"noncentred = TRUE\" #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 29.5 seconds. #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 39.9 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 34.7 seconds. #> Total execution time: 40.0 seconds. #>   # Take the difference in forecast ELPDs; a model with higher ELPD is preferred, # so negative values here indicate that mod3 gave better forecasts for a particular # out of sample timepoint plot(y = lfo_mod2$elpds - lfo_mod3$elpds,     x = lfo_mod2$eval_timepoints, pch = 16,     ylab = 'ELPD_mod2 - ELPD_mod3',     xlab = 'Evaluation timepoint') abline(h = 0, lty = 'dashed')  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/lv_correlations.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate trend correlations based on latent factor loadings for mvgam models — lv_correlations","title":"Calculate trend correlations based on latent factor loadings for mvgam models — lv_correlations","text":"function uses factor loadings fitted dynamic factor mvgam model calculate temporal correlations among series' trends","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/lv_correlations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate trend correlations based on latent factor loadings for mvgam models — lv_correlations","text":"","code":"lv_correlations(object)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/lv_correlations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate trend correlations based on latent factor loadings for mvgam models — lv_correlations","text":"object list object class mvgam used latent factors, either use_lv = TRUE supplying trend_map. See mvgam() details example","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/lv_correlations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate trend correlations based on latent factor loadings for mvgam models — lv_correlations","text":"list object containing mean posterior correlations full array posterior correlations","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/lv_correlations.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate trend correlations based on latent factor loadings for mvgam models — lv_correlations","text":"Although function still work, now recommended use residual_cor() obtain residual correlation information user-friendly format allows deeper investigation relationships among time series.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/lv_correlations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate trend correlations based on latent factor loadings for mvgam models — lv_correlations","text":"","code":"# \\donttest{ # Fit a model that uses two AR(1) dynamic factors to model # the temporal dynamics of the four rodent species in the portal_data mod <- mvgam(captures ~ series,             trend_model = AR(),             use_lv = TRUE,             n_lv = 2,             data = portal_data,             chains = 2,             silent = 2) #> Warning in '/tmp/RtmppUq4oL/model_fd0fdfeaee023a8e623b5071c6b9ec02.stan', line 20, column 31: Found #>     int division: #>       n_lv * (n_lv - 1) / 2 #>     Values will be rounded towards zero. If rounding is not desired you can #>     write #>     the division as #>       n_lv * (n_lv - 1) / 2.0 #>     If rounding is intended please use the integer division operator %/%. #> Warning in '/tmp/RtmppUq4oL/model-22f57ddb5a71.stan', line 20, column 33: Found #>     int division: #>       n_lv * (n_lv - 1) / 2 #>     Values will be rounded towards zero. If rounding is not desired you can #>     write #>     the division as #>       n_lv * (n_lv - 1) / 2.0 #>     If rounding is intended please use the integer division operator %/%.  # Plot the two dynamic factors plot(mod, type = 'factors')  #> # A tibble: 2 × 2 #>   Factor   Contribution #>   <chr>           <dbl> #> 1 Factor 1        0.532 #> 2 Factor 2        0.468  # Calculate correlations among the series using lv_correlations() lvcors <- lv_correlations(mod) names(lvcors) #> [1] \"mean_correlations\"      \"posterior_correlations\" lapply(lvcors, class) #> $mean_correlations #> [1] \"matrix\" \"array\"  #>  #> $posterior_correlations #> [1] \"list\" #>   # The above works, but it is now recommended to use the more # flexible and informative residual_cor() function to # calculate and work with these correlations lvcors <- residual_cor(mod) names(lvcors) #>  [1] \"cor\"        \"cor_lower\"  \"cor_upper\"  \"sig_cor\"    \"cov\"        #>  [6] \"prec\"       \"prec_lower\" \"prec_upper\" \"sig_prec\"   \"trace\"      lvcors$cor #>            DM          DO         PB           PP #> DM  1.0000000 0.538926931 -0.5026149 -0.813423512 #> DO  0.5389269 1.000000000  0.4153731  0.005680965 #> PB -0.5026149 0.415373088  1.0000000  0.895825721 #> PP -0.8134235 0.005680965  0.8958257  1.000000000  # For those correlations whose credible intervals did not include # zero, plot them as a correlation matrix (all other correlations # are shown as zero on this plot) plot(lvcors, cluster = TRUE)   # \\dontshow{ # For R CMD check: make sure any open connections are closed afterward  closeAllConnections() # } # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mcmc_plot.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"MCMC plots of mvgam parameters, as implemented in bayesplot — mcmc_plot.mvgam","title":"MCMC plots of mvgam parameters, as implemented in bayesplot — mcmc_plot.mvgam","text":"Convenient way call MCMC plotting functions implemented bayesplot package mvgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mcmc_plot.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MCMC plots of mvgam parameters, as implemented in bayesplot — mcmc_plot.mvgam","text":"","code":"# S3 method for mvgam mcmc_plot(   object,   type = \"intervals\",   variable = NULL,   regex = FALSE,   use_alias = TRUE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mcmc_plot.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MCMC plots of mvgam parameters, as implemented in bayesplot — mcmc_plot.mvgam","text":"object R object typically class brmsfit type type plot. Supported types (names) hist, dens, hist_by_chain, dens_overlay, violin, intervals, areas, areas_ridges, combo, acf, acf_bar, trace, trace_highlight, scatter, hex, pairs, violin, rhat, rhat_hist, neff, neff_hist nuts_energy. overview various plot types see MCMC-overview. variable Names variables (parameters) plot, given character vector regular expression (regex = TRUE). default, hopefully large selection variables plotted. regex Logical; Indicates whether variable treated regular expressions. Defaults FALSE. use_alias Logical. informative names parameters available (.e. beta coefficients b smoothing parameters rho), replace uninformative names informative alias. Defaults TRUE ... Additional arguments passed plotting functions. See MCMC-overview details.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mcmc_plot.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MCMC plots of mvgam parameters, as implemented in bayesplot — mcmc_plot.mvgam","text":"ggplot object can customized using ggplot2 package.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mcmc_plot.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"MCMC plots of mvgam parameters, as implemented in bayesplot — mcmc_plot.mvgam","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 1, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),              trend_model = AR(),              noncentred = TRUE,              data = simdat$data_train,              chains = 2,              silent = 2) mcmc_plot(mod)  mcmc_plot(mod, type = 'neff_hist') #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  mcmc_plot(mod, variable = 'betas', type = 'areas')  mcmc_plot(mod, variable = 'trend_params', type = 'combo')  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/model.frame.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract model.frame from a fitted mvgam object — model.frame.mvgam","title":"Extract model.frame from a fitted mvgam object — model.frame.mvgam","text":"Extract model.frame fitted mvgam object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/model.frame.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract model.frame from a fitted mvgam object — model.frame.mvgam","text":"","code":"# S3 method for mvgam model.frame(formula, trend_effects = FALSE, ...)  # S3 method for mvgam_prefit model.frame(formula, trend_effects = FALSE, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/model.frame.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract model.frame from a fitted mvgam object — model.frame.mvgam","text":"formula model formula terms     object R object. trend_effects logical, return model.frame observation model (FALSE) underlying process model (ifTRUE) ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/model.frame.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract model.frame from a fitted mvgam object — model.frame.mvgam","text":"matrix containing fitted model frame","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/model.frame.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract model.frame from a fitted mvgam object — model.frame.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":null,"dir":"Reference","previous_headings":"","what":"Monotonic splines in mvgam models — monotonic","title":"Monotonic splines in mvgam models — monotonic","text":"Uses constructors package splines2 build monotonically increasing decreasing splines. Details also Wang & Yan (2021).","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Monotonic splines in mvgam models — monotonic","text":"","code":"# S3 method for moi.smooth.spec smooth.construct(object, data, knots)  # S3 method for mod.smooth.spec smooth.construct(object, data, knots)  # S3 method for moi.smooth Predict.matrix(object, data)  # S3 method for mod.smooth Predict.matrix(object, data)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Monotonic splines in mvgam models — monotonic","text":"object smooth specification object, usually generated term s(x, bs = \"moi\", ...) s(x, bs = \"mod\", ...) data list containing just data (including variable) required term,              names corresponding object$term (object$). variable              last element. knots list containing knots supplied basis setup --- order names data.               Can NULL. See details information.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Monotonic splines in mvgam models — monotonic","text":"object class \"moi.smooth\" \"mod.smooth\". addition usual elements smooth class documented smooth.construct, object contain slot called boundary defines endpoints beyond spline begin extrapolating (extrapolation flat due first order penalty placed smooth function)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Monotonic splines in mvgam models — monotonic","text":"constructor normally called directly, rather used internally mvgam. supplied knots spline placed evenly throughout covariate values term refers: example, fitting 101 data 11 knot spline x knot every 10th (ordered) x value. spline implementation closed-form -spline basis based recursion formula given Ramsay (1988), basis coefficients must constrained either non-negative (monotonically increasing functions) non-positive (monotonically decreasing)  Take note using either monotonic basis, number basis functions k must supplied even integer due manner monotonic basis functions constructed","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Monotonic splines in mvgam models — monotonic","text":"constructor result valid smooth using call gam bam, however resulting functions guaranteed monotonic constraints basis coefficients enforced","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Monotonic splines in mvgam models — monotonic","text":"Wang, Wenjie, Jun Yan. \"Shape-Restricted Regression Splines R Package splines2.\" Journal Data Science 19.3 (2021).  Ramsay, J. O. (1988). Monotone regression splines action. Statistical Science, 3(4), 425--441.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Monotonic splines in mvgam models — monotonic","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/monotonic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Monotonic splines in mvgam models — monotonic","text":"","code":"# \\donttest{ # Simulate data from a monotonically increasing function set.seed(123123) x <- runif(80) * 4 - 1 x <- sort(x) f <- exp(4 * x) / (1 + exp(4 * x)) y <- f + rnorm(80) * 0.1 plot(x, y)   # A standard TRPS smooth doesn't capture monotonicity library(mgcv) #> Loading required package: nlme #> This is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'. #>  #> Attaching package: ‘mgcv’ #> The following objects are masked from ‘package:mvgam’: #>  #>     betar, nb mod_data <- data.frame(y = y, x = x) mod <- gam(y ~ s(x, k = 16),            data = mod_data,            family = gaussian())  library(marginaleffects) plot_predictions(mod,                  by = 'x',                  newdata = data.frame(x = seq(min(x) - 0.5,                                               max(x) + 0.5,                                               length.out = 100)),                  points = 0.5)   # Using the 'moi' basis in mvgam rectifies this mod_data$time <- 1:NROW(mod_data) mod2 <- mvgam(y ~ s(x, bs = 'moi', k = 18),              data = mod_data,              family = gaussian(),              chains = 2,              silent = 2)  plot_predictions(mod2,                  by = 'x',                  newdata = data.frame(x = seq(min(x) - 0.5,                                               max(x) + 0.5,                                               length.out = 100)),                  points = 0.5)   plot(mod2, type = 'smooth', realisations = TRUE)   # 'by' terms that produce a different smooth for each level of the 'by' # factor are also allowed set.seed(123123) x <- runif(80) * 4 - 1 x <- sort(x)  # Two different monotonic smooths, one for each factor level f <- exp(4 * x) / (1 + exp(4 * x)) f2 <- exp(3.5 * x) / (1 + exp(3 * x)) fac <- c(rep('a', 80), rep('b', 80)) y <- c(f + rnorm(80) * 0.1,        f2 + rnorm(80) * 0.2) plot(x, y[1:80])  plot(x, y[81:160])   # Gather all data into a data.frame, including the factor 'by' variable mod_data <- data.frame(y, x, fac = as.factor(fac)) mod_data$time <- 1:NROW(mod_data)  # Fit a model with different smooths per factor level mod <- mvgam(y ~ s(x, bs = 'moi', by = fac, k = 8),              data = mod_data,              family = gaussian(),              chains = 2,              silent = 2)  # Visualise the different monotonic functions plot_predictions(mod, condition = c('x', 'fac', 'fac'),                  points = 0.5)  plot(mod, type = 'smooth', realisations = TRUE)   # First derivatives (on the link scale) should never be # negative for either factor level (derivs <- slopes(mod, variables = 'x',                  by = c('x', 'fac'),                  type = 'link')) #>  #>       x fac Estimate  2.5 % 97.5 % #>  -0.987   a    0.350 0.1047  0.821 #>  -0.987   b    0.195 0.0326  0.655 #>  -0.841   a    0.515 0.3306  0.795 #>  -0.841   b    0.296 0.1419  0.572 #>  -0.796   a    0.555 0.3785  0.806 #> --- 150 rows omitted. See ?print.marginaleffects ---  #>   2.853   b    3.044 1.2379  4.666 #>   2.870   a    0.235 0.0317  0.882 #>   2.870   b    3.189 1.0952  5.021 #>   2.879   a    0.239 0.0294  0.916 #>   2.879   b    3.251 1.0162  5.213 #> Term: x #> Type:  link  #> Comparison: dY/dX #>  all(derivs$estimate > 0) #> [1] TRUE # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted mvgam object description — mvgam-class","title":"Fitted mvgam object description — mvgam-class","text":"fitted mvgam object returned function mvgam. Run methods(class = \"mvgam\") see overview available methods.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam-class.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fitted mvgam object description — mvgam-class","text":"mvgam object contains following elements: call original observation model formula trend_call trend_formula supplied, original trend model formula returned. Otherwise NULL family character description observation distribution trend_model character description latent trend model trend_map data.frame describing mapping trend states observations, supplied original model. Otherwise NULL drift Logical specifying whether drift term used trend model priors model priors updated defaults, prior dataframe returned. Otherwise NULL model_output MCMC object returned fitting engine. model fitted using Stan, object class stanfit (see stanfit-class details). JAGS used backend, object class runjags (see runjags-class details) model_file character string model file used describe model either Stan JAGS syntax model_data return_model_data set TRUE fitting model, list object containing data objects needed condition model returned. item list described detail top model_file. Otherwise NULL inits return_model_data set TRUE fitting model, initial value functions used initialise MCMC chains returned. Otherwise NULL monitor_pars parameters monitored MCMC sampling returned character vector sp_names character vector specifying names smoothing parameter mgcv_model object class gam containing mgcv version observation model. object used generating linear predictor matrix making predictions new data. coefficients model object contain posterior median coefficients GAM linear predictor, used generating plots smooth functions mvgam currently handle (plots three-dimensional smooths). model therefore used inference. See gamObject details trend_mgcv_model trend_formula supplied, object class gam containing mgcv version trend model. Otherwise NULL ytimes matrix object used model fitting indexing series timepoints observed row supplied data. Used internally downstream plotting prediction functions resids named list object containing posterior draws Dunn-Smyth randomized quantile residuals use_lv Logical flag indicating whether latent dynamic factors used model n_lv use_lv == TRUE, number latent dynamic factors used model upper_bounds bounds supplied original model fit, returned. Otherwise NULL obs_data original data object (either list dataframe) supplied model fitting. test_data test data supplied (argument newdata original model), returned. Othwerise NULL fit_engine Character describing fit engine, either stan jags backend Character describing backend used modelling, either rstan, cmdstanr rjags algorithm Character describing algorithm used finding posterior, either sampling, laplace, pathfinder, meanfield fullrank max_treedepth model fitted using Stan, value supplied maximum treedepth tuning parameter returned (see stan details). Otherwise NULL adapt_delta model fitted using Stan, value supplied adapt_delta tuning parameter returned (see stan details). Otherwise NULL","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam-class.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fitted mvgam object description — mvgam-class","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam-package.html","id":null,"dir":"Reference","previous_headings":"","what":"mvgam: Multivariate (Dynamic) Generalized Additive Models — mvgam-package","title":"mvgam: Multivariate (Dynamic) Generalized Additive Models — mvgam-package","text":"Fit Bayesian Dynamic Generalized Additive Models multivariate observations. Users can build nonlinear State-Space models can incorporate semiparametric effects observation process components, using wide range observation families. Estimation performed using Markov Chain Monte Carlo Hamiltonian Monte Carlo software 'Stan'. References: Clark & Wells (2023) doi:10.1111/2041-210X.13974 .","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mvgam: Multivariate (Dynamic) Generalized Additive Models — mvgam-package","text":"Maintainer: Nicholas J Clark nicholas.j.clark1214@gmail.com (ORCID) contributors: Sarah Heaps (ORCID) (VARMA parameterisations) [contributor] Scott Pease (ORCID) (broom enhancements) [contributor] Matthijs Hollanders (ORCID) (ggplot visualizations) [contributor]","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","title":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","text":"function estimates posterior distribution Generalised Additive Models (GAMs) can include smooth spline functions, specified GAM formula, well latent temporal processes, specified trend_model. modelling options include State-Space representations allow covariates dynamic processes occur latent 'State' level also capturing observation-level effects. Prior specifications flexible explicitly encourage users apply prior distributions actually reflect beliefs. addition, model fits can easily assessed compared posterior predictive checks, forecast comparisons leave-one-/ leave-future-cross-validation.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","text":"","code":"mvgam(   formula,   trend_formula,   knots,   trend_knots,   trend_model = \"None\",   noncentred = FALSE,   family = poisson(),   share_obs_params = FALSE,   data,   newdata,   use_lv = FALSE,   n_lv,   trend_map,   priors,   run_model = TRUE,   prior_simulation = FALSE,   residuals = TRUE,   return_model_data = FALSE,   backend = getOption(\"brms.backend\", \"cmdstanr\"),   algorithm = getOption(\"brms.algorithm\", \"sampling\"),   control = list(max_treedepth = 10, adapt_delta = 0.8),   chains = 4,   burnin = 500,   samples = 500,   thin = 1,   parallel = TRUE,   threads = 1,   save_all_pars = FALSE,   silent = 1,   autoformat = TRUE,   refit = FALSE,   lfo = FALSE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","text":"formula formula object specifying GAM observation model formula. exactly like formula GLM except smooth terms, s(), te(), ti(), t2(), well time-varying dynamic() terms, nonparametric gp() terms offsets using offset(), can added right hand side specify linear predictor depends smooth functions predictors (linear functionals ). nmix() family models, formula used set linear predictor detection probability. Details formula syntax used mvgam can found mvgam_formulae trend_formula optional formula object specifying GAM process model formula. supplied, linear predictor modelled latent trends capture process model evolution separately observation model. response variable specified left-hand side formula (.e. valid option ~ season + s(year)). Also note use identifier series formula specify effects vary across time series. Instead use trend. ensure models trend_map supplied still work consistently (.e. allowing effects vary across process models, even time series share underlying process model). feature currently available RW(), AR() VAR() trend models. nmix() family models, trend_formula used set linear predictor underlying latent abundance. aware can challenging simultaneously estimate intercept parameters observation mode (captured formula) process model (captured trend_formula). Users recommended drop one using - 1 convention formula right hand side. knots optional list containing user specified knot values used basis construction. bases user simply supplies knots used, must match k value supplied (note number knots always just k). Different terms can use different numbers knots, unless share covariate trend_knots knots , optional list knot values smooth functions within trend_formula trend_model character  function specifying time series dynamics latent trend. Options : None (latent trend component; .e. GAM component contributes linear predictor, observation process source error; similarly estimated gam) ZMVN ZMVN() (Zero-Mean Multivariate Normal; available Stan) 'RW' RW() 'AR1' AR(p = 1) 'AR2' AR(p = 2) 'AR3' AR(p = 3) 'CAR1' CAR(p = 1) (also known Ornstein–Uhlenbeck process) 'VAR1'  VAR()(available Stan) 'PWlogistic, 'PWlinear' PW() (available Stan) 'GP' GP() (Gaussian Process squared exponential kernel; available Stan) trend types apart ZMVN(), GP(), CAR() PW(), moving average /correlated process error terms can also estimated (example, RW(cor = TRUE) set multivariate Random Walk n_series > 1). also possible many multivariate trends estimate hierarchical correlations data structured among levels relevant grouping factor. See mvgam_trends details see ZMVN() example. noncentred logical Use non-centred parameterisation autoregressive trend models? Setting TRUE reparameterise model avoid possible degeneracies can show estimating latent dynamic random effects. models, can produce big gains efficiency, meaning fewer burnin sampling iterations required posterior exploration. models, data highly informative latent dynamic processes, can actually lead worse performance. available certain trend models (.e. RW(), AR(), CAR(), trend = 'None' using trend_formula). yet available moving average correlated error models family family specifying exponential observation family series. Currently supported families : gaussian() real-valued data betar() proportional data (0,1) lognormal() non-negative real-valued data student_t() real-valued data Gamma() non-negative real-valued data bernoulli() binary data poisson() count data nb() overdispersed count data binomial() count data imperfect detection number trials known; note cbind() function must used bind discrete observations discrete number trials beta_binomial() binomial() allows overdispersion nmix() count data imperfect detection number trials unknown modeled via State-Space N-Mixture model. latent states Poisson, capturing 'true' latent abundance, observation process Binomial account imperfect detection. See mvgam_families example use family Default poisson(). See mvgam_families details share_obs_params logical. TRUE family additional family-specific observation parameters (e.g. variance components student_t() gaussian(), dispersion parameters nb() betar()), parameters shared across outcome variables. handy multiple outcomes (time series mvgam models) believe share properties, species different spatial units. Default FALSE. data dataframe list containing model response variable covariates required GAM formula optional trend_formula. models include columns: series (factor index series IDs; number levels identical number unique series labels (.e. n_series = length(levels(data$series)))) time (numeric integer index time point observation). dynamic trend types available mvgam (see argument trend_model), time measured discrete, regularly spaced intervals (.e. c(1, 2, 3, ...)). However can use irregularly spaced intervals using trend_model = CAR(1), though note temporal intervals exactly 0 adjusted small number (1e-12) prevent sampling errors. See example CAR() trends CAR() Note however special cases identifiers needed. example, models hierarchical temporal correlation processes (e.g. AR(gr = region, subgr = species)) include series identifier, constructed internally (see mvgam_trends AR() details). mvgam() can also fit models include time variable temporal dynamic structures included (.e. trend_model = 'None' trend_model = ZMVN()). data also include variables included linear predictor formula newdata Optional dataframe list test data containing variables data. included, observations variable y set NA fitting model posterior simulations can obtained use_lv logical. TRUE, use dynamic factors estimate series' latent trends reduced dimension format. available RW(), AR() GP() trend models. Defaults FALSE. See lv_correlations worked example n_lv integer number latent dynamic factors use use_lv == TRUE. > n_series. Defaults arbitrarily min(2, floor(n_series / 2)) trend_map Optional data.frame specifying series depend latent trends. Useful allowing multiple series depend latent trend process, different observation processes. supplied, latent factor model set setting use_lv = TRUE using mapping set shared trends. Needs column names series trend, integer values trend column state trend series depend . series column single unique entry series data (names perfectly match factor levels series variable data). Note supplied, intercept parameter process model automatically suppressed. yet supported models latent factors evolve continuous time (CAR()). See examples details priors optional data.frame prior definitions , preferentially, vector containing objects class brmsprior (see. prior() details). See get_mvgam_priors() Details' information changing default prior distributions run_model logical. FALSE, model fitted instead function return model file data / initial values needed fit model outside mvgam prior_simulation logical. TRUE, observations fed model, instead simulations prior distributions returned residuals Logical indicating whether compute series-level randomized quantile residuals include part returned object. Defaults TRUE, can set FALSE save computational time reduce size returned object (users can always add residuals object class mvgam using add_residuals) return_model_data logical. TRUE, list data needed fit model returned, along initial values smooth AR parameters, model fitted. helpful users wish modify model file add stochastic elements currently available mvgam. Default FALSE reduce size returned object, unless run_model == FALSE backend Character string naming package use backend fitting Stan model. Options \"cmdstanr\" (default) \"rstan\". Can set globally current R session via \"brms.backend\" option (see options). Details rstan cmdstanr packages available https://mc-stan.org/rstan/ https://mc-stan.org/cmdstanr/, respectively algorithm Character string naming estimation approach use. Options \"sampling\" MCMC (default), \"meanfield\" variational inference factorized normal distributions, \"fullrank\" variational inference multivariate normal distribution, \"laplace\" Laplace approximation (available using cmdstanr backend) \"pathfinder\" pathfinder algorithm (currently available using cmdstanr backend). Can set globally current R session via \"brms.algorithm\" option (see options). Limited testing suggests \"meanfield\" performs best non-MCMC approximations dynamic GAMs, possibly difficulties estimating covariances among many spline parameters latent trend parameters. rigorous testing carried control named list controlling sampler's behaviour. Valid elements include max_treedepth, adapt_delta init chains integer specifying number parallel chains model. Ignored algorithm %% c('meanfield', 'fullrank', 'pathfinder', 'laplace') burnin integer specifying number warmup iterations Markov chain run tune sampling algorithms. Ignored algorithm %% c('meanfield', 'fullrank', 'pathfinder', 'laplace') samples integer specifying number post-warmup iterations Markov chain run sampling posterior distribution thin Thinning interval monitors.  Ignored algorithm %% c('meanfield', 'fullrank', 'pathfinder', 'laplace') parallel logical specifying whether multiple cores used generating MCMC simulations parallel. TRUE, number cores use min(c(chains, parallel::detectCores() - 1)) threads integer Experimental option use multithreading within-chain parallelisation Stan. recommend use experienced Stan's reduce_sum function slow running model sped means. Currently works families apart nmix() using Cmdstan backend save_all_pars Logical flag indicate draws variables defined Stan's parameters block saved (default FALSE). silent Verbosity level 0 2. 1 (default), informational messages compiler sampler suppressed. 2, even messages suppressed. actual sampling progress still printed. Set refresh = 0 turn well. using backend = \"rstan\" can also set open_progress = FALSE prevent opening additional progress bars. autoformat Logical. Use stanc parser automatically format Stan code check deprecations. development purposes, leave TRUE refit Logical indicating whether refit, called using update.mvgam(). Users leave FALSE lfo Logical indicating whether part call lfo_cv.mvgam. Returns lighter version model residuals fewer monitored parameters speed post-processing. downstream functions work properly, users always leave set FALSE ... arguments passed Stan. backend = \"rstan\" arguments passed sampling() vb(). backend = \"cmdstanr\" arguments passed cmdstanr::sample, cmdstanr::variational, cmdstanr::laplace cmdstanr::pathfinder method","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","text":"list object class mvgam containing model output, text representation model file, mgcv model output (easily generating simulations unsampled covariate values), Dunn-Smyth residuals series key information needed functions package. See mvgam-class details. Use methods(class = \"mvgam\") overview available methods.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","text":"Dynamic GAMs useful wish predict future values time series show temporal dependence want rely extrapolating smooth term (can sometimes lead unpredictable unrealistic behaviours). addition, smooths can often try wiggle excessively capture autocorrelation present time series, exacerbates problem forecasting ahead. GAMs naturally viewed Bayesian lens, often must model time series show complex distributional features missing data, parameters mvgam models estimated Bayesian framework using Markov Chain Monte Carlo default. general overview provided primary vignettes: vignette(\"mvgam_overview\") vignette(\"data_in_mvgam\"). full list available vignettes see vignette(package = \"mvgam\") Formula syntax: Details formula syntax used mvgam can found mvgam_formulae. Note possible supply empty formula predictors intercepts observation model (.e. y ~ 0 y ~ -1). case, intercept-observation model set intercept coefficient fixed zero. can handy wish fit pure State-Space models variation dynamic trend controls average expectation, /intercepts non-identifiable (piecewise trends, see examples ) Families link functions: Details families supported mvgam can found mvgam_families. Trend models: Details latent error process models supported mvgam can found mvgam_trends. Priors: Default priors intercepts variance parameters chosen vaguely informative, always checked user. Prior distributions important model parameters can altered (see get_mvgam_priors() details). Note latent trends estimated link scale choose priors accordingly. However control model specification can accomplished setting run_model = FALSE editing model code ( found model_file slot returned object) running model using either rstan cmdstanr. encouraged complex modelling tasks. Note, priors formally checked ensure right syntax user ensure correct Random effects: smooth terms using random effect basis (smooth.construct.re.smooth.spec), non-centred parameterisation automatically employed avoid degeneracies common hierarchical models. Note however centred versions may perform better series particularly informative, foray Bayesian modelling, worth building understanding model's assumptions limitations following principled workflow. Also note models parameterised using drop.unused.levels = FALSE jagam ensure predictions can made levels supplied factor variable Observation level parameters: one series included data observation family contains one parameter used, additional observation family parameters (.e. phi nb() sigma gaussian()) default estimated independently series. wish series share observation parameters, set share_obs_params = TRUE Residuals: series, randomized quantile (.e. Dunn-Smyth) residuals calculated inspecting model diagnostics fitted model appropriate Dunn-Smyth residuals standard normal distribution autocorrelation evident. particular observation missing, residual calculated comparing independent draws model's posterior distribution Using Stan: mvgam primarily designed use Hamiltonian Monte Carlo parameter estimation via software Stan (using either cmdstanr rstan interface). great advantages using Stan Gibbs / Metropolis Hastings samplers, includes option estimate nonlinear effects via Hilbert space approximate Gaussian Processes, availability variety inference algorithms (.e. variational inference, laplacian inference etc...) capabilities enforce stationarity complex Vector Autoregressions. many advantages Stan JAGS, development package applied Stan. includes planned addition response distributions, plans handle zero-inflation, plans incorporate greater variety trend models. Users strongly encouraged opt Stan JAGS proceeding workflows start?: mvgam cheatsheet good starting place just learning use package. gives overview package's key functions objects, well providing reasonable workflow new users can follow. general recommended 1. Check time series data suitable tidy format mvgam modeling (see data formatting vignette guidance) 2. Inspect features data using plot_mvgam_series. Now also good time familiarise package's example workflows detailed vignettes. particular, getting started vignette, shared latent states vignette, time-varying effects vignette State-Space models vignette provide useful information structure, fit interrogate Dynamic Generalized Additive Models mvgam. specialized -articles include \"Fitting N-mixture models mgam, \"Joint Species Distribution Models mgam, \"Incorporating time-varying seasonality forecast models\" \"Temporal autocorrelation GAMs mvgam package\" 3. Carefully think structure linear predictor effects (.e. smooth terms using s(), te() ti(), GPs using gp(), dynamic time-varying effects using dynamic(), parametric terms), latent temporal trend components (see mvgam_trends) appropriate observation family (see mvgam_families). Use get_mvgam_priors() see default prior distributions stochastic parameters 4. Change default priors using appropriate prior knowledge (see prior()). using State-Space models trend_formula, pay particular attention priors variance parameters process errors observation errors. Default priors parameters chosen vaguely informative avoid zero (using Inverse Gamma priors), informative priors often help model efficiency convergence 5. Fit model using either Hamiltonian Monte Carlo approximation algorithm (.e. change backend argument) use summary.mvgam(), conditional_effects.mvgam(), mcmc_plot.mvgam(), pp_check.mvgam(), pairs.mvgam() plot.mvgam() inspect / interrogate model 6. Update model needed use loo_compare.mvgam() -sample model comparisons, alternatively use forecast.mvgam(), lfo_cv.mvgam() score.mvgam_forecast() compare models based --sample forecasts (see forecast evaluation vignette guidance) 7. satisfied model structure, use predict.mvgam(), plot_predictions() /plot_slopes() targeted inferences (see \"interpret report nonlinear effects Generalized Additive Models\" guidance interpreting GAMs) 8. Use how_to_cite() obtain scaffold methods section (full references) begin describing model scientific publications","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","text":"Nicholas J Clark & Konstans Wells (2023). Dynamic generalised additive models (DGAMs) forecasting discrete ecological time series. Methods Ecology Evolution. 14:3, 771-784.  Nicholas J Clark, SK Morgan Ernest, Henry Senyondo, Juniper Simonis, Ethan P White, Glenda M Yenni, KANK Karunarathna (2025). Beyond single-species models: leveraging multispecies forecasts navigate dynamics ecological predictability. PeerJ. 13:e18929 https://doi.org/10.7717/peerj.18929","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a Bayesian dynamic GAM to a univariate or multivariate set of time series — mvgam","text":"","code":"# \\donttest{ # Simulate three time series that have shared seasonal dynamics, # independent AR(1) trends, and Poisson observations set.seed(0) dat <- sim_mvgam(   T = 80,   n_series = 3,   mu = 2,   trend_model = AR(p = 1),   prop_missing = 0.1,   prop_trend = 0.6 )  # Plot key summary statistics for a single series plot_mvgam_series(data = dat$data_train, series = 1) #> Warning: Removed 5 rows containing non-finite outside the scale range (`stat_bin()`).   # Plot all series together plot_mvgam_series(data = dat$data_train, series = \"all\")   # Formulate a model using Stan where series share a cyclic smooth for # seasonality and each series has an independent AR1 temporal process. # Note that 'noncentred = TRUE' will likely give performance gains. # Set run_model = FALSE to inspect the returned objects mod1 <- mvgam(   formula = y ~ s(season, bs = \"cc\", k = 6),   data = dat$data_train,   trend_model = AR(),   family = poisson(),   noncentred = TRUE,   run_model = FALSE )  # View the model code in Stan language stancode(mod1) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[4, 4] S1; // mgcv smooth penalty matrix S1 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // latent trend AR1 terms #>   vector<lower=-1, upper=1>[n_series] ar1; #>    #>   // latent trend variance parameters #>   vector<lower=0>[n_series] sigma; #>    #>   // raw latent trends #>   matrix[n, n_series] trend_raw; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>    #>   // latent trends #>   matrix[n, n_series] trend; #>   trend = trend_raw .* rep_matrix(sigma', rows(trend_raw)); #>   for (s in 1 : n_series) { #>     trend[2 : n, s] += ar1[s] * trend[1 : (n - 1), s]; #>   } #>   b[1 : num_basis] = b_raw[1 : num_basis]; #> } #> model { #>   // prior for (Intercept)... #>   b_raw[1] ~ student_t(3, 1.9, 2.5); #>    #>   // prior for s(season)... #>   b_raw[2 : 5] ~ multi_normal_prec(zero[2 : 5], S1[1 : 4, 1 : 4] * lambda[1]); #>    #>   // priors for AR parameters #>   ar1 ~ std_normal(); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>    #>   // priors for latent trend variance parameters #>   sigma ~ inv_gamma(1.418, 0.452); #>   to_vector(trend_raw) ~ std_normal(); #>   { #>     // likelihood functions #>     vector[n_nonmissing] flat_trends; #>     flat_trends = to_vector(trend)[obs_ind]; #>     flat_ys ~ poisson_log_glm(append_col(flat_xs, flat_trends), 0.0, #>                               append_row(b, 1.0)); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   vector[n_series] tau; #>   array[n, n_series] int ypred; #>   rho = log(lambda); #>   for (s in 1 : n_series) { #>     tau[s] = pow(sigma[s], -2.0); #>   } #>    #>   // posterior predictions #>   eta = X * b; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]] + trend[1 : n, s]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> } #>  #>   # View the data objects needed to fit the model in Stan sdata1 <- standata(mod1) str(sdata1) #> List of 18 #>  $ y           : num [1:60, 1:3] 4 5 7 39 51 26 6 6 4 2 ... #>  $ n           : int 60 #>  $ X           : num [1:180, 1:5] 1 1 1 1 1 1 1 1 1 1 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:5] \"X.Intercept.\" \"V2\" \"V3\" \"V4\" ... #>  $ S1          : num [1:4, 1:4] 1.244 -0.397 0.384 0.619 -0.397 ... #>  $ zero        : num [1:5] 0 0 0 0 0 #>  $ p_coefs     : Named num 0 #>   ..- attr(*, \"names\")= chr \"(Intercept)\" #>  $ p_taus      : num 0.853 #>  $ ytimes      : int [1:60, 1:3] 1 4 7 10 13 16 19 22 25 28 ... #>  $ n_series    : int 3 #>  $ sp          : Named num 0.368 #>   ..- attr(*, \"names\")= chr \"s(season)\" #>  $ y_observed  : num [1:60, 1:3] 1 1 1 1 1 1 1 1 1 1 ... #>  $ total_obs   : int 180 #>  $ num_basis   : int 5 #>  $ n_sp        : num 1 #>  $ n_nonmissing: int 164 #>  $ obs_ind     : int [1:164] 1 2 3 4 5 6 7 8 9 10 ... #>  $ flat_ys     : num [1:164] 4 5 7 39 51 26 6 6 4 2 ... #>  $ flat_xs     : num [1:164, 1:5] 1 1 1 1 1 1 1 1 1 1 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:5] \"X.Intercept.\" \"V2\" \"V3\" \"V4\" ... #>  - attr(*, \"trend_model\")= chr \"AR1\"  # Now fit the model mod1 <- mvgam(   formula = y ~ s(season, bs = \"cc\", k = 6),   data = dat$data_train,   trend_model = AR(),   family = poisson(),   noncentred = TRUE,   chains = 2,   silent = 2 )  # Extract the model summary summary(mod1) #> GAM formula: #> y ~ s(season, bs = \"cc\", k = 6) #> <environment: 0x55790ae96e20> #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> AR() #>  #>  #> N series: #> 3  #>  #> N timepoints: #> 60  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM coefficient (beta) estimates: #>               2.5%   50% 97.5% Rhat n_eff #> (Intercept)  1.900  2.00  2.10 1.00   582 #> s(season).1  0.079  0.31  0.52 1.00   457 #> s(season).2  0.590  0.82  1.10 1.01   293 #> s(season).3 -0.051  0.17  0.41 1.00   407 #> s(season).4 -0.650 -0.42 -0.22 1.01   594 #>  #> Approximate significance of GAM smooths: #>            edf Ref.df Chi.sq p-value     #> s(season) 3.31      4   42.3 8.3e-07 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Latent trend parameter AR estimates: #>           2.5%   50%   97.5% Rhat n_eff #> ar1[1]    0.28  0.74  0.9800 1.00   231 #> ar1[2]   -0.95 -0.48 -0.0039 1.01   143 #> ar1[3]    0.19  0.72  0.9800 1.00   174 #> sigma[1]  0.41  0.55  0.7600 1.01   364 #> sigma[2]  0.32  0.47  0.6600 1.01   274 #> sigma[3]  0.36  0.49  0.6800 1.00   339 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model  # Plot the estimated historical trend and forecast for one series plot(mod1, type = \"trend\", series = 1)  plot(mod1, type = \"forecast\", series = 1)   # Residual diagnostics plot(mod1, type = \"residuals\", series = 1)  resids <- residuals(mod1) str(resids) #>  num [1:180, 1:4] -0.158 NaN -0.154 0.277 -0.848 ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : NULL #>   ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"  # Fitted values and residuals can also be added to training data augment(mod1) #> # A tibble: 180 × 14 #>        y season  year series    time .observed .fitted .fit.variability #>    <int>  <int> <int> <fct>    <int>     <int>   <dbl>            <dbl> #>  1     4      1     1 series_1     1         4    4.68             1.57 #>  2    NA      1     1 series_2     1        NA    7.06             4.03 #>  3     4      1     1 series_3     1         4    4.63             1.52 #>  4     5      2     1 series_1     2         5    4.73             1.62 #>  5     2      2     1 series_2     2         2    3.84             1.38 #>  6    NA      2     1 series_3     2        NA    5.22             3.01 #>  7     7      3     1 series_1     3         7    8.50             2.51 #>  8    12      3     1 series_2     3        12   11.5              2.67 #>  9     4      3     1 series_3     3         4    5.46             1.94 #> 10    39      4     1 series_1     4        39   36.1              5.62 #> # ℹ 170 more rows #> # ℹ 6 more variables: .fit.cred.low <dbl>, .fit.cred.high <dbl>, .resid <dbl>, #> #   .resid.variability <dbl>, .resid.cred.low <dbl>, .resid.cred.high <dbl>  # Compute the forecast using covariate information in data_test fc <- forecast(mod1, newdata = dat$data_test) str(fc) #> List of 16 #>  $ call              :Class 'formula'  language y ~ s(season, bs = \"cc\", k = 6) #>   .. ..- attr(*, \".Environment\")=<environment: 0x55790ae96e20>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ family_pars       : NULL #>  $ trend_model       :List of 7 #>   ..$ trend_model: chr \"AR1\" #>   ..$ ma         : logi FALSE #>   ..$ cor        : logi FALSE #>   ..$ unit       : chr \"time\" #>   ..$ gr         : chr \"NA\" #>   ..$ subgr      : chr \"series\" #>   ..$ label      : language AR() #>   ..- attr(*, \"class\")= chr \"mvgam_trend\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : Factor w/ 3 levels \"series_1\",\"series_2\",..: 1 2 3 #>  $ train_observations:List of 3 #>   ..$ series_1: int [1:60] 4 5 7 39 51 26 6 6 4 2 ... #>   ..$ series_2: int [1:60] NA 2 12 16 6 31 9 15 5 3 ... #>   ..$ series_3: int [1:60] 4 NA 4 NA NA 16 7 7 3 NA ... #>  $ train_times       :List of 3 #>   ..$ series_1: int [1:60] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_2: int [1:60] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_3: int [1:60] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations :List of 3 #>   ..$ series_1: int [1:20] 1 NA NA 13 18 20 16 6 NA 4 ... #>   ..$ series_2: int [1:20] 4 36 8 6 7 NA NA 1 6 4 ... #>   ..$ series_3: int [1:20] 6 8 5 5 19 14 1 1 7 0 ... #>  $ test_times        :List of 3 #>   ..$ series_1: int [1:20] 61 62 63 64 65 66 67 68 69 70 ... #>   ..$ series_2: int [1:20] 61 62 63 64 65 66 67 68 69 70 ... #>   ..$ series_3: int [1:20] 61 62 63 64 65 66 67 68 69 70 ... #>  $ hindcasts         :List of 3 #>   ..$ series_1: num [1:1000, 1:60] 8 3 1 5 7 1 8 3 5 2 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:60] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>   ..$ series_2: num [1:1000, 1:60] 9 4 6 12 8 4 4 6 4 8 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:60] \"ypred[1,2]\" \"ypred[2,2]\" \"ypred[3,2]\" \"ypred[4,2]\" ... #>   ..$ series_3: num [1:1000, 1:60] 2 3 7 5 5 3 6 1 12 0 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:60] \"ypred[1,3]\" \"ypred[2,3]\" \"ypred[3,3]\" \"ypred[4,3]\" ... #>  $ forecasts         :List of 3 #>   ..$ series_1: int [1:1000, 1:20] 2 3 1 0 3 2 0 12 2 10 ... #>   ..$ series_2: int [1:1000, 1:20] 2 13 3 13 8 1 11 4 7 6 ... #>   ..$ series_3: int [1:1000, 1:20] 15 1 12 3 6 7 7 5 33 10 ... #>  - attr(*, \"class\")= chr \"mvgam_forecast\" fc_summary <- summary(fc) head(fc_summary, 12) #> # A tibble: 12 × 7 #>    series    time predQ50 predQ2.5 predQ97.5 truth type     #>    <fct>    <int>   <dbl>    <dbl>     <dbl> <int> <chr>    #>  1 series_1     1     4.5        1      11       4 response #>  2 series_1     2     4          1      11       5 response #>  3 series_1     3     8          3      17       7 response #>  4 series_1     4    36         22      53      39 response #>  5 series_1     5    50         31      71.0    51 response #>  6 series_1     6    24         13      39      26 response #>  7 series_1     7     7          2      16       6 response #>  8 series_1     8     5          1      13       6 response #>  9 series_1     9     4          1      10       4 response #> 10 series_1    10     3          0       8       2 response #> 11 series_1    11     4          0      11       5 response #> 12 series_1    12     5          1      12       5 response plot(fc) #> Out of sample DRPS: #> 57.427275 #> Warning: Removed 8 rows containing missing values or values outside the scale range #> (`geom_point()`).   # Plot the estimated seasonal smooth function plot(mod1, type = \"smooths\")   # Plot estimated first derivatives of the smooth plot(mod1, type = \"smooths\", derivatives = TRUE)   # Plot partial residuals of the smooth plot(mod1, type = \"smooths\", residuals = TRUE)   # Plot posterior realisations for the smooth plot(mod1, type = \"smooths\", realisations = TRUE)   # Plot conditional response predictions using marginaleffects conditional_effects(mod1)  plot_predictions(mod1, condition = \"season\", points = 0.5) #> Warning: Removed 16 rows containing missing values or values outside the scale range #> (`geom_point()`).   # Generate posterior predictive checks using bayesplot pp_check(mod1) #> Using 10 posterior draws for ppc type 'dens_overlay' by default. #> Warning: NA responses are not shown in 'pp_check'.   # Extract observation model beta coefficient draws as a data.frame beta_draws_df <- as.data.frame(mod1, variable = \"betas\") head(beta_draws_df) #>   (Intercept) s(season).1 s(season).2 s(season).3 s(season).4 #> 1     2.02377    0.339960    0.807651   0.2134910   -0.384225 #> 2     1.90911    0.426211    0.999631   0.1254090   -0.570203 #> 3     1.99496    0.464323    0.766504   0.3942860   -0.506176 #> 4     1.93253    0.429288    0.705769   0.1273360   -0.307990 #> 5     1.97859    0.154809    0.964565   0.0666232   -0.493254 #> 6     1.95921    0.385504    0.686465   0.1625780   -0.223091 str(beta_draws_df) #> 'data.frame':\t1000 obs. of  5 variables: #>  $ (Intercept): num  2.02 1.91 1.99 1.93 1.98 ... #>  $ s(season).1: num  0.34 0.426 0.464 0.429 0.155 ... #>  $ s(season).2: num  0.808 1 0.767 0.706 0.965 ... #>  $ s(season).3: num  0.2135 0.1254 0.3943 0.1273 0.0666 ... #>  $ s(season).4: num  -0.384 -0.57 -0.506 -0.308 -0.493 ...  # Investigate model fit mc.cores.def <- getOption(\"mc.cores\") options(mc.cores = 1) loo(mod1) #> Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. #>  #> Computed from 1000 by 164 log-likelihood matrix. #>  #>          Estimate    SE #> elpd_loo   -812.9  97.8 #> p_loo       128.3  34.8 #> looic      1625.8 195.7 #> ------ #> MCSE of elpd_loo is NA. #> MCSE and ESS estimates assume MCMC draws (r_eff in [0.3, 1.0]). #>  #> Pareto k diagnostic values: #>                           Count Pct.    Min. ESS #> (-Inf, 0.67]   (good)     150   91.5%   46       #>    (0.67, 1]   (bad)       10    6.1%   <NA>     #>     (1, Inf)   (very bad)   4    2.4%   <NA>     #> See help('pareto-k-diagnostic') for details. options(mc.cores = mc.cores.def)    # Fit a model to the portal time series that uses a latent # Vector Autoregression of order 1 mod <- mvgam(   formula = captures ~ -1,   trend_formula = ~ trend,   trend_model = VAR(cor = TRUE),   family = poisson(),   data = portal_data,   chains = 2,   silent = 2 )  # Plot the autoregressive coefficient distributions; # use 'dir = \"v\"' to arrange the order of facets # correctly mcmc_plot(   mod,   variable = 'A',   regex = TRUE,   type = 'hist',   facet_args = list(dir = 'v') ) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # Plot the process error variance-covariance matrix in the same way; mcmc_plot(   mod,   variable = 'Sigma',   regex = TRUE,   type = 'hist',   facet_args = list(dir = 'v') ) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # Calulate Generalized IRFs for each series irfs <- irf(   mod,   h = 12,   cumulative = FALSE )  # Plot some of them plot(irfs, series = 1)  plot(irfs, series = 2)   # Calulate forecast error variance decompositions for each series fevds <- fevd(mod, h = 12)  # Plot median contributions to forecast error variance plot(fevds)    # Now fit a model that uses two RW dynamic factors to model # the temporal dynamics of the four rodent species mod <- mvgam(   captures ~ series,   trend_model = RW(),   use_lv = TRUE,   n_lv = 2,   data = portal_data,   chains = 2,   silent = 2 ) #> Warning in '/tmp/RtmppUq4oL/model_fe1a9373f4bc428a5906ce7b23b30b3c.stan', line 20, column 31: Found #>     int division: #>       n_lv * (n_lv - 1) / 2 #>     Values will be rounded towards zero. If rounding is not desired you can #>     write #>     the division as #>       n_lv * (n_lv - 1) / 2.0 #>     If rounding is intended please use the integer division operator %/%. #> Warning in '/tmp/RtmppUq4oL/model-22f51e4ea749.stan', line 20, column 33: Found #>     int division: #>       n_lv * (n_lv - 1) / 2 #>     Values will be rounded towards zero. If rounding is not desired you can #>     write #>     the division as #>       n_lv * (n_lv - 1) / 2.0 #>     If rounding is intended please use the integer division operator %/%.  # Plot the factors plot(mod, type = 'factors')  #> # A tibble: 2 × 2 #>   Factor   Contribution #>   <chr>           <dbl> #> 1 Factor 1        0.532 #> 2 Factor 2        0.468  # Plot the hindcasts hcs <- hindcast(mod) plot(hcs,      series = 1) #> No non-missing values in test_observations; cannot calculate forecast score #> Warning: Removed 17 rows containing missing values or values outside the scale range #> (`geom_point()`).  plot(hcs,      series = 2) #> No non-missing values in test_observations; cannot calculate forecast score #> Warning: Removed 17 rows containing missing values or values outside the scale range #> (`geom_point()`).  plot(hcs,      series = 3) #> No non-missing values in test_observations; cannot calculate forecast score #> Warning: Removed 17 rows containing missing values or values outside the scale range #> (`geom_point()`).  plot(hcs,      series = 4) #> No non-missing values in test_observations; cannot calculate forecast score #> Warning: Removed 17 rows containing missing values or values outside the scale range #> (`geom_point()`).   # Use residual_cor() to calculate temporal correlations among the series # based on the factor loadings lvcors <- residual_cor(mod) names(lvcors) #>  [1] \"cor\"        \"cor_lower\"  \"cor_upper\"  \"sig_cor\"    \"cov\"        #>  [6] \"prec\"       \"prec_lower\" \"prec_upper\" \"sig_prec\"   \"trace\"      lvcors$cor #>            DM        DO         PB         PP #> DM  1.0000000 0.5156657 -0.5619562 -0.8214555 #> DO  0.5156657 1.0000000  0.3661782  0.0105830 #> PB -0.5619562 0.3661782  1.0000000  0.9171802 #> PP -0.8214555 0.0105830  0.9171802  1.0000000  # For those correlations whose credible intervals did not include # zero, plot them as a correlation matrix (all other correlations # are shown as zero on this plot) plot(lvcors, cluster = TRUE)     # Example of supplying a trend_map so that some series can share # latent trend processes sim <- sim_mvgam(n_series = 3) mod_data <- sim$data_train  # Here, we specify only two latent trends; series 1 and 2 share a trend, # while series 3 has it's own unique latent trend trend_map <- data.frame(   series = unique(mod_data$series),   trend = c(1, 1, 2) )  # Fit the model using AR1 trends mod <- mvgam(   formula = y ~ s(season, bs = \"cc\", k = 6),   trend_map = trend_map,   trend_model = AR(),   data = mod_data,   return_model_data = TRUE,   chains = 2,   silent = 2 )  # The mapping matrix is now supplied as data to the model in the 'Z' element mod$model_data$Z #>      [,1] [,2] #> [1,]    1    0 #> [2,]    1    0 #> [3,]    0    1  # The first two series share an identical latent trend; the third is different plot(residual_cor(mod))  plot(mod, type = \"trend\", series = 1)  plot(mod, type = \"trend\", series = 2)  plot(mod, type = \"trend\", series = 3)    # Example of how to use dynamic coefficients # Simulate a time-varying coefficient for the effect of temperature set.seed(123) N <- 200 beta_temp <- vector(length = N) beta_temp[1] <- 0.4 for (i in 2:N) {   beta_temp[i] <- rnorm(1, mean = beta_temp[i - 1] - 0.0025, sd = 0.05) } plot(beta_temp)   # Simulate a covariate called 'temp' temp <- rnorm(N, sd = 1)  # Simulate some noisy Gaussian observations out <- rnorm(N,   mean = 4 + beta_temp * temp,   sd = 0.5 )  # Gather necessary data into a data.frame; split into training / testing data <- data.frame(out, temp, time = seq_along(temp)) data_train <- data[1:180, ] data_test <- data[181:200, ]  # Fit the model using the dynamic() function mod <- mvgam(   formula =     out ~ dynamic(       temp,       scale = FALSE,       k = 40     ),   family = gaussian(),   data = data_train,   newdata = data_test,   chains = 2,   silent = 2 )  # Inspect the model summary, forecast and time-varying coefficient distribution summary(mod) #> GAM formula: #> out ~ gp(time, by = temp, c = 5/4, k = 40, scale = FALSE) #> <environment: 0x55790ae96e20> #>  #> Family: #> gaussian #>  #> Link function: #> identity #>  #> Trend model: #> None #>  #> N series: #> 1  #>  #> N timepoints: #> 200  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> Observation error parameter estimates: #>              2.5%  50% 97.5% Rhat n_eff #> sigma_obs[1] 0.44 0.49  0.54    1   895 #>  #> GAM coefficient (beta) estimates: #>                    2.5%       50% 97.5% Rhat n_eff #> (Intercept)       4.000   4.0e+00 4.100 1.00  1037 #> gp(time):temp.1   0.920   3.3e+00 6.200 1.00   803 #> gp(time):temp.2  -3.600   1.4e+00 5.400 1.00   464 #> gp(time):temp.3  -5.500  -1.5e+00 3.600 1.00   451 #> gp(time):temp.4  -5.900  -1.2e+00 2.300 1.00   537 #> gp(time):temp.5  -3.000   4.9e-01 3.700 1.01   262 #> gp(time):temp.6  -2.400   2.4e-01 3.600 1.00   866 #> gp(time):temp.7  -3.600  -4.0e-01 1.900 1.01   421 #> gp(time):temp.8  -1.700   2.7e-01 2.700 1.00   937 #> gp(time):temp.9  -1.300   3.7e-01 2.400 1.00   841 #> gp(time):temp.10 -2.400  -3.8e-01 1.000 1.00   720 #> gp(time):temp.11 -2.000  -3.7e-02 1.400 1.00   943 #> gp(time):temp.12 -0.740   1.7e-01 2.100 1.00  1059 #> gp(time):temp.13 -1.400  -9.4e-13 1.300 1.00  1147 #> gp(time):temp.14 -1.600  -6.4e-03 0.780 1.00   970 #> gp(time):temp.15 -0.980  -3.1e-18 1.100 1.00  1218 #> gp(time):temp.16 -0.640   3.0e-06 1.100 1.00  1390 #> gp(time):temp.17 -0.660   7.0e-11 0.960 1.00  1029 #> gp(time):temp.18 -0.670  -3.2e-26 0.710 1.00  1423 #> gp(time):temp.19 -0.830  -1.9e-05 0.310 1.00   890 #> gp(time):temp.20 -0.630   3.3e-26 0.500 1.00  1502 #> gp(time):temp.21 -0.330   7.3e-09 0.860 1.01   526 #> gp(time):temp.22 -0.640  -2.8e-19 0.310 1.00   909 #> gp(time):temp.23 -0.580  -4.8e-13 0.290 1.00  1044 #> gp(time):temp.24 -0.180   2.0e-10 0.550 1.00   794 #> gp(time):temp.25 -0.300  -4.9e-32 0.290 1.00  1154 #> gp(time):temp.26 -0.390  -9.6e-15 0.140 1.00   903 #> gp(time):temp.27 -0.190   1.6e-55 0.340 1.00  1268 #> gp(time):temp.28 -0.100   2.1e-33 0.380 1.00   976 #> gp(time):temp.29 -0.250  -3.6e-40 0.150 1.00   934 #> gp(time):temp.30 -0.200  -4.0e-28 0.170 1.00  1329 #> gp(time):temp.31 -0.140   1.4e-75 0.160 1.00   854 #> gp(time):temp.32 -0.130 -2.2e-100 0.130 1.00   855 #> gp(time):temp.33 -0.130  -4.7e-30 0.120 1.01   963 #> gp(time):temp.34 -0.066   5.4e-86 0.093 1.00   977 #> gp(time):temp.35 -0.074   1.1e-42 0.120 1.00  1098 #> gp(time):temp.36 -0.076  -8.6e-55 0.063 1.00  1034 #> gp(time):temp.37 -0.085  -1.6e-64 0.045 1.00   854 #> gp(time):temp.38 -0.032  4.5e-107 0.039 1.01   862 #> gp(time):temp.39 -0.027  2.0e-140 0.056 1.00   803 #> gp(time):temp.40 -0.051   5.2e-55 0.032 1.00   962 #>  #> GAM gp term marginal deviation (alpha) and length scale (rho) estimates: #>                      2.5%   50% 97.5% Rhat n_eff #> alpha_gp(time):temp  0.17  0.32  0.89    1   235 #> rho_gp(time):temp   11.00 30.00 90.00    1   197 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model plot(mod, type = \"smooths\")  fc <- forecast(mod, newdata = data_test) plot(fc) #> Out of sample CRPS: #> 6.48777085114583   # Propagating the smooth term shows how the coefficient is expected to evolve plot_mvgam_smooth(mod, smooth = 1, newdata = data) abline(v = 180, lty = \"dashed\", lwd = 2) points(beta_temp, pch = 16)    # Example showing how to incorporate an offset; simulate some count data # with different means per series set.seed(100) dat <- sim_mvgam(   prop_trend = 0, mu = c(0, 2, 2),   seasonality = \"hierarchical\" )  # Add offset terms to the training and testing data dat$data_train$offset <- 0.5 * as.numeric(dat$data_train$series) dat$data_test$offset <- 0.5 * as.numeric(dat$data_test$series)  # Fit a model that includes the offset in the linear predictor as well as # hierarchical seasonal smooths mod <- mvgam(   formula = y ~ offset(offset) +     s(series, bs = \"re\") +     s(season, bs = \"cc\") +     s(season, by = series, m = 1, k = 5),   data = dat$data_train,   chains = 2,   silent = 2 )  # Inspect the model file to see the modification to the linear predictor # (eta) stancode(mod) #> // Stan model code generated by package mvgam #> data { #>   int<lower=0> total_obs; // total number of observations #>   int<lower=0> n; // number of timepoints per series #>   int<lower=0> n_sp; // number of smoothing parameters #>   int<lower=0> n_series; // number of series #>   int<lower=0> num_basis; // total number of basis coefficients #>   vector[num_basis] zero; // prior locations for basis coefficients #>   vector[total_obs] off_set; // offset vector #>   matrix[total_obs, num_basis] X; // mgcv GAM design matrix #>   array[n, n_series] int<lower=0> ytimes; // time-ordered matrix (which col in X belongs to each [time, series] observation?) #>   matrix[8, 8] S1; // mgcv smooth penalty matrix S1 #>   matrix[4, 4] S2; // mgcv smooth penalty matrix S2 #>   matrix[4, 4] S3; // mgcv smooth penalty matrix S3 #>   matrix[4, 4] S4; // mgcv smooth penalty matrix S4 #>   int<lower=0> n_nonmissing; // number of nonmissing observations #>   array[n_nonmissing] int<lower=0> flat_ys; // flattened nonmissing observations #>   matrix[n_nonmissing, num_basis] flat_xs; // X values for nonmissing observations #>   array[n_nonmissing] int<lower=0> obs_ind; // indices of nonmissing observations #> } #> parameters { #>   // raw basis coefficients #>   vector[num_basis] b_raw; #>    #>   // random effect variances #>   vector<lower=0>[1] sigma_raw; #>    #>   // random effect means #>   vector[1] mu_raw; #>    #>   // smoothing parameters #>   vector<lower=0>[n_sp] lambda; #> } #> transformed parameters { #>   // basis coefficients #>   vector[num_basis] b; #>   b[1 : 21] = b_raw[1 : 21]; #>   b[22 : 24] = mu_raw[1] + b_raw[22 : 24] * sigma_raw[1]; #> } #> model { #>   // prior for random effect population variances #>   sigma_raw ~ inv_gamma(1.418, 0.452); #>    #>   // prior for random effect population means #>   mu_raw ~ std_normal(); #>    #>   // prior for (Intercept)... #>   b_raw[1] ~ student_t(3, 1.6, 2.5); #>    #>   // prior for s(season)... #>   b_raw[2 : 9] ~ multi_normal_prec(zero[2 : 9], S1[1 : 8, 1 : 8] * lambda[1]); #>    #>   // prior for s(season):seriesseries_1... #>   b_raw[10 : 13] ~ multi_normal_prec(zero[10 : 13], #>                                      S2[1 : 4, 1 : 4] * lambda[2]); #>    #>   // prior for s(season):seriesseries_2... #>   b_raw[14 : 17] ~ multi_normal_prec(zero[14 : 17], #>                                      S3[1 : 4, 1 : 4] * lambda[3]); #>    #>   // prior for s(season):seriesseries_3... #>   b_raw[18 : 21] ~ multi_normal_prec(zero[18 : 21], #>                                      S4[1 : 4, 1 : 4] * lambda[4]); #>    #>   // prior (non-centred) for s(series)... #>   b_raw[22 : 24] ~ std_normal(); #>    #>   // priors for smoothing parameters #>   lambda ~ normal(5, 30); #>   { #>     // likelihood functions #>     flat_ys ~ poisson_log_glm(flat_xs, off_set[obs_ind], b); #>   } #> } #> generated quantities { #>   vector[total_obs] eta; #>   matrix[n, n_series] mus; #>   vector[n_sp] rho; #>   array[n, n_series] int ypred; #>   rho = log(lambda); #>    #>   // posterior predictions #>   eta = X * b + off_set; #>   for (s in 1 : n_series) { #>     mus[1 : n, s] = eta[ytimes[1 : n, s]]; #>     ypred[1 : n, s] = poisson_log_rng(mus[1 : n, s]); #>   } #> } #>  #>   # Forecasts for the first two series will differ in magnitude fc <- forecast(mod, newdata = dat$data_test) plot(fc, series = 1, ylim = c(0, 75)) #> Out of sample DRPS: #> 26.650371  plot(fc, series = 2, ylim = c(0, 75)) #> Out of sample DRPS: #> 101.645578   # Changing the offset for the testing data should lead to changes in # the forecast dat$data_test$offset <- dat$data_test$offset - 2 fc <- forecast(mod, newdata = dat$data_test) plot(fc) #> Out of sample DRPS: #> 41.425863   # Relative Risks can be computed by fixing the offset to the same value # for each series dat$data_test$offset <- rep(1, NROW(dat$data_test)) preds_rr <- predict(mod,   type = \"link\", newdata = dat$data_test,   summary = FALSE ) series1_inds <- which(dat$data_test$series == \"series_1\") series2_inds <- which(dat$data_test$series == \"series_2\")  # Relative Risks are now more comparable among series layout(matrix(1:2, ncol = 2)) plot(preds_rr[1, series1_inds],   type = \"l\", col = \"grey75\",   ylim = range(preds_rr),   ylab = \"Series1 Relative Risk\", xlab = \"Time\" ) for (i in 2:50) {   lines(preds_rr[i, series1_inds], col = \"grey75\") }  plot(preds_rr[1, series2_inds],   type = \"l\", col = \"darkred\",   ylim = range(preds_rr),   ylab = \"Series2 Relative Risk\", xlab = \"Time\" ) for (i in 2:50) {   lines(preds_rr[i, series2_inds], col = \"darkred\") }  layout(1)   # Example showcasing how cbind() is needed for Binomial observations # Simulate two time series of Binomial trials trials <- sample(c(20:25), 50, replace = TRUE) x <- rnorm(50) detprob1 <- plogis(-0.5 + 0.9 * x) detprob2 <- plogis(-0.1 - 0.7 * x) dat <- rbind(   data.frame(     y = rbinom(n = 50, size = trials, prob = detprob1),     time = 1:50,     series = \"series1\",     x = x,     ntrials = trials   ),   data.frame(     y = rbinom(n = 50, size = trials, prob = detprob2),     time = 1:50,     series = \"series2\",     x = x,     ntrials = trials   ) ) dat <- dplyr::mutate(dat, series = as.factor(series)) dat <- dplyr::arrange(dat, time, series) plot_mvgam_series(data = dat, series = \"all\")   # Fit a model using the binomial() family; must specify observations # and number of trials in the cbind() wrapper mod <- mvgam(   formula =     cbind(y, ntrials) ~ series + s(x, by = series),   family = binomial(),   data = dat,   chains = 2,   silent = 2 ) #> Warning: Binomial and Beta-binomial families require cbind(n_successes, n_trials) #> in the formula left-hand side. Do not use cbind(n_successes, n_failures)! #> This warning is displayed once per session. summary(mod) #> GAM formula: #> cbind(y, ntrials) ~ series + s(x, by = series) #> <environment: 0x55790ae96e20> #>  #> Family: #> binomial #>  #> Link function: #> logit #>  #> Trend model: #> None #>  #> N series: #> 2  #>  #> N timepoints: #> 50  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM coefficient (beta) estimates: #>                        2.5%     50%  97.5% Rhat n_eff #> (Intercept)          -0.710 -0.5800 -0.440 1.00   610 #> seriesseries2         0.430  0.6000  0.790 1.00   601 #> s(x):seriesseries1.1 -0.680 -0.0780  0.110 1.03    55 #> s(x):seriesseries1.2 -0.520 -0.0130  0.450 1.01   245 #> s(x):seriesseries1.3 -0.260 -0.0180  0.037 1.02    80 #> s(x):seriesseries1.4 -0.260 -0.0094  0.230 1.01   256 #> s(x):seriesseries1.5 -0.040  0.0067  0.120 1.01   134 #> s(x):seriesseries1.6 -0.290 -0.0160  0.180 1.01   158 #> s(x):seriesseries1.7 -0.160 -0.0085  0.099 1.01   219 #> s(x):seriesseries1.8 -0.770  0.0510  0.990 1.01   238 #> s(x):seriesseries1.9 -0.033  0.6700  0.940 1.03    52 #> s(x):seriesseries2.1 -0.100  0.0470  0.640 1.01    61 #> s(x):seriesseries2.2 -0.160  0.0460  1.300 1.02    54 #> s(x):seriesseries2.3 -0.042  0.0110  0.250 1.02    49 #> s(x):seriesseries2.4 -0.100  0.0230  0.520 1.02    74 #> s(x):seriesseries2.5 -0.180 -0.0076  0.030 1.02    64 #> s(x):seriesseries2.6 -0.089  0.0250  0.570 1.02    61 #> s(x):seriesseries2.7 -0.046  0.0150  0.310 1.02    61 #> s(x):seriesseries2.8 -2.200 -0.1000  0.320 1.02    61 #> s(x):seriesseries2.9 -0.920 -0.6900 -0.110 1.01    70 #>  #> Approximate significance of GAM smooths: #>                     edf Ref.df Chi.sq p-value     #> s(x):seriesseries1 4.20      9   59.3 0.00038 *** #> s(x):seriesseries2 2.62      9   27.4 < 2e-16 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model pp_check(mod,   type = \"bars_grouped\",   group = \"series\", ndraws = 50 )  pp_check(mod,   type = \"ecdf_overlay_grouped\",   group = \"series\", ndraws = 50 )  conditional_effects(mod, type = \"link\")    # To view predictions on the probability scale, # use ntrials = 1 in datagrid() plot_predictions(   mod,   by = c('x', 'series'),   newdata = datagrid(x = runif(100, -2, 2),                     series = unique,                     ntrials = 1),   type = 'expected' )   # \\dontshow{ # For R CMD check: make sure any open connections are closed afterward  closeAllConnections() # } # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract diagnostic quantities of mvgam models — mvgam_diagnostics","title":"Extract diagnostic quantities of mvgam models — mvgam_diagnostics","text":"Extract quantities can used diagnose sampling behavior algorithms applied Stan back-end mvgam.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract diagnostic quantities of mvgam models — mvgam_diagnostics","text":"","code":"# S3 method for mvgam nuts_params(object, pars = NULL, ...)  # S3 method for mvgam log_posterior(object, ...)  # S3 method for mvgam rhat(x, pars = NULL, ...)  # S3 method for mvgam neff_ratio(object, pars = NULL, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract diagnostic quantities of mvgam models — mvgam_diagnostics","text":"object, x mvgam jsdgam object. pars optional character vector parameter names. nuts_params NUTS sampler parameter names rather model parameters. pars omitted parameters included. ... Arguments passed individual methods.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract diagnostic quantities of mvgam models — mvgam_diagnostics","text":"exact form output depends method.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract diagnostic quantities of mvgam models — mvgam_diagnostics","text":"details see bayesplot-extractors.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract diagnostic quantities of mvgam models — mvgam_diagnostics","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 1, trend_model = 'AR1') mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),             trend_model = AR(),             noncentred = TRUE,             data = simdat$data_train,             chains = 2) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 0.3 seconds. #> Chain 2 finished in 0.3 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 0.3 seconds. #> Total execution time: 0.4 seconds. #>  np <- nuts_params(mod) head(np) #>   Chain Iteration     Parameter    Value #> 1     1         1 accept_stat__ 0.743796 #> 2     1         2 accept_stat__ 0.994801 #> 3     1         3 accept_stat__ 0.839215 #> 4     1         4 accept_stat__ 0.900191 #> 5     1         5 accept_stat__ 1.000000 #> 6     1         6 accept_stat__ 0.902151  # extract the number of divergence transitions sum(subset(np, Parameter == \"divergent__\")$Value) #> [1] 0  head(neff_ratio(mod)) #>  mus[1,1]  mus[2,1]  mus[3,1]  mus[4,1]  mus[5,1]  mus[6,1]  #> 0.7265682 0.8289112 0.8111213 0.8450109 1.0742448 0.8096518  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_draws.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract posterior draws from fitted mvgam objects — mvgam_draws","title":"Extract posterior draws from fitted mvgam objects — mvgam_draws","text":"Extract posterior draws conventional formats data.frames, matrices, arrays.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_draws.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract posterior draws from fitted mvgam objects — mvgam_draws","text":"","code":"# S3 method for mvgam as.data.frame(   x,   row.names = NULL,   optional = TRUE,   variable = \"betas\",   use_alias = TRUE,   regex = FALSE,   ... )  # S3 method for mvgam as.matrix(x, variable = \"betas\", regex = FALSE, use_alias = TRUE, ...)  # S3 method for mvgam as.array(x, variable = \"betas\", regex = FALSE, use_alias = TRUE, ...)  # S3 method for mvgam as_draws(   x,   variable = NULL,   regex = FALSE,   inc_warmup = FALSE,   use_alias = TRUE,   ... )  # S3 method for mvgam as_draws_matrix(   x,   variable = NULL,   regex = FALSE,   inc_warmup = FALSE,   use_alias = TRUE,   ... )  # S3 method for mvgam as_draws_df(   x,   variable = NULL,   regex = FALSE,   inc_warmup = FALSE,   use_alias = TRUE,   ... )  # S3 method for mvgam as_draws_array(   x,   variable = NULL,   regex = FALSE,   inc_warmup = FALSE,   use_alias = TRUE,   ... )  # S3 method for mvgam as_draws_list(   x,   variable = NULL,   regex = FALSE,   inc_warmup = FALSE,   use_alias = TRUE,   ... )  # S3 method for mvgam as_draws_rvars(x, variable = NULL, regex = FALSE, inc_warmup = FALSE, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_draws.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract posterior draws from fitted mvgam objects — mvgam_draws","text":"x list object class mvgam row.names Ignored optional Ignored variable character specifying parameters extract. Can either one following options: obs_params (parameters specific observation model, overdispsersions negative binomial models observation error SD gaussian / student-t models) betas (beta coefficients GAM observation model linear predictor; default) smooth_params (smoothing parameters GAM observation model) linpreds (estimated linear predictors whatever link scale used model) trend_params (parameters governing trend dynamics, AR parameters, trend SD parameters Gaussian Process parameters) trend_betas (beta coefficients GAM latent process model linear predictor; available trend_formula supplied original model) trend_smooth_params (process model GAM smoothing parameters; available trend_formula supplied original model) trend_linpreds (process model linear predictors identity scale; available trend_formula supplied original model) can character vector providing variables extract use_alias Logical. informative names parameters available (.e. beta coefficients b smoothing parameters rho), replace uninformative names informative alias. Defaults TRUE regex Logical. using one prespecified options extractions, variable treated (vector ) regular expressions? variable x matching least one regular expressions selected. Defaults FALSE. ... Ignored inc_warmup warmup draws included? Defaults FALSE.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_draws.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract posterior draws from fitted mvgam objects — mvgam_draws","text":"data.frame, matrix, array containing posterior draws.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_draws.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract posterior draws from fitted mvgam objects — mvgam_draws","text":"","code":"# \\donttest{ sim <- sim_mvgam(family = Gamma()) mod1 <- mvgam(y ~ s(season, bs = 'cc'),              trend_model = 'AR1',              data = sim$data_train,              family = Gamma(),              chains = 2,              silent = 2) #> Warning: Supplying trend_model as a character string is deprecated #> Please use the dedicated functions (i.e. RW() or ZMVN()) instead #> This warning is displayed once per session. beta_draws_df <- as.data.frame(mod1, variable = 'betas') head(beta_draws_df) #>   (Intercept) s(season).1 s(season).2 s(season).3 s(season).4 s(season).5 #> 1    0.374777   -0.733198    -1.36907   -0.519307   -0.211382   -0.676882 #> 2    0.427450   -0.425934    -1.32330   -0.617301   -0.201891   -1.480630 #> 3    0.430866   -0.340452    -1.36755   -0.480587   -0.819081   -0.804817 #> 4    0.430781   -0.389531    -1.33819   -0.504064   -0.784152   -0.814420 #> 5    0.424774   -0.397475    -1.40472   -0.586206   -0.706911   -0.805801 #> 6    0.420481   -0.655663    -1.34370   -0.783575   -0.324282   -1.158140 #>   s(season).6 s(season).7 s(season).8 #> 1   -0.568080    0.780525    0.362101 #> 2   -0.777516    0.394582    0.780230 #> 3   -0.844599    0.853809    0.354951 #> 4   -0.912800    0.870433    0.341703 #> 5   -0.993340    0.882721    0.342518 #> 6   -0.673337    1.062960    1.102170 str(beta_draws_df) #> 'data.frame':\t1000 obs. of  9 variables: #>  $ (Intercept): num  0.375 0.427 0.431 0.431 0.425 ... #>  $ s(season).1: num  -0.733 -0.426 -0.34 -0.39 -0.397 ... #>  $ s(season).2: num  -1.37 -1.32 -1.37 -1.34 -1.4 ... #>  $ s(season).3: num  -0.519 -0.617 -0.481 -0.504 -0.586 ... #>  $ s(season).4: num  -0.211 -0.202 -0.819 -0.784 -0.707 ... #>  $ s(season).5: num  -0.677 -1.481 -0.805 -0.814 -0.806 ... #>  $ s(season).6: num  -0.568 -0.778 -0.845 -0.913 -0.993 ... #>  $ s(season).7: num  0.781 0.395 0.854 0.87 0.883 ... #>  $ s(season).8: num  0.362 0.78 0.355 0.342 0.343 ...  beta_draws_mat <- as.matrix(mod1, variable = 'betas') head(beta_draws_mat) #>     variable #> draw (Intercept) s(season).1 s(season).2 s(season).3 s(season).4 s(season).5 #>    1    0.374777   -0.733198    -1.36907   -0.519307   -0.211382   -0.676882 #>    2    0.427450   -0.425934    -1.32330   -0.617301   -0.201891   -1.480630 #>    3    0.430866   -0.340452    -1.36755   -0.480587   -0.819081   -0.804817 #>    4    0.430781   -0.389531    -1.33819   -0.504064   -0.784152   -0.814420 #>    5    0.424774   -0.397475    -1.40472   -0.586206   -0.706911   -0.805801 #>    6    0.420481   -0.655663    -1.34370   -0.783575   -0.324282   -1.158140 #>     variable #> draw s(season).6 s(season).7 s(season).8 #>    1   -0.568080    0.780525    0.362101 #>    2   -0.777516    0.394582    0.780230 #>    3   -0.844599    0.853809    0.354951 #>    4   -0.912800    0.870433    0.341703 #>    5   -0.993340    0.882721    0.342518 #>    6   -0.673337    1.062960    1.102170 str(beta_draws_mat) #>  num [1:1000, 1:9] 0.375 0.427 0.431 0.431 0.425 ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ draw    : chr [1:1000] \"1\" \"2\" \"3\" \"4\" ... #>   ..$ variable: chr [1:9] \"(Intercept)\" \"s(season).1\" \"s(season).2\" \"s(season).3\" ... #>  - attr(*, \"nchains\")= int 2  shape_pars <- as.matrix(mod1, variable = 'shape', regex = TRUE) head(shape_pars)# } #>     variable #> draw shape[1] shape[2] shape[3] #>    1 1.163980 1.209840 0.851010 #>    2 0.980268 0.826011 0.840381 #>    3 1.003790 1.296510 1.437170 #>    4 1.040720 1.318460 1.451680 #>    5 1.046620 1.356160 1.391470 #>    6 1.068660 0.874556 0.737471"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_families.html","id":null,"dir":"Reference","previous_headings":"","what":"Supported mvgam families — mvgam_families","title":"Supported mvgam families — mvgam_families","text":"Supported mvgam families","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_families.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supported mvgam families — mvgam_families","text":"","code":"tweedie(link = \"log\")  student_t(link = \"identity\")  betar(...)  nb(...)  lognormal(...)  student(...)  bernoulli(...)  beta_binomial(...)  nmix(link = \"log\")"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_families.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supported mvgam families — mvgam_families","text":"link specification family link function. present changed ... Arguments passed mgcv version associated functions","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_families.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supported mvgam families — mvgam_families","text":"Objects class family","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_families.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Supported mvgam families — mvgam_families","text":"mvgam currently supports following standard observation families: gaussian identity link, real-valued data poisson log-link, count data Gamma log-link, non-negative real-valued data binomial logit-link, count data number trials known (must supplied) addition, following extended families mgcv brms packages supported: betar logit-link, proportional data (0,1) nb log-link, count data lognormal identity-link, non-negative real-valued data bernoulli logit-link, binary data beta_binomial logit-link, binomial() allows overdispersion Finally, mvgam supports three extended families described : tweedie log-link, count data (power parameter p fixed 1.5) student_t() (student) identity-link, real-valued data nmix count data imperfect detection modeled via State-Space N-Mixture model. latent states Poisson (log link), capturing 'true' latent abundance, observation process Binomial account imperfect detection. observation formula models used set linear predictor detection probability (logit link). See example detailed worked explanation nmix() family poisson(), nb(), tweedie() available using JAGS. families, apart tweedie(), supported using Stan. Note currently possible change default link functions mvgam, call change silently ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_families.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Supported mvgam families — mvgam_families","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_families.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supported mvgam families — mvgam_families","text":"","code":"# \\donttest{ # Example showing how to set up N-mixture models set.seed(999) # Simulate observations for species 1, which shows a declining trend and 0.7 detection probability data.frame(site = 1,           # five replicates per year; six years           replicate = rep(1:5, 6),           time = sort(rep(1:6, 5)),           species = 'sp_1',           # true abundance declines nonlinearly           truth = c(rep(28, 5),                     rep(26, 5),                     rep(23, 5),                     rep(16, 5),                     rep(14, 5),                     rep(14, 5)),           # observations are taken with detection prob = 0.7           obs = c(rbinom(5, 28, 0.7),                   rbinom(5, 26, 0.7),                   rbinom(5, 23, 0.7),                   rbinom(5, 15, 0.7),                   rbinom(5, 14, 0.7),                   rbinom(5, 14, 0.7))) %>%  # add 'series' information, which is an identifier of site, replicate and species  dplyr::mutate(series = paste0('site_', site,                                '_', species,                                '_rep_', replicate),                time = as.numeric(time),                # add a 'cap' variable that defines the maximum latent N to                # marginalize over when estimating latent abundance; in other words                # how large do we realistically think the true abundance could be?                cap = 80) %>%  dplyr::select(- replicate) -> testdat  # Now add another species that has a different temporal trend and a smaller # detection probability (0.45 for this species) testdat = testdat %>%  dplyr::bind_rows(data.frame(site = 1,                              replicate = rep(1:5, 6),                              time = sort(rep(1:6, 5)),                              species = 'sp_2',                              truth = c(rep(4, 5),                                        rep(7, 5),                                        rep(15, 5),                                        rep(16, 5),                                        rep(19, 5),                                        rep(18, 5)),                              obs = c(rbinom(5, 4, 0.45),                                      rbinom(5, 7, 0.45),                                      rbinom(5, 15, 0.45),                                      rbinom(5, 16, 0.45),                                      rbinom(5, 19, 0.45),                                      rbinom(5, 18, 0.45))) %>%                     dplyr::mutate(series = paste0('site_', site,                                                   '_', species,                                                   '_rep_', replicate),                                   time = as.numeric(time),                                   cap = 50) %>%                     dplyr::select(-replicate))  # series identifiers testdat$species <- factor(testdat$species,                           levels = unique(testdat$species)) testdat$series <- factor(testdat$series,                          levels = unique(testdat$series))  # The trend_map to state how replicates are structured testdat %>% # each unique combination of site*species is a separate process dplyr::mutate(trend = as.numeric(factor(paste0(site, species)))) %>%  dplyr::select(trend, series) %>%  dplyr::distinct() -> trend_map trend_map #>    trend            series #> 1      1 site_1_sp_1_rep_1 #> 2      1 site_1_sp_1_rep_2 #> 3      1 site_1_sp_1_rep_3 #> 4      1 site_1_sp_1_rep_4 #> 5      1 site_1_sp_1_rep_5 #> 6      2 site_1_sp_2_rep_1 #> 7      2 site_1_sp_2_rep_2 #> 8      2 site_1_sp_2_rep_3 #> 9      2 site_1_sp_2_rep_4 #> 10     2 site_1_sp_2_rep_5  # Fit a model mod <- mvgam(             # the observation formula sets up linear predictors for             # detection probability on the logit scale             formula = obs ~ species - 1,              # the trend_formula sets up the linear predictors for             # the latent abundance processes on the log scale             trend_formula = ~ s(time, by = trend, k = 4) + species,              # the trend_map takes care of the mapping             trend_map = trend_map,              # nmix() family and data             family = nmix(),             data = testdat,              # priors can be set in the usual way             priors = c(prior(std_normal(), class = b),                        prior(normal(1, 1.5), class = Intercept_trend)),             chains = 2) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 5.9 seconds. #> Chain 2 finished in 5.8 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 5.8 seconds. #> Total execution time: 6.0 seconds. #>   # The usual diagnostics summary(mod) #> GAM observation formula: #> obs ~ species - 1 #> <environment: 0x5578ecc9da08> #>  #> GAM process formula: #> ~s(time, by = trend, k = 4) + species #> <environment: 0x5578ecc9da08> #>  #> Family: #> nmix #>  #> Link function: #> log #>  #> Trend model: #> None #>  #> N process models: #> 2  #>  #> N series: #> 10  #>  #> N timepoints: #> 6  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM observation model coefficient (beta) estimates: #>              2.5%   50% 97.5% Rhat n_eff #> speciessp_1 -0.21 0.710  1.40    1   395 #> speciessp_2 -1.20 0.048  0.92    1   417 #>  #> GAM process model coefficient (beta) estimates: #>                               2.5%     50%  97.5% Rhat n_eff #> (Intercept)_trend            2.700  3.0000  3.400 1.00   355 #> speciessp_2_trend           -1.200 -0.6300  0.110 1.00   403 #> s(time):trendtrend1.1_trend -0.070  0.0140  0.180 1.01   284 #> s(time):trendtrend1.2_trend -0.240  0.0038  0.240 1.00   502 #> s(time):trendtrend1.3_trend -0.440 -0.2500 -0.036 1.01   522 #> s(time):trendtrend2.1_trend -0.250 -0.0150  0.079 1.01   117 #> s(time):trendtrend2.2_trend -0.220  0.0220  0.480 1.01   162 #> s(time):trendtrend2.3_trend  0.045  0.3200  0.600 1.00   396 #>  #> Approximate significance of GAM process smooths: #>                        edf Ref.df Chi.sq p-value #> s(time):seriestrend1 0.934      3   1.93    0.88 #> s(time):seriestrend2 1.037      3   1.14    0.90 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model  # Plotting conditional effects library(ggplot2); library(marginaleffects) plot_predictions(mod, condition = 'species',                  type = 'detection') +      ylab('Pr(detection)') +      ylim(c(0, 1)) +      theme_classic() +      theme(legend.position = 'none')   # Example showcasing how cbind() is needed for Binomial observations # Simulate two time series of Binomial trials trials <- sample(c(20:25), 50, replace = TRUE) x <- rnorm(50) detprob1 <- plogis(-0.5 + 0.9*x) detprob2 <- plogis(-0.1 -0.7*x) dat <- rbind(data.frame(y = rbinom(n = 50, size = trials, prob = detprob1),                         time = 1:50,                         series = 'series1',                         x = x,                         ntrials = trials),              data.frame(y = rbinom(n = 50, size = trials, prob = detprob2),                         time = 1:50,                         series = 'series2',                         x = x,                         ntrials = trials)) dat <- dplyr::mutate(dat, series = as.factor(series)) dat <- dplyr::arrange(dat, time, series)  # Fit a model using the binomial() family; must specify observations # and number of trials in the cbind() wrapper mod <- mvgam(cbind(y, ntrials) ~ series + s(x, by = series),              family = binomial(),              data = dat) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 4 chains, at most 3 in parallel... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 3 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 3 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 3 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 3 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 3 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 3 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 3 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 3 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 3 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 3 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 finished in 1.7 seconds. #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 3 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 3 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 4 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 3 finished in 1.8 seconds. #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 2.0 seconds. #> Chain 4 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 4 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 4 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 4 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 4 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 4 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 4 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 4 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 4 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 4 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 4 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 4 finished in 1.2 seconds. #>  #> All 4 chains finished successfully. #> Mean chain execution time: 1.7 seconds. #> Total execution time: 3.1 seconds. #>  summary(mod) #> GAM formula: #> cbind(y, ntrials) ~ series + s(x, by = series) #> <environment: 0x5578ecc9da08> #>  #> Family: #> binomial #>  #> Link function: #> logit #>  #> Trend model: #> None #>  #> N series: #> 2  #>  #> N timepoints: #> 50  #>  #> Status: #> Fitted using Stan  #> 4 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 2000 #>  #>  #> GAM coefficient (beta) estimates: #>                        2.5%      50%  97.5% Rhat n_eff #> (Intercept)          -0.650 -0.52000 -0.390 1.00  1204 #> seriesseries2         0.190  0.39000  0.570 1.00  1206 #> s(x):seriesseries1.1 -0.210 -0.00350  0.200 1.00   867 #> s(x):seriesseries1.2 -0.200  0.00360  0.270 1.00   633 #> s(x):seriesseries1.3 -0.079  0.00430  0.120 1.00   680 #> s(x):seriesseries1.4 -0.120  0.00330  0.120 1.00   691 #> s(x):seriesseries1.5 -0.050  0.00015  0.059 1.00   679 #> s(x):seriesseries1.6 -0.120 -0.00370  0.130 1.00   649 #> s(x):seriesseries1.7 -0.079 -0.00230  0.090 1.00   685 #> s(x):seriesseries1.8 -0.570 -0.01800  0.580 1.00   653 #> s(x):seriesseries1.9  0.670  0.95000  1.200 1.00  1085 #> s(x):seriesseries2.1 -0.620 -0.06200  0.120 1.00   225 #> s(x):seriesseries2.2 -0.390 -0.01800  0.220 1.00   444 #> s(x):seriesseries2.3 -0.170 -0.00560  0.087 1.00   414 #> s(x):seriesseries2.4 -0.120  0.00670  0.220 1.00   442 #> s(x):seriesseries2.5 -0.110 -0.00380  0.051 1.00   412 #> s(x):seriesseries2.6 -0.200 -0.00510  0.120 1.00   379 #> s(x):seriesseries2.7 -0.140 -0.00360  0.081 1.00   373 #> s(x):seriesseries2.8 -0.780  0.00250  0.580 1.00   416 #> s(x):seriesseries2.9 -1.000 -0.75000 -0.190 1.01   266 #>  #> Approximate significance of GAM smooths: #>                     edf Ref.df Chi.sq p-value     #> s(x):seriesseries1 1.41      9   51.3  <2e-16 *** #> s(x):seriesseries2 1.85      9   38.7  <2e-16 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_fevd-class.html","id":null,"dir":"Reference","previous_headings":"","what":"mvgam_fevd object description — mvgam_fevd-class","title":"mvgam_fevd object description — mvgam_fevd-class","text":"mvgam_fevd object returned function fevd(). Run methods(class = \"mvgam_fevd\") see overview available methods.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_fevd-class.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mvgam_fevd object description — mvgam_fevd-class","text":"forecast error variance decomposition useful quantifying amount information series Vector Autoregression contributes forecast distributions series autoregression. object contains forecast error variance decomposition using orthogonalised impulse response coefficient matrices \\(\\Psi_h\\), can used quantify contribution series \\(j\\) h-step forecast error variance series \\(k\\): $$ \\sigma_k^2(h) = \\sum_{j=1}^K(\\psi_{kj, 0}^2 + \\ldots + \\psi_{kj, h-1}^2) \\quad $$ orthogonalised impulse reponses \\((\\psi_{kj, 0}^2 + \\ldots + \\psi_{kj, h-1}^2)\\) divided variance forecast error \\(\\sigma_k^2(h)\\), yields interpretable percentage representing much forecast error variance \\(k\\) can explained exogenous shock \\(j\\). percentage calculated returned objects class mvgam_fevd, posterior distribution variance decompositions variable original model contained separate slot within returned list object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_fevd-class.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"mvgam_fevd object description — mvgam_fevd-class","text":"Lütkepohl, H (2006). New Introduction Multiple Time Series Analysis. Springer, New York.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_fevd-class.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mvgam_fevd object description — mvgam_fevd-class","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_forecast-class.html","id":null,"dir":"Reference","previous_headings":"","what":"mvgam_forecast object description — mvgam_forecast-class","title":"mvgam_forecast object description — mvgam_forecast-class","text":"mvgam_forecast object returned function hindcast forecast. Run methods(class = \"mvgam_forecast\") see overview available methods.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_forecast-class.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mvgam_forecast object description — mvgam_forecast-class","text":"mvgam_forecast object contains following elements: call original observation model formula trend_call trend_formula supplied, original trend model formula returned. Otherwise NULL family character description observation distribution family_pars list containing draws family-specific parameters (.e. shape, scale overdispersion parameters). returned type = link. Otherwise NULL trend_model character description latent trend model drift Logical specifying whether drift term used trend model use_lv Logical flag indicating whether latent dynamic factors used model fit_engine Character describing fit engine, either stan jags type type predictions included (either link, response trend) series_names Names time series, taken levels(data$series) original model fit train_observations list training observation vectors length n_series train_times list unique training times length n_series test_observations forecast function used, list test observation vectors length n_series. Otherwise NULL test_times forecast function used, list unique testing (validation) times length n_series. Otherwise NULL hindcasts list posterior hindcast distributions length n_series. forecasts forecast function used, list posterior forecast distributions length n_series. Otherwise NULL","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_forecast-class.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mvgam_forecast object description — mvgam_forecast-class","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_formulae.html","id":null,"dir":"Reference","previous_headings":"","what":"Details of formula specifications in mvgam models — mvgam_formulae","title":"Details of formula specifications in mvgam models — mvgam_formulae","text":"Details formula specifications mvgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_formulae.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Details of formula specifications in mvgam models — mvgam_formulae","text":"mvgam accept observation model formula optional process model formula (via argument trend_formula). Neither formulae can specified lists, contrary accepted behaviour mgcv brms models.  Note possible supply empty formula predictors intercepts observation model (.e. y ~ 0 y ~ -1). case, intercept-observation model set intercept coefficient fixed zero. can handy wish fit pure State-Space models variation dynamic trend controls average expectation, /intercepts non-identifiable.  formulae supplied mvgam jsdgam exactly like supplied glm except smooth terms, s, te, ti t2, time-varying effects using dynamic, monotonically increasing (using s(x, bs = 'moi')) decreasing splines (using s(x, bs = 'mod'); see smooth.construct.moi.smooth.spec details), well Gaussian Process functions using gp offsets using offset can added right hand side (. supported mvgam formulae).  details specifying different kinds smooth functions, control behaviours modifying potential complexities / penalties behave, can found extensive documentation mgcv package.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_formulae.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Details of formula specifications in mvgam models — mvgam_formulae","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_irf-class.html","id":null,"dir":"Reference","previous_headings":"","what":"mvgam_irf object description — mvgam_irf-class","title":"mvgam_irf object description — mvgam_irf-class","text":"mvgam_irf object returned function irf. Run methods(class = \"mvgam_irf\") see overview available methods.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_irf-class.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mvgam_irf object description — mvgam_irf-class","text":"Generalized Orthogonalized Impulse Response Functions can computed using posterior estimates Vector Autoregressive parameters. function generates positive \"shock\" target process time t = 0 calculates  remaining processes latent VAR expected respond forecast horizon h. function computes IRFs processes object returns array can plotted using S3 plot function. inspect community-level metrics stability using latent VAR processes, can use related stability() function. mvgam_irf object contains list posterior impulse response functions, stored list","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_irf-class.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"mvgam_irf object description — mvgam_irf-class","text":"PH Pesaran & Shin Yongcheol (1998). Generalized impulse response analysis linear multivariate models. Economics Letters 58: 17–29.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_irf-class.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mvgam_irf object description — mvgam_irf-class","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_marginaleffects.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper functions for marginaleffects calculations in mvgam models — mvgam_marginaleffects","title":"Helper functions for marginaleffects calculations in mvgam models — mvgam_marginaleffects","text":"Helper functions marginaleffects calculations mvgam models Functions needed working marginaleffects Functions needed getting data / objects insight","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_marginaleffects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper functions for marginaleffects calculations in mvgam models — mvgam_marginaleffects","text":"","code":"# S3 method for mvgam get_coef(model, trend_effects = FALSE, ...)  # S3 method for mvgam set_coef(model, coefs, trend_effects = FALSE, ...)  # S3 method for mvgam get_vcov(model, vcov = NULL, ...)  # S3 method for mvgam get_predict(model, newdata, type = \"response\", process_error = FALSE, ...)  # S3 method for mvgam get_data(x, source = \"environment\", verbose = TRUE, ...)  # S3 method for mvgam_prefit get_data(x, source = \"environment\", verbose = TRUE, ...)  # S3 method for mvgam find_predictors(   x,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"all\", \"conditional\", \"zi\", \"zero_inflated\", \"dispersion\", \"instruments\",     \"correlation\", \"smooth_terms\"),   flatten = FALSE,   verbose = TRUE,   ... )  # S3 method for mvgam_prefit find_predictors(   x,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"all\", \"conditional\", \"zi\", \"zero_inflated\", \"dispersion\", \"instruments\",     \"correlation\", \"smooth_terms\"),   flatten = FALSE,   verbose = TRUE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_marginaleffects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper functions for marginaleffects calculations in mvgam models — mvgam_marginaleffects","text":"model Model object trend_effects logical, extract process model component (applicable trend_formula specified model) ... Additional arguments passed predict() method supplied modeling package.arguments particularly useful mixed-effects bayesian models (see online vignettes marginaleffects website). Available arguments can vary model model, depending range supported arguments modeling package. See \"Model-Specific Arguments\" section ?slopes documentation non-exhaustive list available arguments. coefs vector coefficients insert model object vcov Type uncertainty estimates report (e.g., robust standard errors). Acceptable values: FALSE: compute standard errors. can speed computation considerably. TRUE: Unit-level standard errors using default vcov(model) variance-covariance matrix. String indicates kind uncertainty estimates return. Heteroskedasticity-consistent: \"HC\", \"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\", \"HC4m\", \"HC5\". See ?sandwich::vcovHC Heteroskedasticity autocorrelation consistent: \"HAC\" Mixed-Models degrees freedom: \"satterthwaite\", \"kenward-roger\" : \"NeweyWest\", \"KernHAC\", \"OPG\". See sandwich package documentation. One-sided formula indicates name cluster variables (e.g., ~unit_id). formula passed cluster argument sandwich::vcovCL function. Square covariance matrix Function returns covariance matrix (e.g., stats::vcov(model)) newdata Grid predictor values evaluate slopes. Warning: Please avoid modifying dataset fitting model calling marginaleffects function. can sometimes lead unexpected results. NULL (default): Unit-level slopes observed value dataset (empirical distribution). dataset retrieved using insight::get_data(), tries extract data environment. may produce unexpected results original data frame altered since fitting model. datagrid() call specify custom grid regressors. example: newdata = datagrid(cyl = c(4, 6)): cyl variable equal 4 6 regressors fixed means modes. See Examples section datagrid() documentation. subset() call single argument select subset dataset used fit model, ex: newdata = subset(treatment == 1) dplyr::filter() call single argument select subset dataset used fit model, ex: newdata = filter(treatment == 1) string: \"mean\": Slopes evaluated predictor held mean mode. \"median\": Slopes evaluated predictor held median mode. \"balanced\": Slopes evaluated balanced grid every combination categories numeric variables held means. \"tukey\": Slopes evaluated Tukey's 5 numbers. \"grid\": Slopes evaluated grid representative numbers (Tukey's 5 numbers unique values categorical predictors). type string indicates type (scale) predictions used compute contrasts slopes. can differ based model type, typically string : \"response\", \"link\", \"probs\", \"zero\". unsupported string entered, model-specific list acceptable values returned error message. type NULL, first entry error message used default. process_error logical. TRUE, uncertainty latent process (trend) model incorporated predictions x fitted model. source String, indicating data recovered. source = \"environment\" (default), data recovered environment (e.g. data workspace). option usually fastest way getting data ensures original variables used model fitting returned. Note always current data recovered environment. Hence, data modified model fitting (e.g., variables recoded rows filtered), returned data may longer equal model data. source = \"frame\" (\"mf\"), data taken model frame. transformed variables back-transformed, possible. option returns data even available environment, however, certain edge cases back-transforming original data may fail. source = \"environment\" fails recover data, tries extract data model frame; source = \"frame\" data extracted model frame, data recovered environment. ways returns observations missing data variables used model fitting. verbose Toggle messages warnings. effects model data fixed effects (\"fixed\"), random effects (\"random\") (\"\") returned? applies mixed gee models. component type parameters return, parameters conditional model, zero-inflated part model, dispersion term, instrumental variables marginal effects returned? Applies models zero-inflated /dispersion formula, models instrumental variables (called fixed-effects regressions), models marginal effects (mfx). See details section Model Components .May abbreviated. Note conditional component also refers count mean component - names may differ, depending modeling package. three convenient shortcuts (applicable model classes): component = \"\" returns possible parameters. component = \"location\", location parameters conditional, zero_inflated, smooth_terms, instruments returned (everything fixed random effects - depending effects argument - auxiliary parameters). component = \"distributional\" (\"auxiliary\"), components like sigma, dispersion, beta precision (auxiliary parameters) returned. flatten Logical, TRUE, values returned character vector, list. Duplicated values removed.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_marginaleffects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper functions for marginaleffects calculations in mvgam models — mvgam_marginaleffects","text":"Objects suitable internal 'marginaleffects' functions proceed. See marginaleffects::get_coef(), marginaleffects::set_coef(), marginaleffects::get_vcov(), marginaleffects::get_predict(), insight::get_data() insight::find_predictors() details","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_marginaleffects.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Helper functions for marginaleffects calculations in mvgam models — mvgam_marginaleffects","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_residcor-class.html","id":null,"dir":"Reference","previous_headings":"","what":"mvgam_residcor object description — mvgam_residcor-class","title":"mvgam_residcor object description — mvgam_residcor-class","text":"mvgam_residcor object returned function residual_cor(). Run methods(class = \"mvgam_residcor\") see overview available methods.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_residcor-class.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mvgam_residcor object description — mvgam_residcor-class","text":"Objects class structured list following components: cor, cor_lower, cor_upper set \\(p \\times p\\) correlation matrices, containing either posterior median mean estimate, plus lower upper limits corresponding credible intervals supplied probs sig_cor \\(p \\times p\\) correlation matrix containing correlations whose credible interval contain zero. correlations set zero prec, prec_lower, prec_upper set \\(p \\times p\\) precision matrices, containing either posterior median mean estimate, plus lower upper limits corresponding credible intervals supplied probs sig_prec \\(p \\times p\\) precision matrix containing precisions whose credible interval contain zero. precisions set zero cov \\(p \\times p\\) posterior median mean covariance matrix trace median/mean point estimator trace (sum diagonal elements) residual covariance matrix cov","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_residcor-class.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mvgam_residcor object description — mvgam_residcor-class","text":"Hui (2016) provides excellent description quantities function calculates, passage heavily paraphrased associated boral package. latent factor models, residual covariance matrix calculated based matrix latent factor loading matrix \\(\\Theta\\), residual covariance matrix \\(\\Sigma = \\Theta\\Theta'\\). strong residual covariance/correlation matrix two species can interpreted evidence species interactions (e.g., facilitation competition), missing covariates, well additional species correlation accounted shared environmental captured formula. residual precision matrix (also known partial correlation matrix, Ovaskainen et al., 2016) defined inverse residual correlation matrix. precision matrix often used identify direct causal relationships two species e.g., two species can zero precision still correlated, can interpreted saying two species directly associated, still correlated species. words, conditionally independent given species. important precision matrix exhibit exact properties correlation e.g., diagonal elements equal 1. Nevertheless, relatively larger values precision may imply stronger direct relationships two species. addition residual correlation precision matrices, median mean point estimator trace residual covariance matrix returned, \\(\\sum\\limits_{j=1}^p [\\Theta\\Theta']_{jj}\\). Often used areas multivariate statistics, trace may interpreted amount covariation explained latent factors. One situation trace may useful comparing pure latent factor model (terms suppled formula) versus model latent factors additional predictors formula -- proportional difference trace two models may interpreted proportion covariation species explained predictors formula. course, trace random due MCMC sampling, always guaranteed produce sensible answers.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_residcor-class.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"mvgam_residcor object description — mvgam_residcor-class","text":"Francis KC Hui (2016). BORAL - Bayesian ordination regression analysis multivariate abundance data R. Methods Ecology Evolution. 7, 744-750.  Otso Ovaskainen et al. (2016). Using latent variable models identify large networks species--species associations different spatial scales. Methods Ecology Evolution, 7, 549-555.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_residcor-class.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mvgam_residcor object description — mvgam_residcor-class","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_trends.html","id":null,"dir":"Reference","previous_headings":"","what":"Supported latent trend models in mvgam — mvgam_trends","title":"Supported latent trend models in mvgam — mvgam_trends","text":"Supported latent trend models mvgam","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_trends.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Supported latent trend models in mvgam — mvgam_trends","text":"mvgam currently supports following dynamic trend models: None (latent trend component; .e. GAM component contributes linear predictor, observation process source error; similarly estimated gam) ZMVN() (zero-mean correlated errors, useful modelling time series autoregressive terms needed modelling data sampled time series) RW() AR(p = 1, 2, 3) CAR(p = 1)(continuous time autoregressive trends; available Stan) VAR()(available Stan) PW() (piecewise linear logistic trends; available Stan) GP() (Gaussian Process squared exponential kernel; available Stan) dynamic trend types available mvgam (see argument trend_model), time measured discrete, regularly spaced intervals (.e. c(1, 2, 3, ...)). However can use irregularly spaced intervals using trend_model = CAR(1), though note temporal intervals exactly 0 adjusted small number (1e-12) prevent sampling errors. autoregressive trend types apart CAR(), moving average /correlated process error terms can also estimated (example, RW(cor = TRUE) set multivariate Random Walk data contains >1 series). Hierarchical process error correlations can also handled data contain relevant observation units nested relevant grouping subgrouping levels (.e. using AR(gr = region, subgr = species)) Note RW, AR1, AR2 AR3 available using JAGS. trend models supported using Stan. Dynamic factor models can used latent factors evolve either RW, AR1-3, VAR GP. VAR models (.e. VAR VARcor models), users can either fix trend error covariances 0 (using VAR) estimate potentially allow contemporaneously correlated errors using VARcor. VAR models, stationarity latent process enforced prior using parameterisation given Heaps (2022). Stationarity enforced using AR1, AR2 AR3 models, though can changed user specifying lower upper bounds autoregressive parameters using functionality get_mvgam_priors priors argument mvgam. Piecewise trends follow formulation popular prophet package produced Facebook, users can allow changepoints control potential flexibility trend. See Taylor Letham (2018) details","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/mvgam_trends.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Supported latent trend models in mvgam — mvgam_trends","text":"Sarah E. Heaps (2022) Enforcing stationarity prior Vector Autoregressions. Journal Computational Graphical Statistics. 32:1, 1-10. Sean J. Taylor Benjamin Letham (2018) Forecasting scale. American Statistician 72.1, 37-45.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/ordinate.jsdgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Latent variable ordination plots from jsdgam objects — ordinate.jsdgam","title":"Latent variable ordination plots from jsdgam objects — ordinate.jsdgam","text":"Plot ordination latent variables factor loadings jsdgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ordinate.jsdgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Latent variable ordination plots from jsdgam objects — ordinate.jsdgam","text":"","code":"ordinate(object, ...)  # S3 method for jsdgam ordinate(   object,   which_lvs = c(1, 2),   biplot = TRUE,   alpha = 0.5,   label_sites = TRUE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/ordinate.jsdgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Latent variable ordination plots from jsdgam objects — ordinate.jsdgam","text":"object list object class jsdgam resulting call jsdgam() ... ignored which_lvs vector indices indicating two latent variables plotted (number latent variables specified model 2). Defaults c(1, 2) biplot Logical. TRUE, site species scores plotted, names taxa interpreted based species argument original call jsdgam(). FALSE, site scores plotted alpha proportional numeric scalar 0 1 controls relative scaling latent variables loading coefficients label_sites Logical flag. TRUE, site scores plotted labels using names based unit argument original call jsdgam(). FALSE, site scores shown points ","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ordinate.jsdgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Latent variable ordination plots from jsdgam objects — ordinate.jsdgam","text":"ggplot object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ordinate.jsdgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Latent variable ordination plots from jsdgam objects — ordinate.jsdgam","text":"function constructs two-dimensional scatterplot ordination space. chosen latent variables first re-rotated using singular value decomposition, first plotted latent variable first latent variable estimated original model. Posterior median estimates variables species' loadings variables used construct resulting plot. attempt de-cluttering resulting plot made using geom_label_repel() geom_text_repel ggrepel package, many sites /species labels may removed automatically. Note can typically get better, readable plot layouts also ggarrow ggpp packages installed","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/ordinate.jsdgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Latent variable ordination plots from jsdgam objects — ordinate.jsdgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ordinate.jsdgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Latent variable ordination plots from jsdgam objects — ordinate.jsdgam","text":"","code":"# \\donttest{ # Fit a JSDGAM to the portal_data captures mod <- jsdgam(   formula = captures ~     # Fixed effects of NDVI and mintemp, row effect as a GP of time     ndvi_ma12:series + mintemp:series + gp(time, k = 15),   factor_formula = ~ -1,   data = portal_data,   unit = time,   species = series,   family = poisson(),   n_lv = 2,   silent = 2,   chains = 2 )  # Plot a residual ordination biplot ordinate(   mod,   alpha = 0.7 ) #> Warning: ggrepel: 16 unlabeled data points (too many overlaps). Consider increasing max.overlaps   # Compare to a residual correlation plot plot(   residual_cor(mod) )  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/pairs.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a matrix of output plots from a mvgam object — pairs.mvgam","title":"Create a matrix of output plots from a mvgam object — pairs.mvgam","text":"pairs method customized MCMC output.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pairs.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a matrix of output plots from a mvgam object — pairs.mvgam","text":"","code":"# S3 method for mvgam pairs(x, variable = NULL, regex = FALSE, use_alias = TRUE, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/pairs.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a matrix of output plots from a mvgam object — pairs.mvgam","text":"x object class mvgam jsdgam variable Names variables (parameters) plot, given character vector regular expression (regex = TRUE). default, hopefully large selection variables plotted. regex Logical; Indicates whether variable treated regular expressions. Defaults FALSE. use_alias Logical. informative names parameters available (.e. beta coefficients b smoothing parameters rho), replace uninformative names informative alias. Defaults TRUE ... arguments passed mcmc_pairs.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pairs.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a matrix of output plots from a mvgam object — pairs.mvgam","text":"Plottable objects whose classes depend arguments supplied. See mcmc_pairs details.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pairs.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a matrix of output plots from a mvgam object — pairs.mvgam","text":"detailed description see mcmc_pairs.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pairs.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a matrix of output plots from a mvgam object — pairs.mvgam","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 1, trend_model = 'AR1') mod <- mvgam(y ~ s(season, bs = 'cc'),              trend_model = AR(),              noncentred = TRUE,              data = simdat$data_train,              chains = 2) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 0.4 seconds. #> Chain 2 finished in 0.4 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 0.4 seconds. #> Total execution time: 0.5 seconds. #>  pairs(mod)  pairs(mod, variable = c('ar1', 'sigma'), regex = TRUE)  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/piecewise_trends.html","id":null,"dir":"Reference","previous_headings":"","what":"Specify piecewise linear or logistic trends in mvgam models — PW","title":"Specify piecewise linear or logistic trends in mvgam models — PW","text":"Set piecewise linear logistic trend models mvgam. functions evaluate arguments – exist purely help set model particular piecewise trend models.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/piecewise_trends.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specify piecewise linear or logistic trends in mvgam models — PW","text":"","code":"PW(   n_changepoints = 10,   changepoint_range = 0.8,   changepoint_scale = 0.05,   growth = \"linear\" )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/piecewise_trends.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specify piecewise linear or logistic trends in mvgam models — PW","text":"n_changepoints non-negative integer specifying number potential changepoints. Potential changepoints selected uniformly first changepoint_range proportion timepoints data. Default 10 changepoint_range Proportion history data trend changepoints estimated. Defaults 0.8 first 80%. changepoint_scale Parameter modulating flexibility automatic changepoint selection altering scale parameter Laplace distribution. resulting prior double_exponential(0, changepoint_scale). Large values allow many changepoints flexible trend, small values allow changepoints. Default 0.05. growth Character string specifying either 'linear' 'logistic' growth trend. 'logistic', variable labelled cap MUST data specify maximum saturation point trend (see details examples mvgam information). Default 'linear'.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/piecewise_trends.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specify piecewise linear or logistic trends in mvgam models — PW","text":"object class mvgam_trend, contains list arguments interpreted parsing functions mvgam","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/piecewise_trends.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Specify piecewise linear or logistic trends in mvgam models — PW","text":"Offsets intercepts: trend models, offset parameter included trend estimation process. parameter incredibly difficult identify also include intercept observation formula. reason, highly recommended drop intercept formula (.e. y ~ x + 0 y ~ x - 1, x optional predictor terms). Logistic growth cap variable: forecasting growth, often maximum achievable point time series can reach. example, total market size, total population size carrying capacity population dynamics. can advantageous forecast saturate near point predictions sensible. function allows make forecasts using logistic growth trend model, specified carrying capacity. Note capacity need static time, can vary series x timepoint combination necessary. must supply cap value observation data using growth = 'logistic'. observation families use non-identity link function, cap value internally transformed link scale (.e. specified cap log transformed using poisson() nb() family). therefore important specify cap values scale outcome. Note also missing values allowed cap.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/piecewise_trends.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Specify piecewise linear or logistic trends in mvgam models — PW","text":"Taylor, Sean J., Benjamin Letham. \"Forecasting scale.\" American Statistician 72.1 (2018): 37-45.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/piecewise_trends.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Specify piecewise linear or logistic trends in mvgam models — PW","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/piecewise_trends.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Specify piecewise linear or logistic trends in mvgam models — PW","text":"","code":"# \\donttest{ # Example of logistic growth with possible changepoints # Simple logistic growth model dNt = function(r, N, k){    r * N * (k - N) }  # Iterate growth through time Nt = function(r, N, t, k) { for (i in 1:(t - 1)) {   # population at next time step is current population + growth,  # but we introduce several 'shocks' as changepoints  if(i %in% c(5, 15, 25, 41, 45, 60, 80)){    N[i + 1] <- max(1, N[i] + dNt(r + runif(1, -0.1, 0.1),                                  N[i], k))    } else {    N[i + 1] <- max(1, N[i] + dNt(r, N[i], k))    }   }  N }  # Simulate expected values set.seed(11) expected <- Nt(0.004, 2, 100, 30) plot(expected, xlab = 'Time')   # Take Poisson draws y <- rpois(100, expected) plot(y, xlab = 'Time')   # Assemble data into dataframe and model. We set a # fixed carrying capacity of 35 for this example, but note that # this value is not required to be fixed at each timepoint mod_data <- data.frame(y = y,                        time = 1:100,                        cap = 35,                        series = as.factor('series_1')) plot_mvgam_series(data = mod_data)   # The intercept is nonidentifiable when using piecewise # trends because the trend functions have their own offset # parameters 'm'; it is recommended to always drop intercepts # when using these trend models mod <- mvgam(y ~ 0,              trend_model = PW(growth = 'logistic'),              family = poisson(),              data = mod_data,              chains = 2,              silent = 2) summary(mod) #> GAM formula: #> y ~ 1 #> <environment: 0x5578f7ba0850> #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> PW(growth = \"logistic\") #>  #>  #> N series: #> 1  #>  #> N timepoints: #> 100  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM coefficient (beta) estimates: #>             2.5% 50% 97.5% Rhat n_eff #> (Intercept)    0   0     0  NaN   NaN #>  #> Latent trend growth rate estimates: #>             2.5%   50% 97.5% Rhat n_eff #> k_trend[1] -0.26 -0.15 -0.07    1   391 #>  #> Latent trend offset estimates: #>            2.5%  50% 97.5% Rhat n_eff #> m_trend[1]  -18 -4.2 -0.38    1   235 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✖ 102 of 1000 iterations saturated the maximum tree depth of 10 (10.2%) #>     Try a larger max_treedepth to avoid saturation #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model  # Plot the posterior hindcast hc <- hindcast(mod) plot(hc) #> No non-missing values in test_observations; cannot calculate forecast score   # View the changepoints with ggplot2 utilities library(ggplot2) mcmc_plot(mod, variable = 'delta_trend',           regex = TRUE) + scale_y_discrete(labels = mod$trend_model$changepoints) + labs(y = 'Potential changepoint',      x = 'Rate change') #> Scale for y is already present. #> Adding another scale for y, which will replace the existing scale.   # Generate a methods description scaffold how_to_cite(mod) #> Methods text skeleton #> We used the R package mvgam (version 1.1.57; Clark & Wells, 2023) to #>   construct, fit and interrogate the model. mvgam fits Bayesian #>   State-Space models that can include flexible predictor effects in both #>   the process and observation components by incorporating functionalities #>   from the brms (Burkner 2017), mgcv (Wood 2017) and splines2 (Wang & Yan, #>   2023) packages. Piecewise dynamic trends were parameterized and #>   estimated following methods described by Taylor and Letham (2018). The #>   mvgam-constructed model and observed data were passed to the #>   probabilistic programming environment Stan (version 2.36.0; Carpenter et #>   al. 2017, Stan Development Team 2025), specifically through the cmdstanr #>   interface (Gabry & Cesnovar, 2021). We ran 2 Hamiltonian Monte Carlo #>   chains for 500 warmup iterations and 500 sampling iterations for joint #>   posterior estimation. Rank normalized split Rhat (Vehtari et al. 2021) #>   and effective sample sizes were used to monitor convergence. #>  #> Primary references #> Clark, NJ and Wells K (2023). Dynamic Generalized Additive Models #>   (DGAMs) for forecasting discrete ecological time series. Methods in #>   Ecology and Evolution, 14, 771-784. doi.org/10.1111/2041-210X.13974 #> Burkner, PC (2017). brms: An R Package for Bayesian Multilevel Models #>   Using Stan. Journal of Statistical Software, 80(1), 1-28. #>   doi:10.18637/jss.v080.i01 #> Wood, SN (2017). Generalized Additive Models: An Introduction with R #>   (2nd edition). Chapman and Hall/CRC. #> Wang W and Yan J (2021). Shape-Restricted Regression Splines with R #>   Package splines2. Journal of Data Science, 19(3), 498-517. #>   doi:10.6339/21-JDS1020 https://doi.org/10.6339/21-JDS1020. #> Taylor S and Letham B (2018). Forecasting at scale. The American #>   Statistician 72(1) 37-45. https://doi.org/10.1080/00031305.2017.1380080 #> Carpenter B, Gelman A, Hoffman MD, Lee D, Goodrich B, Betancourt M, #>   Brubaker M, Guo J, Li P and Riddell A (2017). Stan: A probabilistic #>   programming language. Journal of Statistical Software 76. #> Gabry J, Cesnovar R, Johnson A, and Bronder S (2025). cmdstanr: R #>   Interface to 'CmdStan'. https://mc-stan.org/cmdstanr/, #>   https://discourse.mc-stan.org. #> Vehtari A, Gelman A, Simpson D, Carpenter B, and Burkner P (2021). #>   Rank-normalization, folding, and localization: An improved Rhat for #>   assessing convergence of MCMC (with discussion). Bayesian Analysis 16(2) #>   667-718. https://doi.org/10.1214/20-BA1221. #>  #> Other useful references #> Arel-Bundock V, Greifer N, and Heiss A (2024). How to interpret #>   statistical models using marginaleffects for R and Python. Journal of #>   Statistical Software, 111(9), 1-32. #>   https://doi.org/10.18637/jss.v111.i09 #> Gabry J, Simpson D, Vehtari A, Betancourt M, and Gelman A (2019). #>   Visualization in Bayesian workflow. Journal of the Royal Statatistical #>   Society A, 182, 389-402. doi:10.1111/rssa.12378. #> Vehtari A, Gelman A, and Gabry J (2017). Practical Bayesian model #>   evaluation using leave-one-out cross-validation and WAIC. Statistics and #>   Computing, 27, 1413-1432. doi:10.1007/s11222-016-9696-4. #> Burkner PC, Gabry J, and Vehtari A. (2020). Approximate leave-future-out #>   cross-validation for Bayesian time series models. Journal of Statistical #>   Computation and Simulation, 90(14), 2499-2523. #>   https://doi.org/10.1080/00949655.2020.1783262 # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://nicholasjclark.github.io/mvgam/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Default plots for mvgam models — plot.mvgam","title":"Default plots for mvgam models — plot.mvgam","text":"function takes fitted mvgam object produces plots smooth functions, forecasts, trends uncertainty components","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default plots for mvgam models — plot.mvgam","text":"","code":"# S3 method for mvgam plot(   x,   type = \"residuals\",   series = 1,   residuals = FALSE,   newdata,   data_test,   trend_effects = FALSE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Default plots for mvgam models — plot.mvgam","text":"x list object returned mvgam. See mvgam() type character specifying type plot return. Options : series, residuals, smooths, re (random effect smooths), pterms (parametric effects), forecast, trend, uncertainty, factors series integer specifying series set plotted. ignored type == 're' residuals logical. TRUE type = 'smooths', posterior quantiles partial residuals added plots 1-D smooths series ribbon rectangles. Partial residuals smooth term median Dunn-Smyth residuals obtained dropping term concerned model, leaving estimates fixed (.e. estimates term plus original median Dunn-Smyth residuals). Note mvgam works Dunn-Smyth residuals working residuals, used mgcv, magnitudes partial residuals different expect plot.gam. Interpretation similar though, partial residuals evenly scattered around smooth function function well estimated newdata Optional dataframe list test data containing least 'series' 'time' addition variables included linear predictor original formula. argument optional plotting sample forecast period observations (type = forecast) required plotting uncertainty components (type = uncertainty). data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows trend_effects logical. TRUE trend_formula used model fitting, terms trend (.e. process) model plotted ... Additional arguments individual plotting function.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Default plots for mvgam models — plot.mvgam","text":"base R plot set plots","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Default plots for mvgam models — plot.mvgam","text":"plots useful getting overview fitted model estimated random effects smooth functions, individual plotting functions functions marginaleffects gratia packages offer far customisation.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Default plots for mvgam models — plot.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Default plots for mvgam models — plot.mvgam","text":"","code":"# \\donttest{ # Simulate some time series dat <- sim_mvgam(T = 80, n_series = 3)  # Fit a basic model mod <- mvgam(y ~ s(season, bs = 'cc') + s(series, bs = 're'),             data = dat$data_train,             trend_model = RW(),             chains = 2,             silent = 2)  # Plot predictions and residuals for each series plot(mod, type = 'forecast', series = 1)  plot(mod, type = 'forecast', series = 2)  plot(mod, type = 'forecast', series = 3)  plot(mod, type = 'residuals', series = 1)  plot(mod, type = 'residuals', series = 2)  plot(mod, type = 'residuals', series = 3)   # Plot model effects plot(mod, type = 'smooths')  plot(mod, type = 're')   # More flexible plots with 'marginaleffects' utilities library(marginaleffects) plot_predictions(mod, condition = 'season', type = 'link')  plot_predictions(mod,                 condition = c('season', 'series', 'series'),                 type = 'link')  plot_predictions(mod, condition = 'series', type = 'link')   # When using a State-Space model with predictors on the process # model, set trend_effects = TRUE to visualise process effects mod <- mvgam(y ~ -1,             trend_formula = ~ s(season, bs = 'cc'),             data = dat$data_train,             trend_model = RW(),             chains = 2,             silent = 2) plot(mod, type = 'smooths', trend_effects = TRUE)   # But marginaleffects functions work without any modification plot_predictions(mod, condition = 'season', type = 'link')   # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_fevd.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot forecast error variance decompositions from an mvgam_fevd object — plot.mvgam_fevd","title":"Plot forecast error variance decompositions from an mvgam_fevd object — plot.mvgam_fevd","text":"function takes mvgam_fevd object produces plot posterior median contributions forecast variance series fitted Vector Autoregression","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_fevd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot forecast error variance decompositions from an mvgam_fevd object — plot.mvgam_fevd","text":"","code":"# S3 method for mvgam_fevd plot(x, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_fevd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot forecast error variance decompositions from an mvgam_fevd object — plot.mvgam_fevd","text":"x list object class mvgam_fevd. See fevd() ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_fevd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot forecast error variance decompositions from an mvgam_fevd object — plot.mvgam_fevd","text":"ggplot object, can customized using ggplot2 package","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_fevd.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot forecast error variance decompositions from an mvgam_fevd object — plot.mvgam_fevd","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_irf.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot impulse responses from an mvgam_irf object — plot.mvgam_irf","title":"Plot impulse responses from an mvgam_irf object — plot.mvgam_irf","text":"function takes mvgam_irf object produces plots Impulse Response Functions","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_irf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot impulse responses from an mvgam_irf object — plot.mvgam_irf","text":"","code":"# S3 method for mvgam_irf plot(x, series = 1, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_irf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot impulse responses from an mvgam_irf object — plot.mvgam_irf","text":"x list object class mvgam_irf. See irf() series integer specifying process series given shock ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_irf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot impulse responses from an mvgam_irf object — plot.mvgam_irf","text":"ggplot object showing expected response latent time series shock focal series","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_irf.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot impulse responses from an mvgam_irf object — plot.mvgam_irf","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_lfo.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Pareto-k and ELPD values from a mvgam_lfo object — plot.mvgam_lfo","title":"Plot Pareto-k and ELPD values from a mvgam_lfo object — plot.mvgam_lfo","text":"function takes object class mvgam_lfo creates several informative diagnostic plots","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_lfo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Pareto-k and ELPD values from a mvgam_lfo object — plot.mvgam_lfo","text":"","code":"# S3 method for mvgam_lfo plot(x, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_lfo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Pareto-k and ELPD values from a mvgam_lfo object — plot.mvgam_lfo","text":"x object class mvgam_lfo ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_lfo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Pareto-k and ELPD values from a mvgam_lfo object — plot.mvgam_lfo","text":"ggplot object presenting Pareto-k ELPD values evaluation timepoints. Pareto-k plot, dashed red line indicates specified threshold chosen triggering model refits. ELPD plot, dashed red line indicates bottom 10% quantile ELPD values. Points threshold may represent outliers difficult forecast","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_residcor.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot residual correlations based on latent factors — plot.mvgam_residcor","title":"Plot residual correlations based on latent factors — plot.mvgam_residcor","text":"Plot residual correlation estimates Joint Species Distribution (jsdgam) dynamic factor (mvgam) models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_residcor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot residual correlations based on latent factors — plot.mvgam_residcor","text":"","code":"# S3 method for mvgam_residcor plot(x, cluster = FALSE, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_residcor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot residual correlations based on latent factors — plot.mvgam_residcor","text":"x list object class mvgam_residcor resulting call residual_cor(..., summary = TRUE) cluster Logical. variables re-arranged within plot group correlation matrix clusters positive negative correlations? Defaults FALSE ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_residcor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot residual correlations based on latent factors — plot.mvgam_residcor","text":"ggplot object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_residcor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot residual correlations based on latent factors — plot.mvgam_residcor","text":"function plots significant residual correlations mvgam_residcor object, whereby posterior mean (robust = FALSE) posterior median (robust = TRUE) correlations shown correlations whose credible interval contain zero. correlations set zero returned plot","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot.mvgam_residcor.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot residual correlations based on latent factors — plot.mvgam_residcor","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_factors.html","id":null,"dir":"Reference","previous_headings":"","what":"Latent factor summaries for a fitted mvgam object — plot_mvgam_factors","title":"Latent factor summaries for a fitted mvgam object — plot_mvgam_factors","text":"function takes fitted mvgam object returns plots summary statistics latent dynamic factors","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_factors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Latent factor summaries for a fitted mvgam object — plot_mvgam_factors","text":"","code":"plot_mvgam_factors(object, plot = TRUE)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_factors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Latent factor summaries for a fitted mvgam object — plot_mvgam_factors","text":"object list object returned mvgam. See mvgam() plot logical specifying whether factors plotted","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_factors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Latent factor summaries for a fitted mvgam object — plot_mvgam_factors","text":"data.frame factor contributions , optionally, ggplot object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_factors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Latent factor summaries for a fitted mvgam object — plot_mvgam_factors","text":"model object estimated using dynamic factors, possible factors contributed estimated trends. due regularisation penalty acts independently factor's Gaussian precision, squeeze un-needed factors white noise process (effectively dropping factor model). function, factor tested null hypothesis white noise calculating sum factor's 2nd derivatives. factor larger contribution larger sum due weaker penalty factor's precision. plot == TRUE, factors also plotted.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_factors.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Latent factor summaries for a fitted mvgam object — plot_mvgam_factors","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_factors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Latent factor summaries for a fitted mvgam object — plot_mvgam_factors","text":"","code":"# \\donttest{ simdat <- sim_mvgam() mod <- mvgam(y ~ s(season, bs = 'cc',                   k = 6),             trend_model = AR(),             use_lv = TRUE,             n_lv = 2,             data = simdat$data_train,             chains = 2,             silent = 2) #> Warning in '/tmp/RtmppUq4oL/model_0f5e123adc47a9208b1c8eaa9e58906b.stan', line 23, column 31: Found #>     int division: #>       n_lv * (n_lv - 1) / 2 #>     Values will be rounded towards zero. If rounding is not desired you can #>     write #>     the division as #>       n_lv * (n_lv - 1) / 2.0 #>     If rounding is intended please use the integer division operator %/%. #> Warning in '/tmp/RtmppUq4oL/model-22f5272def87.stan', line 23, column 33: Found #>     int division: #>       n_lv * (n_lv - 1) / 2 #>     Values will be rounded towards zero. If rounding is not desired you can #>     write #>     the division as #>       n_lv * (n_lv - 1) / 2.0 #>     If rounding is intended please use the integer division operator %/%. plot_mvgam_factors(mod)  #> # A tibble: 2 × 2 #>   Factor   Contribution #>   <chr>           <dbl> #> 1 Factor 1        0.490 #> 2 Factor 2        0.510 # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_forecasts.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot posterior forecast predictions from mvgam models — plot_mvgam_forecasts","title":"Plot posterior forecast predictions from mvgam models — plot_mvgam_forecasts","text":"Plot posterior forecast predictions mvgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_forecasts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot posterior forecast predictions from mvgam models — plot_mvgam_forecasts","text":"","code":"plot_mvgam_fc(   object,   series = 1,   newdata,   data_test,   realisations = FALSE,   n_realisations = 15,   hide_xlabels = FALSE,   xlab,   ylab,   ylim,   n_cores = 1,   return_forecasts = FALSE,   return_score = FALSE,   ... )  # S3 method for mvgam_forecast plot(   x,   series = 1,   realisations = FALSE,   n_realisations = 15,   xlab,   ylab,   ylim,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_forecasts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot posterior forecast predictions from mvgam models — plot_mvgam_forecasts","text":"object list object class mvgam. See mvgam() series integer specifying series set plotted newdata Optional dataframe list test data containing least 'series' 'time' addition variables included linear predictor original formula. included, covariate information newdata used generate forecasts fitted model equations. newdata originally included call mvgam, forecasts already produced generative model simply extracted plotted. However newdata supplied original model call, assumption made newdata supplied comes sequentially data supplied data original model (.e. assume time gap last observation series 1 data first observation series 1 newdata). newdata contains observations column y, observations used compute Discrete Rank Probability Score forecast distribution data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows realisations logical. TRUE, forecast realisations shown spaghetti plot, making easier visualise diversity possible forecasts. FALSE, default, empirical quantiles forecast distribution shown n_realisations integer specifying number posterior realisations plot, realisations = TRUE. Ignored otherwise hide_xlabels logical. TRUE, xlabels printed allow user add custom labels using axis base R xlab label x axis. ylab label y axis. ylim Optional vector y-axis limits (min, max) n_cores integer specifying number cores generating forecasts parallel return_forecasts logical. TRUE, function plot forecast well returning forecast object (matrix dimension n_samples x horizon) return_score logical. TRUE sample test data provided newdata, probabilistic score calculated returned. score used depend observation family fitted model. Discrete families (poisson, negative binomial, tweedie) use Discrete Rank Probability Score. families use Continuous Rank Probability Score. value returned sum scores within sample forecast horizon ... par graphical parameters. x Object class mvgam_forecast","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_forecasts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot posterior forecast predictions from mvgam models — plot_mvgam_forecasts","text":"base R graphics plot (plot_mvgam_fc) ggplot object (plot.mvgam_forecast) optional list containing forecast distribution sample probabilistic forecast score","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_forecasts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot posterior forecast predictions from mvgam models — plot_mvgam_forecasts","text":"plot_mvgam_fc generates posterior predictions object class mvgam, calculates posterior empirical quantiles plots observed data. realisations = FALSE, returned plot shows 90, 60, 40 20 percent posterior quantiles (ribbons increasingly darker shades red) well posterior median (dark red line). realisations = FALSE, set n_realisations posterior draws shown. function produces older style base R plot, opposed plot.mvgam_forecast plot.mvgam_forecast takes object class mvgam_forecast, forecasts already computed, plots resulting forecast distribution ggplot object. function therefore versatile recommended older clunkier plot_mvgam_fc version realisations = FALSE, posterior quantiles plotted along true observed data used train model. Otherwise, spaghetti plot returned show possible forecast paths.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_forecasts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot posterior forecast predictions from mvgam models — plot_mvgam_forecasts","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 3, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),             trend_model = AR(),             noncentred = TRUE,             data = simdat$data_train,             chains = 2,             silent = 2)  # Hindcasts on response scale hc <- hindcast(mod) str(hc) #> List of 15 #>  $ call              :Class 'formula'  language y ~ s(season, bs = \"cc\", k = 6) #>   .. ..- attr(*, \".Environment\")=<environment: 0x5578f7411a60>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ trend_model       :List of 7 #>   ..$ trend_model: chr \"AR1\" #>   ..$ ma         : logi FALSE #>   ..$ cor        : logi FALSE #>   ..$ unit       : chr \"time\" #>   ..$ gr         : chr \"NA\" #>   ..$ subgr      : chr \"series\" #>   ..$ label      : language AR() #>   ..- attr(*, \"class\")= chr \"mvgam_trend\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : chr [1:3] \"series_1\" \"series_2\" \"series_3\" #>  $ train_observations:List of 3 #>   ..$ series_1: int [1:75] 4 3 0 0 0 0 0 3 1 0 ... #>   ..$ series_2: int [1:75] 5 1 0 0 0 0 0 0 2 3 ... #>   ..$ series_3: int [1:75] 1 2 1 0 1 0 0 0 0 2 ... #>  $ train_times       :List of 3 #>   ..$ series_1: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_2: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_3: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations : NULL #>  $ test_times        : NULL #>  $ hindcasts         :List of 3 #>   ..$ series_1: num [1:1000, 1:75] 3 1 6 1 2 3 4 2 6 6 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>   ..$ series_2: num [1:1000, 1:75] 2 4 0 3 3 3 3 0 1 2 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,2]\" \"ypred[2,2]\" \"ypred[3,2]\" \"ypred[4,2]\" ... #>   ..$ series_3: num [1:1000, 1:75] 1 5 1 4 1 3 5 1 0 3 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,3]\" \"ypred[2,3]\" \"ypred[3,3]\" \"ypred[4,3]\" ... #>  $ forecasts         : NULL #>  - attr(*, \"class\")= chr \"mvgam_forecast\" plot(hc, series = 1) #> No non-missing values in test_observations; cannot calculate forecast score  plot(hc, series = 2) #> No non-missing values in test_observations; cannot calculate forecast score  plot(hc, series = 3) #> No non-missing values in test_observations; cannot calculate forecast score   # Forecasts on response scale fc <- forecast(mod, newdata = simdat$data_test) str(fc) #> List of 16 #>  $ call              :Class 'formula'  language y ~ s(season, bs = \"cc\", k = 6) #>   .. ..- attr(*, \".Environment\")=<environment: 0x5578f7411a60>  #>  $ trend_call        : NULL #>  $ family            : chr \"poisson\" #>  $ family_pars       : NULL #>  $ trend_model       :List of 7 #>   ..$ trend_model: chr \"AR1\" #>   ..$ ma         : logi FALSE #>   ..$ cor        : logi FALSE #>   ..$ unit       : chr \"time\" #>   ..$ gr         : chr \"NA\" #>   ..$ subgr      : chr \"series\" #>   ..$ label      : language AR() #>   ..- attr(*, \"class\")= chr \"mvgam_trend\" #>  $ drift             : logi FALSE #>  $ use_lv            : logi FALSE #>  $ fit_engine        : chr \"stan\" #>  $ type              : chr \"response\" #>  $ series_names      : Factor w/ 3 levels \"series_1\",\"series_2\",..: 1 2 3 #>  $ train_observations:List of 3 #>   ..$ series_1: int [1:75] 4 3 0 0 0 0 0 3 1 0 ... #>   ..$ series_2: int [1:75] 5 1 0 0 0 0 0 0 2 3 ... #>   ..$ series_3: int [1:75] 1 2 1 0 1 0 0 0 0 2 ... #>  $ train_times       :List of 3 #>   ..$ series_1: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_2: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ series_3: int [1:75] 1 2 3 4 5 6 7 8 9 10 ... #>  $ test_observations :List of 3 #>   ..$ series_1: int [1:25] 0 1 0 0 4 3 1 1 6 8 ... #>   ..$ series_2: int [1:25] 0 0 0 0 0 4 0 1 3 2 ... #>   ..$ series_3: int [1:25] 0 0 0 0 0 0 0 1 1 3 ... #>  $ test_times        :List of 3 #>   ..$ series_1: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>   ..$ series_2: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>   ..$ series_3: int [1:25] 76 77 78 79 80 81 82 83 84 85 ... #>  $ hindcasts         :List of 3 #>   ..$ series_1: num [1:1000, 1:75] 3 1 6 1 2 3 4 2 6 6 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,1]\" \"ypred[2,1]\" \"ypred[3,1]\" \"ypred[4,1]\" ... #>   ..$ series_2: num [1:1000, 1:75] 2 4 0 3 3 3 3 0 1 2 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,2]\" \"ypred[2,2]\" \"ypred[3,2]\" \"ypred[4,2]\" ... #>   ..$ series_3: num [1:1000, 1:75] 1 5 1 4 1 3 5 1 0 3 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : chr [1:75] \"ypred[1,3]\" \"ypred[2,3]\" \"ypred[3,3]\" \"ypred[4,3]\" ... #>  $ forecasts         :List of 3 #>   ..$ series_1: int [1:1000, 1:25] 0 2 0 0 1 0 1 1 1 1 ... #>   ..$ series_2: int [1:1000, 1:25] 0 0 2 0 2 2 2 0 0 2 ... #>   ..$ series_3: int [1:1000, 1:25] 0 0 0 1 0 2 0 0 0 0 ... #>  - attr(*, \"class\")= chr \"mvgam_forecast\" plot(fc, series = 1) #> Out of sample DRPS: #> 36.739542  plot(fc, series = 2) #> Out of sample DRPS: #> 12.462298  plot(fc, series = 3) #> Out of sample DRPS: #> 9.974369   # Forecasts as expectations fc <- forecast(mod, newdata = simdat$data_test, type = 'expected') plot(fc, series = 1)  plot(fc, series = 2)  plot(fc, series = 3)   # Dynamic trend extrapolations fc <- forecast(mod, newdata = simdat$data_test, type = 'trend') plot(fc, series = 1)  plot(fc, series = 2)  plot(fc, series = 3)  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_pterms.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot parametric term partial effects for mvgam models — plot_mvgam_pterms","title":"Plot parametric term partial effects for mvgam models — plot_mvgam_pterms","text":"function plots posterior empirical quantiles partial effects parametric terms","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_pterms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot parametric term partial effects for mvgam models — plot_mvgam_pterms","text":"","code":"plot_mvgam_pterms(object, trend_effects = FALSE)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_pterms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot parametric term partial effects for mvgam models — plot_mvgam_pterms","text":"object list object class mvgam. See mvgam() trend_effects logical. TRUE trend_formula used model fitting, terms trend (.e. process) model plotted","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_pterms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot parametric term partial effects for mvgam models — plot_mvgam_pterms","text":"base R graphics plot","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_pterms.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot parametric term partial effects for mvgam models — plot_mvgam_pterms","text":"Posterior empirical quantiles parametric term's partial effect estimates (link scale) calculated visualised ribbon plots. effects can interpreted partial effect parametric term contributes terms model set 0","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_randomeffects.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot random effect terms from mvgam models — plot_mvgam_randomeffects","title":"Plot random effect terms from mvgam models — plot_mvgam_randomeffects","text":"function plots posterior empirical quantiles random effect smooths (bs = re)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_randomeffects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot random effect terms from mvgam models — plot_mvgam_randomeffects","text":"","code":"plot_mvgam_randomeffects(object, trend_effects = FALSE)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_randomeffects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot random effect terms from mvgam models — plot_mvgam_randomeffects","text":"object list object class mvgam. See mvgam() trend_effects logical. TRUE trend_formula used model fitting, terms trend (.e. process) model plotted","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_randomeffects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot random effect terms from mvgam models — plot_mvgam_randomeffects","text":"base R graphics plot","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_randomeffects.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot random effect terms from mvgam models — plot_mvgam_randomeffects","text":"Posterior empirical quantiles random effect coefficient estimates (link scale) calculated visualised ribbon plots. Labels coefficients taken levels original factor variable used specify smooth model's formula","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_resids.html","id":null,"dir":"Reference","previous_headings":"","what":"Residual diagnostics for a fitted mvgam object — plot_mvgam_resids","title":"Residual diagnostics for a fitted mvgam object — plot_mvgam_resids","text":"function takes fitted mvgam object returns various residual diagnostic plots","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_resids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Residual diagnostics for a fitted mvgam object — plot_mvgam_resids","text":"","code":"plot_mvgam_resids(object, series = 1, n_draws = 100L, n_points = 1000L)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_resids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Residual diagnostics for a fitted mvgam object — plot_mvgam_resids","text":"object list object returned mvgam. See mvgam() series integer specifying series set plotted n_draws integer specifying number posterior residual draws use calculating uncertainty \"ACF\" \"pACF\" frames. Default 100 n_points integer specifying maximum number points show \"Resids vs Fitted\" \"Normal Q-Q Plot\" frames. Default 1000","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_resids.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Residual diagnostics for a fitted mvgam object — plot_mvgam_resids","text":"series facetted ggplot object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_resids.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Residual diagnostics for a fitted mvgam object — plot_mvgam_resids","text":"total four ggplot plots generated examine posterior Dunn-Smyth residuals specified series. Plots include residuals vs fitted values plot, Q-Q plot, two plots check remaining temporal autocorrelation residuals. Note, plots report statistics sample 100 posterior draws (save computational time), uncertainty relationships may adequately represented.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_resids.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Residual diagnostics for a fitted mvgam object — plot_mvgam_resids","text":"Nicholas J Clark Nicholas J Clark Matthijs Hollanders","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_resids.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Residual diagnostics for a fitted mvgam object — plot_mvgam_resids","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 3, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),             trend_model = AR(),             noncentred = TRUE,             data = simdat$data_train,             chains = 2,             silent = 2)  # Plot Dunn Smyth residuals for some series plot_mvgam_resids(mod)  plot_mvgam_resids(mod, series = 2)  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot observed time series used for mvgam modelling — plot_mvgam_series","title":"Plot observed time series used for mvgam modelling — plot_mvgam_series","text":"function takes either fitted mvgam object data.frame object produces plots observed time series, ACF, CDF histograms exploratory data analysis","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot observed time series used for mvgam modelling — plot_mvgam_series","text":"","code":"plot_mvgam_series(   object,   data,   newdata,   y = \"y\",   lines = TRUE,   series = 1,   n_bins,   log_scale = FALSE )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot observed time series used for mvgam modelling — plot_mvgam_series","text":"object Optional list object returned mvgam. Either object data must supplied. data Optional data.frame list training data containing least 'series' 'time'. Use argument training data gathered correct format mvgam modelling model yet fitted. newdata Optional data.frame list test data containing least 'series' 'time' forecast horizon, addition variables included linear predictor formula. included, observed values test data compared model's forecast distribution exploring biases model predictions. y Character. name outcome variable supplied data? Defaults 'y' lines Logical. TRUE, line plots used visualizing time series. FALSE, points used. series Either integer specifying series set plotted string '', plots series available supplied data n_bins integer specifying number bins use binning observed values plotting histogram. Default use number bins returned call hist base R log_scale logical. series == '', flag used control whether time series plot shown log scale (using log(Y + 1)). can useful visualizing many series may different observed ranges. Default FALSE","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot observed time series used for mvgam modelling — plot_mvgam_series","text":"set ggplot objects. series integer, plots show observed time series, autocorrelation cumulative distribution functions, histogram series. series == '', set observed time series plots returned series shown plot single focal series highlighted, remaining series shown faint gray lines.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_series.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot observed time series used for mvgam modelling — plot_mvgam_series","text":"Nicholas J Clark Matthijs Hollanders","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_series.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot observed time series used for mvgam modelling — plot_mvgam_series","text":"","code":"# Simulate and plot series with observations bounded at 0 and 1 (Beta responses) sim_data <- sim_mvgam(family = betar(),                      trend_model = RW(), prop_trend = 0.6) plot_mvgam_series(data = sim_data$data_train, series = 'all')  plot_mvgam_series(data = sim_data$data_train,                  newdata = sim_data$data_test, series = 1)   # Now simulate series with overdispersed discrete observations sim_data <- sim_mvgam(family = nb(), trend_model = RW(),                      prop_trend = 0.6, phi = 10) plot_mvgam_series(data = sim_data$data_train, series = 'all')"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_smooth.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot smooth terms from mvgam models — plot_mvgam_smooth","title":"Plot smooth terms from mvgam models — plot_mvgam_smooth","text":"function plots posterior empirical quantiles series-specific smooth term","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_smooth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot smooth terms from mvgam models — plot_mvgam_smooth","text":"","code":"plot_mvgam_smooth(   object,   trend_effects = FALSE,   series = 1,   smooth,   residuals = FALSE,   n_resid_bins = 25,   realisations = FALSE,   n_realisations = 15,   derivatives = FALSE,   newdata )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_smooth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot smooth terms from mvgam models — plot_mvgam_smooth","text":"object list object class mvgam. See mvgam() trend_effects logical. TRUE trend_formula used model fitting, terms trend (.e. process) model plotted series integer specifying series set plotted smooth either character integer specifying smooth term plotted residuals logical. TRUE posterior quantiles partial residuals added plots 1-D smooths series ribbon rectangles. Partial residuals smooth term median Dunn-Smyth residuals obtained dropping term concerned model, leaving estimates fixed (.e. estimates term plus original median Dunn-Smyth residuals). Note mvgam works Dunn-Smyth residuals working residuals, used mgcv, magnitudes partial residuals different expect plot.gam. Interpretation similar though, partial residuals evenly scattered around smooth function function well estimated n_resid_bins integer specifying number bins group covariate plotting partial residuals. Setting argument high can make messy plots difficult interpret, setting low likely mask potentially useful patterns partial residuals. Default 25 realisations logical. TRUE, posterior realisations shown spaghetti plot, making easier visualise diversity possible functions. FALSE, default, empirical quantiles posterior distribution shown n_realisations integer specifying number posterior realisations plot, realisations = TRUE. Ignored otherwise derivatives logical. TRUE, additional plot returned show estimated 1st derivative specified smooth (Note, works univariate smooths) newdata Optional dataframe predicting smooth, containing least 'series' addition variables included linear predictor original model's formula. Note currently supported plotting univariate smooths","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_smooth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot smooth terms from mvgam models — plot_mvgam_smooth","text":"base R graphics plot","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_smooth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot smooth terms from mvgam models — plot_mvgam_smooth","text":"Smooth functions shown empirical quantiles (spaghetti plots) posterior partial expectations across sequence values variable's min max, zeroing effects variables. present, univariate bivariate smooth plots allowed, though note bivariate smooths rely default behaviour plot.gam. plot_mvgam_smooth generates posterior predictions object class mvgam, calculates posterior empirical quantiles plots . realisations = FALSE, returned plot shows 90, 60, 40 20 percent posterior quantiles (ribbons increasingly darker shades red) well posterior median (dark red line). realisations = FALSE, set n_realisations posterior draws shown. nuanced visualisation, supply newdata just predicting gam model use flexible conditional_effects.mvgam. Alternatively, prefer use partial effect plots style gratia, gratia package installed, can use draw.mvgam. See gratia_mvgam_enhancements details.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_smooth.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot smooth terms from mvgam models — plot_mvgam_smooth","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_trend.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot latent trend predictions from mvgam models — plot_mvgam_trend","title":"Plot latent trend predictions from mvgam models — plot_mvgam_trend","text":"Plot latent trend predictions mvgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_trend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot latent trend predictions from mvgam models — plot_mvgam_trend","text":"","code":"plot_mvgam_trend(   object,   series = 1,   newdata,   data_test,   realisations = FALSE,   n_realisations = 15,   n_cores = 1,   derivatives = FALSE,   xlab,   ylab )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_trend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot latent trend predictions from mvgam models — plot_mvgam_trend","text":"object list object returned mvgam. See mvgam() series integer specifying series set plotted newdata Optional dataframe list test data containing least 'series' 'time' addition variables included linear predictor original formula. data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows realisations logical. TRUE, posterior trend realisations shown spaghetti plot, making easier visualise diversity possible trend paths. FALSE, default, empirical quantiles posterior distribution shown n_realisations integer specifying number posterior realisations plot, realisations = TRUE. Ignored otherwise n_cores Deprecated. Parallel processing longer supported derivatives logical. TRUE, additional plot returned show estimated 1st derivative estimated trend xlab label x axis. ylab label y axis.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_trend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot latent trend predictions from mvgam models — plot_mvgam_trend","text":"ggplot object","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_trend.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot latent trend predictions from mvgam models — plot_mvgam_trend","text":"","code":"# \\donttest{ simdat <- sim_mvgam(n_series = 3, trend_model = 'AR1') mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),             trend_model = AR(),             noncentred = TRUE,             data = simdat$data_train,             chains = 2) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 0.9 seconds. #> Chain 2 finished in 0.8 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 0.9 seconds. #> Total execution time: 1.0 seconds. #>   # Plot estimated trends for some series plot_mvgam_trend(mod)  plot_mvgam_trend(mod, series = 2)   # Extrapolate trends forward in time and plot on response scale plot_mvgam_trend(mod, newdata = simdat$data_test)  plot_mvgam_trend(mod, newdata = simdat$data_test, series = 2)   # But it is recommended to compute extrapolations for all series # first and then plot trend_fc <- forecast(mod, newdata = simdat$data_test) plot(trend_fc, series = 1) #> Out of sample DRPS: #> 17.939343  plot(trend_fc, series = 2) #> Out of sample DRPS: #> 23.675087  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_uncertainty.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot forecast uncertainty contributions from mvgam models — plot_mvgam_uncertainty","title":"Plot forecast uncertainty contributions from mvgam models — plot_mvgam_uncertainty","text":"Plot forecast uncertainty contributions mvgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_uncertainty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot forecast uncertainty contributions from mvgam models — plot_mvgam_uncertainty","text":"","code":"plot_mvgam_uncertainty(   object,   series = 1,   newdata,   data_test,   legend_position = \"topleft\",   hide_xlabels = FALSE )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_uncertainty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot forecast uncertainty contributions from mvgam models — plot_mvgam_uncertainty","text":"object list object returned mvgam. See mvgam() series integer specifying series set plotted newdata dataframe list containing least 'series' 'time' forecast horizon, addition variables included linear predictor formula data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows legend_position location may also specified setting x single keyword list: \"none\", \"bottomright\", \"bottom\", \"bottomleft\", \"left\", \"topleft\", \"top\", \"topright\", \"right\" \"center\". places legend inside plot frame given location (\"none\"). hide_xlabels logical. TRUE, xlabels printed allow user add custom labels using axis base R","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_uncertainty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot forecast uncertainty contributions from mvgam models — plot_mvgam_uncertainty","text":"base R graphics plot","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/plot_mvgam_uncertainty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot forecast uncertainty contributions from mvgam models — plot_mvgam_uncertainty","text":"basic idea function compute forecasts ignoring one two primary components correlated residual model (.e. either ignoring linear predictor effects ignoring residual dynamics). caution required however, function designed early mvgam development cycle now many types models handle well. example, models shared latent states, type State-Space models include terms trend_formula, either fail give nonsensical results. Improvements works provide general way decompose forecast uncertainties, please check back later date.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/portal_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Portal Project rodent capture survey data — portal_data","title":"Portal Project rodent capture survey data — portal_data","text":"dataset containing time series total captures (across control plots) select rodent species Portal Project","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/portal_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Portal Project rodent capture survey data — portal_data","text":"","code":"portal_data"},{"path":"https://nicholasjclark.github.io/mvgam/reference/portal_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Portal Project rodent capture survey data — portal_data","text":"data.frame containing following fields: time time sampling, lunar monthly cycles series factor indicator time series, .e. species captures total captures across control plots time point ndvi_ma12 12-month moving average mean Normalised Difference Vegetation Index mintemp monthly mean minimum temperature","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/portal_data.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Portal Project rodent capture survey data — portal_data","text":"https://github.com/weecology/PortalData/blob/main/SiteandMethods/Methods.md","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_epred.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Draws from the expected value of the posterior predictive distribution for mvgam objects — posterior_epred.mvgam","title":"Draws from the expected value of the posterior predictive distribution for mvgam objects — posterior_epred.mvgam","text":"Compute posterior draws expected value posterior predictive distribution (.e. conditional expectation). Can performed data used fit model (posterior predictive checks) new data. definition, predictions smaller variance posterior predictions performed posterior_predict.mvgam method. uncertainty expected value posterior predictive distribution incorporated draws computed posterior_epred residual error ignored . However, estimated means methods averaged across draws similar.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_epred.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draws from the expected value of the posterior predictive distribution for mvgam objects — posterior_epred.mvgam","text":"","code":"# S3 method for mvgam posterior_epred(   object,   newdata,   data_test,   ndraws = NULL,   process_error = TRUE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_epred.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draws from the expected value of the posterior predictive distribution for mvgam objects — posterior_epred.mvgam","text":"object list object class mvgam jsdgam. See mvgam() newdata Optional dataframe list test data containing variables included original data used fit model. supplied, predictions generated original observations used model fit. data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows ndraws Positive integer indicating many posterior draws used. NULL (default) draws used. process_error Logical. TRUE newdata supplied, expected uncertainty process model accounted using draws latent trend SD parameters. FALSE, uncertainty latent trend component ignored calculating predictions. newdata supplied, draws fitted model's posterior predictive distribution used (always include uncertainty latent trend components) ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_epred.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draws from the expected value of the posterior predictive distribution for mvgam objects — posterior_epred.mvgam","text":"matrix dimension n_samples x new_obs, n_samples number posterior samples fitted object n_obs number observations newdata","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_epred.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Draws from the expected value of the posterior predictive distribution for mvgam objects — posterior_epred.mvgam","text":"Note types predictions models include trend_formula, uncertainty dynamic trend component can ignored setting process_error = FALSE. However, trend_formula supplied model, predictions component ignored. process_error = TRUE, trend predictions ignore autocorrelation coefficients GP length scale coefficients, ultimately assuming process stationary. method similar types posterior predictions returned brms models using autocorrelated error predictions newdata. function therefore suited posterior simulation GAM components mvgam model, forecasting functions plot_mvgam_fc forecast.mvgam better suited generate h-step ahead forecasts respect temporal dynamics estimated latent trends.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_epred.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Draws from the expected value of the posterior predictive distribution for mvgam objects — posterior_epred.mvgam","text":"","code":"# \\donttest{ # Simulate some data and fit a model simdat <- sim_mvgam(n_series = 1, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc'),             trend_model = AR(),             noncentred = TRUE,             data = simdat$data_train,             chains = 2,             silent = 2)  # Compute posterior expectations expectations <- posterior_epred(mod) str(expectations) #>  num [1:1000, 1:75] 1.99 1.12 3.3 1.46 3.25 ... # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_linpred.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior draws of the linear predictor for mvgam objects — posterior_linpred.mvgam","title":"Posterior draws of the linear predictor for mvgam objects — posterior_linpred.mvgam","text":"Compute posterior draws linear predictor, draws applying link functions transformations. Can performed data used fit model (posterior predictive checks) new data.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_linpred.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior draws of the linear predictor for mvgam objects — posterior_linpred.mvgam","text":"","code":"# S3 method for mvgam posterior_linpred(   object,   transform = FALSE,   newdata,   ndraws = NULL,   data_test,   process_error = TRUE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_linpred.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior draws of the linear predictor for mvgam objects — posterior_linpred.mvgam","text":"object list object class mvgam jsdgam. See mvgam() transform Logical; FALSE (default), draws linear predictor returned. TRUE, draws transformed linear predictor, .e. conditional expectation, returned. newdata Optional dataframe list test data containing variables included original data used fit model. supplied, predictions generated original observations used model fit. ndraws Positive integer indicating many posterior draws used. NULL (default) draws used. data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows process_error Logical. TRUE newdata supplied, expected uncertainty process model accounted using draws latent trend SD parameters. FALSE, uncertainty latent trend component ignored calculating predictions. newdata supplied, draws fitted model's posterior predictive distribution used (always include uncertainty latent trend components) ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_linpred.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior draws of the linear predictor for mvgam objects — posterior_linpred.mvgam","text":"matrix dimension n_samples x new_obs, n_samples number posterior samples fitted object n_obs number observations newdata","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_linpred.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Posterior draws of the linear predictor for mvgam objects — posterior_linpred.mvgam","text":"Note types predictions models include trend_formula, uncertainty dynamic trend component can ignored setting process_error = FALSE. However, trend_formula supplied model, predictions component ignored. process_error = TRUE, trend predictions ignore autocorrelation coefficients GP length scale coefficients, ultimately assuming process stationary. method similar types posterior predictions returned brms models using autocorrelated error predictions newdata. function therefore suited posterior simulation GAM components mvgam model, forecasting functions plot_mvgam_fc forecast.mvgam better suited generate h-step ahead forecasts respect temporal dynamics estimated latent trends.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_linpred.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posterior draws of the linear predictor for mvgam objects — posterior_linpred.mvgam","text":"","code":"# \\donttest{ # Simulate some data and fit a model simdat <- sim_mvgam(n_series = 1, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc'),              trend_model = AR(),              noncentred = TRUE,              data = simdat$data_train,              chains = 2,              silent = 2)  # Extract linear predictor values linpreds <- posterior_linpred(mod) str(linpreds) #>  num [1:1000, 1:75] -0.3269 0.0167 -0.0897 -0.1996 0.0402 ... # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_predict.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Draws from the posterior predictive distribution for mvgam objects — posterior_predict.mvgam","title":"Draws from the posterior predictive distribution for mvgam objects — posterior_predict.mvgam","text":"Compute posterior draws posterior predictive distribution. Can performed data used fit model (posterior predictive checks) new data. definition, draws higher variance draws expected value posterior predictive distribution computed posterior_epred.mvgam. residual error incorporated posterior_predict. However, estimated means methods averaged across draws similar.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_predict.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draws from the posterior predictive distribution for mvgam objects — posterior_predict.mvgam","text":"","code":"# S3 method for mvgam posterior_predict(   object,   newdata,   data_test,   ndraws = NULL,   process_error = TRUE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_predict.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draws from the posterior predictive distribution for mvgam objects — posterior_predict.mvgam","text":"object list object class mvgam jsdgam. See mvgam() newdata Optional dataframe list test data containing variables included original data used fit model. supplied, predictions generated original observations used model fit. data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows ndraws Positive integer indicating many posterior draws used. NULL (default) draws used. process_error Logical. TRUE newdata supplied, expected uncertainty process model accounted using draws latent trend SD parameters. FALSE, uncertainty latent trend component ignored calculating predictions. newdata supplied, draws fitted model's posterior predictive distribution used (always include uncertainty latent trend components) ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_predict.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draws from the posterior predictive distribution for mvgam objects — posterior_predict.mvgam","text":"matrix dimension n_samples x new_obs, n_samples number posterior samples fitted object n_obs number observations newdata","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_predict.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Draws from the posterior predictive distribution for mvgam objects — posterior_predict.mvgam","text":"Note types predictions models include trend_formula, uncertainty dynamic trend component can ignored setting process_error = FALSE. However, trend_formula supplied model, predictions component ignored. process_error = TRUE, trend predictions ignore autocorrelation coefficients GP length scale coefficients, ultimately assuming process stationary. method similar types posterior predictions returned brms models using autocorrelated error predictions newdata. function therefore suited posterior simulation GAM components mvgam model, forecasting functions plot_mvgam_fc forecast.mvgam better suited generate h-step ahead forecasts respect temporal dynamics estimated latent trends.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/posterior_predict.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Draws from the posterior predictive distribution for mvgam objects — posterior_predict.mvgam","text":"","code":"# \\donttest{ # Simulate some data and fit a model simdat <- sim_mvgam(n_series = 1, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc'),             trend_model = AR(),             data = simdat$data_train,             chains = 2,             silent = 2)  # Compute posterior predictions predictions <- posterior_predict(mod) str(predictions) #>  int [1:1000, 1:75] 1 0 0 0 0 0 1 0 0 0 ... # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/pp_check.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior Predictive Checks for mvgam models — pp_check.mvgam","title":"Posterior Predictive Checks for mvgam models — pp_check.mvgam","text":"Perform unconditional posterior predictive checks help bayesplot package.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pp_check.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior Predictive Checks for mvgam models — pp_check.mvgam","text":"","code":"# S3 method for mvgam pp_check(   object,   type,   ndraws = NULL,   prefix = c(\"ppc\", \"ppd\"),   group = NULL,   x = NULL,   newdata = NULL,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/pp_check.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior Predictive Checks for mvgam models — pp_check.mvgam","text":"object object class mvgam. type Type ppc plot given character string. See PPC overview currently supported types. may also use invalid type (e.g. type = \"xyz\") get list supported types resulting error message. ndraws Positive integer indicating many posterior draws used. NULL draws used. specified, number posterior draws chosen automatically. Ignored draw_ids NULL. prefix prefix bayesplot function applied. Either `\"ppc\"` (posterior predictive check; default) `\"ppd\"` (posterior predictive distribution), latter former except observed data shown `\"ppd\"`. group Optional name factor variable model stratify ppc plot. argument required ppc *_grouped types ignored otherwise. x Optional name variable model. used ppc types x argument ignored otherwise. newdata Optional dataframe list test data containing variables included linear predictor formula. supplied, predictions generated original observations used model fit. Ignored using one residual plots (.e. 'resid_hist') ... arguments passed predict.mvgam well PPC function specified type.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pp_check.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior Predictive Checks for mvgam models — pp_check.mvgam","text":"ggplot object can customized using ggplot2 package.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pp_check.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Posterior Predictive Checks for mvgam models — pp_check.mvgam","text":"Unlike conditional posterior checks provided ppc, function computes unconditional posterior predictive checks (.e. generates predictions fake data without considering true observations associated fake data). detailed explanation ppc functions, see PPC documentation bayesplot package.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/pp_check.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Posterior Predictive Checks for mvgam models — pp_check.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/pp_check.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posterior Predictive Checks for mvgam models — pp_check.mvgam","text":"","code":"# \\donttest{ simdat <- sim_mvgam(seasonality = \"hierarchical\") mod <- mvgam(   y ~ series +     s(season, bs = \"cc\", k = 6) +     s(season, series, bs = \"fs\", k = 4),   data = simdat$data_train,   chains = 2,   silent = 2 ) #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable. #> Warning: model has repeated 1-d smooths of same variable.  # Use pp_check(mod, type = \"xyz\") for a list of available plot types  # Default is a density overlay for all observations pp_check(mod) #> Using 10 posterior draws for ppc type 'dens_overlay' by default.   # Rootograms particularly useful for count data pp_check(mod, type = \"rootogram\") #> Using all posterior draws for ppc type 'rootogram' by default.   # Grouping plots by series is useful pp_check(mod,   type = \"bars_grouped\",   group = \"series\", ndraws = 50 )  pp_check(mod,   type = \"ecdf_overlay_grouped\",   group = \"series\", ndraws = 50 )  pp_check(mod,   type = \"stat_freqpoly_grouped\",   group = \"series\", ndraws = 50 ) #> Note: in most cases the default test statistic 'mean' is too weak to detect anything of interest. #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # Several types can be used to plot distributions of randomized # quantile residuals pp_check(   object = mod,   x = \"season\",   type = \"resid_ribbon\" ) #> Using 10 posterior draws for ppc type 'resid_ribbon' by default.  pp_check(   object = mod,   x = \"season\",   group = \"series\",   type = \"resid_ribbon_grouped\" ) #> Using 10 posterior draws for ppc type 'resid_ribbon_grouped' by default.  pp_check(mod,   ndraws = 5,   type = \"resid_hist_grouped\",   group = \"series\" ) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # Custom functions accepted pp_check(mod, type = \"stat\", stat = function(x) mean(x == 0)) #> Using all posterior draws for ppc type 'stat' by default. #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  pp_check(mod,   type = \"stat_grouped\",   stat = function(x) mean(x == 0),   group = \"series\" ) #> Using all posterior draws for ppc type 'stat_grouped' by default. #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   # Some functions accept covariates to set the x-axes pp_check(mod,   x = \"season\",   type = \"ribbon_grouped\",   prob = 0.5,   prob_outer = 0.8,   group = \"series\" ) #> Using all posterior draws for ppc type 'ribbon_grouped' by default.   # Many plots can be made without the observed data pp_check(mod, prefix = \"ppd\") #> Using 10 posterior draws for ppc type 'dens_overlay' by default.  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/ppc.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot conditional posterior predictive checks from mvgam models — ppc.mvgam","title":"Plot conditional posterior predictive checks from mvgam models — ppc.mvgam","text":"Plot conditional posterior predictive checks mvgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ppc.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot conditional posterior predictive checks from mvgam models — ppc.mvgam","text":"","code":"ppc(object, ...)  # S3 method for mvgam ppc(   object,   newdata,   data_test,   series = 1,   type = \"hist\",   n_bins,   legend_position,   xlab,   ylab,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/ppc.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot conditional posterior predictive checks from mvgam models — ppc.mvgam","text":"object list object returned mvgam. See mvgam() ... par graphical parameters. newdata Optional dataframe list test data containing least 'series' 'time' forecast horizon, addition variables included linear predictor formula. included, observed values test data compared model's forecast distribution exploring biases model predictions. Note useful newdata also included fitting original model. data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows series integer specifying series set plotted type character specifying type posterior predictive check calculate plot. Valid options : 'rootogram', 'mean', 'hist', 'density', 'prop_zero', 'pit' 'cdf' n_bins integer specifying number bins use binning observed values plotting rootogram histogram. Default 50 bins rootogram, means >50 unique observed values, bins used prevent overplotting facilitate interpretation. Default histogram use number bins returned call hist base R legend_position location may also specified setting x single keyword list \"bottomright\", \"bottom\", \"bottomleft\", \"left\", \"topleft\", \"top\", \"topright\", \"right\" \"center\". places legend inside plot frame given location. alternatively, use \"none\" hide legend. xlab label x axis. ylab label y axis.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ppc.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot conditional posterior predictive checks from mvgam models — ppc.mvgam","text":"base R graphics plot showing either posterior rootogram (type == 'rootogram'), predicted vs observed mean series (type == 'mean'), predicted vs observed proportion zeroes series (type == 'prop_zero'),predicted vs observed histogram series (type == 'hist'), kernel density empirical CDF estimates posterior predictions (type == 'density' type == 'cdf') Probability Integral Transform histogram (type == 'pit').","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ppc.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot conditional posterior predictive checks from mvgam models — ppc.mvgam","text":"Conditional posterior predictions drawn fitted mvgam compared empirical distribution observed data specified series help evaluate model's ability generate unbiased predictions. plots apart type = 'rootogram', posterior predictions can also compared sample observations long observations included ' data_test' original model fit supplied . Rootograms currently plotted using ' hanging' style.  Note predictions used plots conditional observed data, .e. predictions generated directly within mvgam() model. can misleading model included flexible dynamic trend components. broader range posterior checks created using unconditional \"new data\" predictions, see pp_check.mvgam","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/ppc.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot conditional posterior predictive checks from mvgam models — ppc.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/ppc.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot conditional posterior predictive checks from mvgam models — ppc.mvgam","text":"","code":"# \\donttest{ # Simulate some smooth effects and fit a model set.seed(0) dat <- mgcv::gamSim(1, n = 200, scale = 2) #> Gu & Wahba 4 term additive model mod <- mvgam(y ~ s(x0) + s(x1) + s(x2) + s(x3),   data = dat,   family = gaussian(),   chains = 2,   silent = 2 )  # Posterior checks ppc(mod, type = \"hist\")  ppc(mod, type = \"density\")  ppc(mod, type = \"cdf\")   # Many more options are available with pp_check() pp_check(mod) #> Using 10 posterior draws for ppc type 'dens_overlay' by default.  pp_check(mod, type = \"ecdf_overlay\") #> Using 10 posterior draws for ppc type 'ecdf_overlay' by default.  pp_check(mod, type = \"freqpoly\") #> Using 10 posterior draws for ppc type 'freqpoly' by default. #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/predict.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict from a fitted mvgam model — predict.mvgam","title":"Predict from a fitted mvgam model — predict.mvgam","text":"Predict fitted mvgam model","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/predict.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict from a fitted mvgam model — predict.mvgam","text":"","code":"# S3 method for mvgam predict(   object,   newdata,   data_test,   type = \"link\",   process_error = FALSE,   summary = TRUE,   robust = FALSE,   probs = c(0.025, 0.975),   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/predict.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict from a fitted mvgam model — predict.mvgam","text":"object list object class mvgam jsdgam. See mvgam() newdata Optional dataframe list test data containing variables included original data used fit model. supplied, predictions generated original observations used model fit. data_test Deprecated. Still works place newdata users recommended use newdata instead seamless integration R workflows type value link (default) linear predictor calculated link scale. expected used, predictions reflect expectation response (mean) ignore uncertainty observation process. response used, predictions take uncertainty observation process account return predictions outcome scale. variance used, variance response respect mean (mean-variance relationship) returned. type = \"terms\", component linear predictor returned separately form list (possibly standard errors, summary = TRUE): includes parametric model components, followed smooth component, excludes offset intercept. Two special cases also allowed: type latent_N return estimated latent abundances N-mixture distribution, type detection return estimated detection probability N-mixture distribution process_error Logical. TRUE dynamic trend model fit, expected uncertainty process model accounted using draws latent trend SD parameters. FALSE, uncertainty latent trend component ignored calculating predictions summary summary statistics returned instead raw values? Default TRUE.. robust FALSE (default) mean used measure central tendency standard deviation measure variability. TRUE, median median absolute deviation (MAD) applied instead. used summary TRUE. probs percentiles computed quantile function. used summary TRUE. ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/predict.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict from a fitted mvgam model — predict.mvgam","text":"Predicted values appropriate scale. summary = FALSE type != \"terms\", output matrix dimension n_draw x n_observations containing predicted values posterior draw object. summary = TRUE type != \"terms\", output n_observations x E matrix. number summary statistics E equal 2 +   length(probs): Estimate column contains point estimates (either mean median depending argument robust), Est.Error column contains uncertainty estimates (either standard deviation median absolute deviation depending argument robust). remaining columns starting Q contain quantile estimates specified via argument probs. type = \"terms\" summary = FALSE, output named list containing separate slot effect, effects returned matrices dimension n_draw x 1. summary = TRUE, output resembles predict.gam using call predict.gam(object, type = \"terms\", se.fit = TRUE), mean contributions effect returned matrix form standard errors (representing interval: (max(probs) - min(probs)) / 2) returned separate matrix","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/predict.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict from a fitted mvgam model — predict.mvgam","text":"Note types predictions models include trend_formula, uncertainty dynamic trend component can ignored setting process_error = FALSE. However, trend_formula supplied model, predictions component ignored. process_error = TRUE, trend predictions ignore autocorrelation coefficients GP length scale coefficients, ultimately assuming process stationary. method similar types posterior predictions returned brms models using autocorrelated error predictions newdata. function therefore suited posterior simulation GAM components mvgam model, forecasting functions plot_mvgam_fc forecast.mvgam better suited generate h-step ahead forecasts respect temporal dynamics estimated latent trends.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/predict.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict from a fitted mvgam model — predict.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/predict.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict from a fitted mvgam model — predict.mvgam","text":"","code":"# \\donttest{ # Simulate 4 time series with hierarchical seasonality # and independent AR1 dynamic processes set.seed(111) simdat <- sim_mvgam(seasonality = 'hierarchical',                    trend_model = 'AR1',                    family = gaussian())  # Fit a model with shared seasonality mod1 <- mvgam(y ~ s(season, bs = 'cc', k = 6),              data = simdat$data_train,              family = gaussian(),              trend_model = AR(),              noncentred = TRUE,              chains = 2,              silent = 2)  # Generate predictions against observed data preds <- predict(mod1,                 summary = TRUE) head(preds) #>          Estimate Est.Error       Q2.5       Q97.5 #> [1,] -0.008915377 0.1113811 -0.2359634  0.19933611 #> [2,] -0.008915377 0.1113811 -0.2359634  0.19933611 #> [3,] -0.008915377 0.1113811 -0.2359634  0.19933611 #> [4,] -0.224003798 0.1024206 -0.4350552 -0.03151531 #> [5,] -0.224003798 0.1024206 -0.4350552 -0.03151531 #> [6,] -0.224003798 0.1024206 -0.4350552 -0.03151531  # Generate predictions against test data preds <- predict(mod1,                 newdata = simdat$data_test,                 summary = TRUE) head(preds) #>        Estimate Est.Error        Q2.5      Q97.5 #> [1,] -0.1716277 0.1117367 -0.38581602 0.04014399 #> [2,] -0.1716277 0.1117367 -0.38581602 0.04014399 #> [3,] -0.1716277 0.1117367 -0.38581602 0.04014399 #> [4,]  0.2923566 0.1278500  0.03420638 0.53215879 #> [5,]  0.2923566 0.1278500  0.03420638 0.53215879 #> [6,]  0.2923566 0.1278500  0.03420638 0.53215879 # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/print.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary for a fitted mvgam object — print.mvgam","title":"Summary for a fitted mvgam object — print.mvgam","text":"function takes fitted mvgam jsdgam object prints quick summary","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/print.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary for a fitted mvgam object — print.mvgam","text":"","code":"# S3 method for mvgam print(x, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/print.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary for a fitted mvgam object — print.mvgam","text":"x list object returned mvgam ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/print.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary for a fitted mvgam object — print.mvgam","text":"list printed -screen","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/print.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summary for a fitted mvgam object — print.mvgam","text":"brief summary model's call printed","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/print.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summary for a fitted mvgam object — print.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. brms conditional_effects, gp, mcmc_plot, prior, prior_, prior_string, set_prior, stancode, standata generics augment, forecast, tidy insight get_data loo loo, loo_compare marginaleffects avg_predictions, comparisons, datagrid, get_predict, hypotheses, plot_comparisons, plot_predictions, plot_slopes, predictions, slopes mgcv s, t2, te, ti posterior as_draws, as_draws_array, as_draws_df, as_draws_list, as_draws_matrix, as_draws_rvars rstantools posterior_epred, posterior_linpred, posterior_predict","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/residual_cor.jsdgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract residual correlations based on latent factors — residual_cor.jsdgam","title":"Extract residual correlations based on latent factors — residual_cor.jsdgam","text":"Compute residual correlation estimates Joint Species Distribution (jsdgam) mvgam models either used latent factors included correlated process errors directly","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/residual_cor.jsdgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract residual correlations based on latent factors — residual_cor.jsdgam","text":"","code":"residual_cor(object, ...)  # S3 method for mvgam residual_cor(   object,   summary = TRUE,   robust = FALSE,   probs = c(0.025, 0.975),   ... )  # S3 method for jsdgam residual_cor(   object,   summary = TRUE,   robust = FALSE,   probs = c(0.025, 0.975),   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/residual_cor.jsdgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract residual correlations based on latent factors — residual_cor.jsdgam","text":"object list object class mvgam resulting call jsdgam() call mvgam() either use_lv = TRUE multivariate process used cor = TRUE (see RW() VAR() examples) ... ignored summary summary statistics returned instead raw values? Default TRUE.. robust FALSE (default) mean used measure central tendency. TRUE, median used instead. used summary TRUE probs percentiles computed quantile function. used summary TRUE.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/residual_cor.jsdgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract residual correlations based on latent factors — residual_cor.jsdgam","text":"summary = TRUE, list  mvgam_residcor-class following components: cor, cor_lower, cor_upper set \\(p \\times p\\) correlation matrices, containing either posterior median mean estimate, plus lower upper limits corresponding credible intervals supplied probs sig_cor \\(p \\times p\\) correlation matrix containing correlations whose credible interval contain zero. correlations set zero prec, prec_lower, prec_upper set \\(p \\times p\\) precision matrices, containing either posterior median mean estimate, plus lower upper limits corresponding credible intervals supplied probs sig_prec \\(p \\times p\\) precision matrix containing precisions whose credible interval contain zero. precisions set zero cov \\(p \\times p\\) posterior median mean covariance matrix trace median/mean point estimator trace (sum diagonal elements) residual covariance matrix cov summary = FALSE, function returns list containing following components: all_cormat \\(n_{draws} \\times p \\times p\\) array posterior residual correlation matrix draws all_covmat \\(n_{draws} \\times p \\times p\\) array posterior residual covariance matrix draws all_presmat \\(n_{draws} \\times p \\times p\\) array posterior residual precision matrix draws all_trace \\(n_{draws}\\) vector posterior covariance trace draws","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/residual_cor.jsdgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract residual correlations based on latent factors — residual_cor.jsdgam","text":"See mvgam_residcor-class full description quantities computed returned function, along key references.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/residual_cor.jsdgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract residual correlations based on latent factors — residual_cor.jsdgam","text":"","code":"# \\donttest{ # Fit a JSDGAM to the portal_data captures mod <- jsdgam(   formula = captures ~     # Fixed effects of NDVI and mintemp, row effect as a GP of time     ndvi_ma12:series + mintemp:series + gp(time, k = 15),   factor_formula = ~ -1,   data = portal_data,   unit = time,   species = series,   family = poisson(),   n_lv = 2,   silent = 2,   chains = 2 )  # Plot residual correlations plot(   residual_cor(mod) )   # Compare to a residual ordination biplot if(requireNamespace('ggrepel', quietly = TRUE)){   ordinate(mod) } #> Warning: ggrepel: 18 unlabeled data points (too many overlaps). Consider increasing max.overlaps  # \\dontshow{ # For R CMD check: make sure any open connections are closed afterward  closeAllConnections() # } # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/residuals.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior draws of residuals from mvgam models — residuals.mvgam","title":"Posterior draws of residuals from mvgam models — residuals.mvgam","text":"method extracts posterior draws Dunn-Smyth (randomized quantile) residuals order data supplied model. included additional arguments obtaining summaries computed residuals","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/residuals.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior draws of residuals from mvgam models — residuals.mvgam","text":"","code":"# S3 method for mvgam residuals(object, summary = TRUE, robust = FALSE, probs = c(0.025, 0.975), ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/residuals.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior draws of residuals from mvgam models — residuals.mvgam","text":"object object class mvgam summary summary statistics returned instead raw values? Default TRUE.. robust FALSE (default) mean used measure central tendency standard deviation measure variability. TRUE, median median absolute deviation (MAD) applied instead. used summary TRUE. probs percentiles computed quantile function. used summary TRUE. ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/residuals.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior draws of residuals from mvgam models — residuals.mvgam","text":"array randomized quantile residual values. summary = FALSE output resembles posterior_epred.mvgam predict.mvgam. summary = TRUE output n_observations x E matrix. number summary statistics E equal 2 +   length(probs): Estimate column contains point estimates (either mean median depending argument robust), Est.Error column contains uncertainty estimates (either standard deviation median absolute deviation depending argument robust). remaining columns starting Q contain quantile estimates specified via argument probs.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/residuals.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Posterior draws of residuals from mvgam models — residuals.mvgam","text":"method gives residuals Dunn-Smyth (randomized quantile) residuals. observations missing (.e. NA) original data missing values residuals","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/residuals.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Posterior draws of residuals from mvgam models — residuals.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/residuals.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posterior draws of residuals from mvgam models — residuals.mvgam","text":"","code":"# \\donttest{ # Simulate some data and fit a model simdat <- sim_mvgam(n_series = 1, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc'),              trend_model = AR(),              noncentred = TRUE,              data = simdat$data_train,              chains = 2,              silent = 2)  # Extract posterior residuals resids <- residuals(mod) str(resids) #>  num [1:75, 1:4] 0.7409 0.5358 0.5017 0.0956 0.5376 ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : NULL #>   ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"  # Or add them directly to the observed data, along with fitted values augment(mod, robust = FALSE, probs = c(0.25, 0.75)) #> # A tibble: 75 × 14 #>        y season  year series    time .observed .fitted .fit.variability #>    <int>  <int> <int> <fct>    <int>     <int>   <dbl>            <dbl> #>  1     2      1     1 series_1     1         2   1.26             0.426 #>  2     2      2     1 series_1     2         2   1.50             0.483 #>  3     2      3     1 series_1     3         2   1.53             0.515 #>  4     1      4     1 series_1     4         1   1.09             0.404 #>  5     1      5     1 series_1     5         1   0.662            0.280 #>  6     0      6     1 series_1     6         0   0.360            0.192 #>  7     0      7     1 series_1     7         0   0.240            0.127 #>  8     0      8     1 series_1     8         0   0.243            0.128 #>  9     1      9     1 series_1     9         1   0.410            0.183 #> 10     0     10     1 series_1    10         0   0.676            0.282 #> # ℹ 65 more rows #> # ℹ 6 more variables: .fit.cred.low <dbl>, .fit.cred.high <dbl>, .resid <dbl>, #> #   .resid.variability <dbl>, .resid.cred.low <dbl>, .resid.cred.high <dbl> # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/score.mvgam_forecast.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute probabilistic forecast scores for mvgam models — score.mvgam_forecast","title":"Compute probabilistic forecast scores for mvgam models — score.mvgam_forecast","text":"Compute probabilistic forecast scores mvgam models","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/score.mvgam_forecast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute probabilistic forecast scores for mvgam models — score.mvgam_forecast","text":"","code":"# S3 method for mvgam_forecast score(   object,   score = \"crps\",   log = FALSE,   weights,   interval_width = 0.9,   n_cores = 1,   ... )  score(object, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/score.mvgam_forecast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute probabilistic forecast scores for mvgam models — score.mvgam_forecast","text":"object mvgam_forecast object. See forecast.mvgam(). score character specifying type proper scoring rule use evaluation. Options : sis (.e. Scaled Interval Score), energy, variogram, elpd (.e. Expected log pointwise Predictive Density), drps (.e. Discrete Rank Probability Score), crps (Continuous Rank Probability Score) brier (latter applicable bernoulli models. Note choosing elpd, supplied object must forecasts link scale expectations can calculated prior scoring. choosing brier, object must forecasts expected scale (.e. probability predictions). scores, forecasts supplied response scale (.e. posterior predictions) log logical. forecasts truths logged prior scoring? often appropriate comparing performance models series vary observation ranges. Ignored score = 'brier' weights optional vector weights (length(weights) == n_series) weighting pairwise correlations evaluating variogram score multivariate forecasts. Useful -weighting series larger magnitude observations less interest forecasting. Ignored score != 'variogram' interval_width proportional value [0.05,0.95] defining forecast interval calculating coverage , score = 'sis', calculating interval score. Ignored score = 'brier' n_cores integer specifying number cores calculating scores parallel ... Ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/score.mvgam_forecast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute probabilistic forecast scores for mvgam models — score.mvgam_forecast","text":"list containing scores interval coverages per forecast horizon. score %% c('drps', 'crps', 'elpd', 'brier'), list also contain return sum series-level scores per horizon. score %% c('energy','variogram'), series-level scores computed score returned series. scores apart elpd brier, in_interval column series-level slot binary indicator whether true value within forecast's corresponding posterior empirical quantiles. Intervals calculated using elpd forecasts contain linear predictors","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/score.mvgam_forecast.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute probabilistic forecast scores for mvgam models — score.mvgam_forecast","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/score.mvgam_forecast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute probabilistic forecast scores for mvgam models — score.mvgam_forecast","text":"","code":"# \\donttest{ # Simulate observations for three count-valued time series data <- sim_mvgam() # Fit a dynamic model using 'newdata' to automatically produce forecasts mod <- mvgam(y ~ 1,             trend_model = RW(),             data = data$data_train,             newdata = data$data_test,             chains = 2,             silent = 2)  # Extract forecasts into a 'mvgam_forecast' object fc <- forecast(mod) plot(fc) #> Out of sample DRPS: #> 16.813595   # Compute Discrete Rank Probability Scores and 0.90 interval coverages fc_scores <- score(fc, score = 'drps') str(fc_scores) #> List of 4 #>  $ series_1  :'data.frame':\t25 obs. of  5 variables: #>   ..$ score         : num [1:25] 0.237 1.007 0.232 3.908 1.889 ... #>   ..$ in_interval   : num [1:25] 1 1 1 0 0 0 1 1 1 1 ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"drps\" \"drps\" \"drps\" \"drps\" ... #>  $ series_2  :'data.frame':\t25 obs. of  5 variables: #>   ..$ score         : num [1:25] 0.862 0.388 0.83 2.947 2.907 ... #>   ..$ in_interval   : num [1:25] 1 1 1 1 1 1 1 1 1 1 ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"drps\" \"drps\" \"drps\" \"drps\" ... #>  $ series_3  :'data.frame':\t25 obs. of  5 variables: #>   ..$ score         : num [1:25] 2.077 0.354 1.953 0.214 0.349 ... #>   ..$ in_interval   : num [1:25] 0 1 0 1 1 0 1 1 1 1 ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"drps\" \"drps\" \"drps\" \"drps\" ... #>  $ all_series:'data.frame':\t25 obs. of  3 variables: #>   ..$ score       : num [1:25] 3.18 1.75 3.01 7.07 5.14 ... #>   ..$ eval_horizon: int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type  : chr [1:25] \"sum_drps\" \"sum_drps\" \"sum_drps\" \"sum_drps\" ...  # An example using binary data data <- sim_mvgam(family = bernoulli()) mod <- mvgam(y ~ s(season, bs = 'cc', k = 6),             trend_model = AR(),             data = data$data_train,             newdata = data$data_test,             family = bernoulli(),             chains = 2,             silent = 2)  # Extract forecasts on the expectation (probability) scale fc <- forecast(mod, type = 'expected') plot(fc) #> Out of sample Brier: #> 8.75181507536641   # Compute Brier scores fc_scores <- score(fc, score = 'brier') str(fc_scores) #> List of 4 #>  $ series_1  :'data.frame':\t25 obs. of  5 variables: #>   ..$ score         : num [1:25] 0.426 0.441 0.373 0.281 0.261 ... #>   ..$ in_interval   : num [1:25] NA NA NA NA NA NA NA NA NA NA ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"brier\" \"brier\" \"brier\" \"brier\" ... #>  $ series_2  :'data.frame':\t25 obs. of  5 variables: #>   ..$ score         : num [1:25] 0.0943 0.1037 0.3927 0.3796 0.1763 ... #>   ..$ in_interval   : num [1:25] NA NA NA NA NA NA NA NA NA NA ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"brier\" \"brier\" \"brier\" \"brier\" ... #>  $ series_3  :'data.frame':\t25 obs. of  5 variables: #>   ..$ score         : num [1:25] 0.23 0.204 0.262 0.227 0.208 ... #>   ..$ in_interval   : num [1:25] NA NA NA NA NA NA NA NA NA NA ... #>   ..$ interval_width: num [1:25] 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 ... #>   ..$ eval_horizon  : int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type    : chr [1:25] \"brier\" \"brier\" \"brier\" \"brier\" ... #>  $ all_series:'data.frame':\t25 obs. of  3 variables: #>   ..$ score       : num [1:25] 0.751 0.749 1.028 0.888 0.645 ... #>   ..$ eval_horizon: int [1:25] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ score_type  : chr [1:25] \"sum_brier\" \"sum_brier\" \"sum_brier\" \"sum_brier\" ... # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/series_to_mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert timeseries object to format necessary for mvgam models — series_to_mvgam","title":"Convert timeseries object to format necessary for mvgam models — series_to_mvgam","text":"function converts univariate multivariate time series (xts ts objects) format necessary mvgam","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/series_to_mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert timeseries object to format necessary for mvgam models — series_to_mvgam","text":"","code":"series_to_mvgam(series, freq, train_prop = 0.85)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/series_to_mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert timeseries object to format necessary for mvgam models — series_to_mvgam","text":"series xts ts object converted mvgam format freq integer. seasonal frequency series train_prop numeric stating proportion data use training. 0.25 0.95","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/series_to_mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert timeseries object to format necessary for mvgam models — series_to_mvgam","text":"list object containing outputs needed mvgam, including 'data_train' 'data_test'","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/series_to_mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert timeseries object to format necessary for mvgam models — series_to_mvgam","text":"","code":"# A ts object example data(\"sunspots\") series <- cbind(sunspots, sunspots) colnames(series) <- c('blood', 'bone') head(series) #>          blood bone #> Jan 1749  58.0 58.0 #> Feb 1749  62.6 62.6 #> Mar 1749  70.0 70.0 #> Apr 1749  55.7 55.7 #> May 1749  85.0 85.0 #> Jun 1749  83.5 83.5 series_to_mvgam(series, frequency(series), 0.85) #> $data_train #>          y season year                date series time #> 1     58.0      1 1749 1749-01-01 00:00:00  blood    1 #> 2     58.0      1 1749 1749-01-01 00:00:00   bone    1 #> 3     62.6      2 1749 1749-01-31 09:59:59  blood    2 #> 4     62.6      2 1749 1749-01-31 09:59:59   bone    2 #> 5     70.0      3 1749 1749-03-02 20:00:00  blood    3 #> 6     70.0      3 1749 1749-03-02 20:00:00   bone    3 #> 7     55.7      4 1749 1749-04-02 06:00:00  blood    4 #> 8     55.7      4 1749 1749-04-02 06:00:00   bone    4 #> 9     85.0      5 1749 1749-05-02 15:59:59  blood    5 #> 10    85.0      5 1749 1749-05-02 15:59:59   bone    5 #> 11    83.5      6 1749 1749-06-02 02:00:00  blood    6 #> 12    83.5      6 1749 1749-06-02 02:00:00   bone    6 #> 13    94.8      7 1749 1749-07-02 12:00:00  blood    7 #> 14    94.8      7 1749 1749-07-02 12:00:00   bone    7 #> 15    66.3      8 1749 1749-08-01 21:59:59  blood    8 #> 16    66.3      8 1749 1749-08-01 21:59:59   bone    8 #> 17    75.9      9 1749 1749-09-01 08:00:00  blood    9 #> 18    75.9      9 1749 1749-09-01 08:00:00   bone    9 #> 19    75.5     10 1749 1749-10-01 18:00:00  blood   10 #> 20    75.5     10 1749 1749-10-01 18:00:00   bone   10 #> 21   158.6     11 1749 1749-11-01 03:59:59  blood   11 #> 22   158.6     11 1749 1749-11-01 03:59:59   bone   11 #> 23    85.2     12 1749 1749-12-01 14:00:00  blood   12 #> 24    85.2     12 1749 1749-12-01 14:00:00   bone   12 #> 25    73.3      1 1750 1750-01-01 00:00:00  blood   13 #> 26    73.3      1 1750 1750-01-01 00:00:00   bone   13 #> 27    75.9      2 1750 1750-01-31 09:59:59  blood   14 #> 28    75.9      2 1750 1750-01-31 09:59:59   bone   14 #> 29    89.2      3 1750 1750-03-02 20:00:00  blood   15 #> 30    89.2      3 1750 1750-03-02 20:00:00   bone   15 #> 31    88.3      4 1750 1750-04-02 06:00:00  blood   16 #> 32    88.3      4 1750 1750-04-02 06:00:00   bone   16 #> 33    90.0      5 1750 1750-05-02 15:59:59  blood   17 #> 34    90.0      5 1750 1750-05-02 15:59:59   bone   17 #> 35   100.0      6 1750 1750-06-02 02:00:00  blood   18 #> 36   100.0      6 1750 1750-06-02 02:00:00   bone   18 #> 37    85.4      7 1750 1750-07-02 12:00:00  blood   19 #> 38    85.4      7 1750 1750-07-02 12:00:00   bone   19 #> 39   103.0      8 1750 1750-08-01 21:59:59  blood   20 #> 40   103.0      8 1750 1750-08-01 21:59:59   bone   20 #> 41    91.2      9 1750 1750-09-01 08:00:00  blood   21 #> 42    91.2      9 1750 1750-09-01 08:00:00   bone   21 #> 43    65.7     10 1750 1750-10-01 18:00:00  blood   22 #> 44    65.7     10 1750 1750-10-01 18:00:00   bone   22 #> 45    63.3     11 1750 1750-11-01 03:59:59  blood   23 #> 46    63.3     11 1750 1750-11-01 03:59:59   bone   23 #> 47    75.4     12 1750 1750-12-01 14:00:00  blood   24 #> 48    75.4     12 1750 1750-12-01 14:00:00   bone   24 #> 49    70.0      1 1751 1751-01-01 00:00:00  blood   25 #> 50    70.0      1 1751 1751-01-01 00:00:00   bone   25 #> 51    43.5      2 1751 1751-01-31 09:59:59  blood   26 #> 52    43.5      2 1751 1751-01-31 09:59:59   bone   26 #> 53    45.3      3 1751 1751-03-02 20:00:00  blood   27 #> 54    45.3      3 1751 1751-03-02 20:00:00   bone   27 #> 55    56.4      4 1751 1751-04-02 06:00:00  blood   28 #> 56    56.4      4 1751 1751-04-02 06:00:00   bone   28 #> 57    60.7      5 1751 1751-05-02 15:59:59  blood   29 #> 58    60.7      5 1751 1751-05-02 15:59:59   bone   29 #> 59    50.7      6 1751 1751-06-02 02:00:00  blood   30 #> 60    50.7      6 1751 1751-06-02 02:00:00   bone   30 #> 61    66.3      7 1751 1751-07-02 12:00:00  blood   31 #> 62    66.3      7 1751 1751-07-02 12:00:00   bone   31 #> 63    59.8      8 1751 1751-08-01 21:59:59  blood   32 #> 64    59.8      8 1751 1751-08-01 21:59:59   bone   32 #> 65    23.5      9 1751 1751-09-01 08:00:00  blood   33 #> 66    23.5      9 1751 1751-09-01 08:00:00   bone   33 #> 67    23.2     10 1751 1751-10-01 18:00:00  blood   34 #> 68    23.2     10 1751 1751-10-01 18:00:00   bone   34 #> 69    28.5     11 1751 1751-11-01 03:59:59  blood   35 #> 70    28.5     11 1751 1751-11-01 03:59:59   bone   35 #> 71    44.0     12 1751 1751-12-01 14:00:00  blood   36 #> 72    44.0     12 1751 1751-12-01 14:00:00   bone   36 #> 73    35.0      1 1752 1752-01-01 00:00:00  blood   37 #> 74    35.0      1 1752 1752-01-01 00:00:00   bone   37 #> 75    50.0      2 1752 1752-01-31 11:59:59  blood   38 #> 76    50.0      2 1752 1752-01-31 11:59:59   bone   38 #> 77    71.0      3 1752 1752-03-02 00:00:00  blood   39 #> 78    71.0      3 1752 1752-03-02 00:00:00   bone   39 #> 79    59.3      4 1752 1752-04-01 12:00:00  blood   40 #> 80    59.3      4 1752 1752-04-01 12:00:00   bone   40 #> 81    59.7      5 1752 1752-05-01 23:59:59  blood   41 #> 82    59.7      5 1752 1752-05-01 23:59:59   bone   41 #> 83    39.6      6 1752 1752-06-01 12:00:00  blood   42 #> 84    39.6      6 1752 1752-06-01 12:00:00   bone   42 #> 85    78.4      7 1752 1752-07-02 00:00:00  blood   43 #> 86    78.4      7 1752 1752-07-02 00:00:00   bone   43 #> 87    29.3      8 1752 1752-08-01 11:59:59  blood   44 #> 88    29.3      8 1752 1752-08-01 11:59:59   bone   44 #> 89    27.1      9 1752 1752-09-01 00:00:00  blood   45 #> 90    27.1      9 1752 1752-09-01 00:00:00   bone   45 #> 91    46.6     10 1752 1752-10-01 12:00:00  blood   46 #> 92    46.6     10 1752 1752-10-01 12:00:00   bone   46 #> 93    37.6     11 1752 1752-10-31 23:59:59  blood   47 #> 94    37.6     11 1752 1752-10-31 23:59:59   bone   47 #> 95    40.0     12 1752 1752-12-01 12:00:00  blood   48 #> 96    40.0     12 1752 1752-12-01 12:00:00   bone   48 #> 97    44.0      1 1753 1753-01-01 00:00:00  blood   49 #> 98    44.0      1 1753 1753-01-01 00:00:00   bone   49 #> 99    32.0      2 1753 1753-01-31 09:59:59  blood   50 #> 100   32.0      2 1753 1753-01-31 09:59:59   bone   50 #> 101   45.7      3 1753 1753-03-02 20:00:00  blood   51 #> 102   45.7      3 1753 1753-03-02 20:00:00   bone   51 #> 103   38.0      4 1753 1753-04-02 06:00:00  blood   52 #> 104   38.0      4 1753 1753-04-02 06:00:00   bone   52 #> 105   36.0      5 1753 1753-05-02 15:59:59  blood   53 #> 106   36.0      5 1753 1753-05-02 15:59:59   bone   53 #> 107   31.7      6 1753 1753-06-02 02:00:00  blood   54 #> 108   31.7      6 1753 1753-06-02 02:00:00   bone   54 #> 109   22.2      7 1753 1753-07-02 12:00:00  blood   55 #> 110   22.2      7 1753 1753-07-02 12:00:00   bone   55 #> 111   39.0      8 1753 1753-08-01 21:59:59  blood   56 #> 112   39.0      8 1753 1753-08-01 21:59:59   bone   56 #> 113   28.0      9 1753 1753-09-01 08:00:00  blood   57 #> 114   28.0      9 1753 1753-09-01 08:00:00   bone   57 #> 115   25.0     10 1753 1753-10-01 18:00:00  blood   58 #> 116   25.0     10 1753 1753-10-01 18:00:00   bone   58 #> 117   20.0     11 1753 1753-11-01 03:59:59  blood   59 #> 118   20.0     11 1753 1753-11-01 03:59:59   bone   59 #> 119    6.7     12 1753 1753-12-01 14:00:00  blood   60 #> 120    6.7     12 1753 1753-12-01 14:00:00   bone   60 #> 121    0.0      1 1754 1754-01-01 00:00:00  blood   61 #> 122    0.0      1 1754 1754-01-01 00:00:00   bone   61 #> 123    3.0      2 1754 1754-01-31 09:59:59  blood   62 #> 124    3.0      2 1754 1754-01-31 09:59:59   bone   62 #> 125    1.7      3 1754 1754-03-02 20:00:00  blood   63 #> 126    1.7      3 1754 1754-03-02 20:00:00   bone   63 #> 127   13.7      4 1754 1754-04-02 06:00:00  blood   64 #> 128   13.7      4 1754 1754-04-02 06:00:00   bone   64 #> 129   20.7      5 1754 1754-05-02 15:59:59  blood   65 #> 130   20.7      5 1754 1754-05-02 15:59:59   bone   65 #> 131   26.7      6 1754 1754-06-02 02:00:00  blood   66 #> 132   26.7      6 1754 1754-06-02 02:00:00   bone   66 #> 133   18.8      7 1754 1754-07-02 12:00:00  blood   67 #> 134   18.8      7 1754 1754-07-02 12:00:00   bone   67 #> 135   12.3      8 1754 1754-08-01 21:59:59  blood   68 #> 136   12.3      8 1754 1754-08-01 21:59:59   bone   68 #> 137    8.2      9 1754 1754-09-01 08:00:00  blood   69 #> 138    8.2      9 1754 1754-09-01 08:00:00   bone   69 #> 139   24.1     10 1754 1754-10-01 18:00:00  blood   70 #> 140   24.1     10 1754 1754-10-01 18:00:00   bone   70 #> 141   13.2     11 1754 1754-11-01 03:59:59  blood   71 #> 142   13.2     11 1754 1754-11-01 03:59:59   bone   71 #> 143    4.2     12 1754 1754-12-01 14:00:00  blood   72 #> 144    4.2     12 1754 1754-12-01 14:00:00   bone   72 #> 145   10.2      1 1755 1755-01-01 00:00:00  blood   73 #> 146   10.2      1 1755 1755-01-01 00:00:00   bone   73 #> 147   11.2      2 1755 1755-01-31 09:59:59  blood   74 #> 148   11.2      2 1755 1755-01-31 09:59:59   bone   74 #> 149    6.8      3 1755 1755-03-02 20:00:00  blood   75 #> 150    6.8      3 1755 1755-03-02 20:00:00   bone   75 #> 151    6.5      4 1755 1755-04-02 06:00:00  blood   76 #> 152    6.5      4 1755 1755-04-02 06:00:00   bone   76 #> 153    0.0      5 1755 1755-05-02 15:59:59  blood   77 #> 154    0.0      5 1755 1755-05-02 15:59:59   bone   77 #> 155    0.0      6 1755 1755-06-02 02:00:00  blood   78 #> 156    0.0      6 1755 1755-06-02 02:00:00   bone   78 #> 157    8.6      7 1755 1755-07-02 12:00:00  blood   79 #> 158    8.6      7 1755 1755-07-02 12:00:00   bone   79 #> 159    3.2      8 1755 1755-08-01 21:59:59  blood   80 #> 160    3.2      8 1755 1755-08-01 21:59:59   bone   80 #> 161   17.8      9 1755 1755-09-01 08:00:00  blood   81 #> 162   17.8      9 1755 1755-09-01 08:00:00   bone   81 #> 163   23.7     10 1755 1755-10-01 18:00:00  blood   82 #> 164   23.7     10 1755 1755-10-01 18:00:00   bone   82 #> 165    6.8     11 1755 1755-11-01 03:59:59  blood   83 #> 166    6.8     11 1755 1755-11-01 03:59:59   bone   83 #> 167   20.0     12 1755 1755-12-01 14:00:00  blood   84 #> 168   20.0     12 1755 1755-12-01 14:00:00   bone   84 #> 169   12.5      1 1756 1756-01-01 00:00:00  blood   85 #> 170   12.5      1 1756 1756-01-01 00:00:00   bone   85 #> 171    7.1      2 1756 1756-01-31 11:59:59  blood   86 #> 172    7.1      2 1756 1756-01-31 11:59:59   bone   86 #> 173    5.4      3 1756 1756-03-02 00:00:00  blood   87 #> 174    5.4      3 1756 1756-03-02 00:00:00   bone   87 #> 175    9.4      4 1756 1756-04-01 12:00:00  blood   88 #> 176    9.4      4 1756 1756-04-01 12:00:00   bone   88 #> 177   12.5      5 1756 1756-05-01 23:59:59  blood   89 #> 178   12.5      5 1756 1756-05-01 23:59:59   bone   89 #> 179   12.9      6 1756 1756-06-01 12:00:00  blood   90 #> 180   12.9      6 1756 1756-06-01 12:00:00   bone   90 #> 181    3.6      7 1756 1756-07-02 00:00:00  blood   91 #> 182    3.6      7 1756 1756-07-02 00:00:00   bone   91 #> 183    6.4      8 1756 1756-08-01 11:59:59  blood   92 #> 184    6.4      8 1756 1756-08-01 11:59:59   bone   92 #> 185   11.8      9 1756 1756-09-01 00:00:00  blood   93 #> 186   11.8      9 1756 1756-09-01 00:00:00   bone   93 #> 187   14.3     10 1756 1756-10-01 12:00:00  blood   94 #> 188   14.3     10 1756 1756-10-01 12:00:00   bone   94 #> 189   17.0     11 1756 1756-10-31 23:59:59  blood   95 #> 190   17.0     11 1756 1756-10-31 23:59:59   bone   95 #> 191    9.4     12 1756 1756-12-01 12:00:00  blood   96 #> 192    9.4     12 1756 1756-12-01 12:00:00   bone   96 #> 193   14.1      1 1757 1757-01-01 00:00:00  blood   97 #> 194   14.1      1 1757 1757-01-01 00:00:00   bone   97 #> 195   21.2      2 1757 1757-01-31 09:59:59  blood   98 #> 196   21.2      2 1757 1757-01-31 09:59:59   bone   98 #> 197   26.2      3 1757 1757-03-02 20:00:00  blood   99 #> 198   26.2      3 1757 1757-03-02 20:00:00   bone   99 #> 199   30.0      4 1757 1757-04-02 06:00:00  blood  100 #> 200   30.0      4 1757 1757-04-02 06:00:00   bone  100 #> 201   38.1      5 1757 1757-05-02 15:59:59  blood  101 #> 202   38.1      5 1757 1757-05-02 15:59:59   bone  101 #> 203   12.8      6 1757 1757-06-02 02:00:00  blood  102 #> 204   12.8      6 1757 1757-06-02 02:00:00   bone  102 #> 205   25.0      7 1757 1757-07-02 12:00:00  blood  103 #> 206   25.0      7 1757 1757-07-02 12:00:00   bone  103 #> 207   51.3      8 1757 1757-08-01 21:59:59  blood  104 #> 208   51.3      8 1757 1757-08-01 21:59:59   bone  104 #> 209   39.7      9 1757 1757-09-01 08:00:00  blood  105 #> 210   39.7      9 1757 1757-09-01 08:00:00   bone  105 #> 211   32.5     10 1757 1757-10-01 18:00:00  blood  106 #> 212   32.5     10 1757 1757-10-01 18:00:00   bone  106 #> 213   64.7     11 1757 1757-11-01 03:59:59  blood  107 #> 214   64.7     11 1757 1757-11-01 03:59:59   bone  107 #> 215   33.5     12 1757 1757-12-01 14:00:00  blood  108 #> 216   33.5     12 1757 1757-12-01 14:00:00   bone  108 #> 217   37.6      1 1758 1758-01-01 00:00:00  blood  109 #> 218   37.6      1 1758 1758-01-01 00:00:00   bone  109 #> 219   52.0      2 1758 1758-01-31 09:59:59  blood  110 #> 220   52.0      2 1758 1758-01-31 09:59:59   bone  110 #> 221   49.0      3 1758 1758-03-02 20:00:00  blood  111 #> 222   49.0      3 1758 1758-03-02 20:00:00   bone  111 #> 223   72.3      4 1758 1758-04-02 06:00:00  blood  112 #> 224   72.3      4 1758 1758-04-02 06:00:00   bone  112 #> 225   46.4      5 1758 1758-05-02 15:59:59  blood  113 #> 226   46.4      5 1758 1758-05-02 15:59:59   bone  113 #> 227   45.0      6 1758 1758-06-02 02:00:00  blood  114 #> 228   45.0      6 1758 1758-06-02 02:00:00   bone  114 #> 229   44.0      7 1758 1758-07-02 12:00:00  blood  115 #> 230   44.0      7 1758 1758-07-02 12:00:00   bone  115 #> 231   38.7      8 1758 1758-08-01 21:59:59  blood  116 #> 232   38.7      8 1758 1758-08-01 21:59:59   bone  116 #> 233   62.5      9 1758 1758-09-01 08:00:00  blood  117 #> 234   62.5      9 1758 1758-09-01 08:00:00   bone  117 #> 235   37.7     10 1758 1758-10-01 18:00:00  blood  118 #> 236   37.7     10 1758 1758-10-01 18:00:00   bone  118 #> 237   43.0     11 1758 1758-11-01 03:59:59  blood  119 #> 238   43.0     11 1758 1758-11-01 03:59:59   bone  119 #> 239   43.0     12 1758 1758-12-01 14:00:00  blood  120 #> 240   43.0     12 1758 1758-12-01 14:00:00   bone  120 #> 241   48.3      1 1759 1759-01-01 00:00:00  blood  121 #> 242   48.3      1 1759 1759-01-01 00:00:00   bone  121 #> 243   44.0      2 1759 1759-01-31 09:59:59  blood  122 #> 244   44.0      2 1759 1759-01-31 09:59:59   bone  122 #> 245   46.8      3 1759 1759-03-02 20:00:00  blood  123 #> 246   46.8      3 1759 1759-03-02 20:00:00   bone  123 #> 247   47.0      4 1759 1759-04-02 06:00:00  blood  124 #> 248   47.0      4 1759 1759-04-02 06:00:00   bone  124 #> 249   49.0      5 1759 1759-05-02 15:59:59  blood  125 #> 250   49.0      5 1759 1759-05-02 15:59:59   bone  125 #> 251   50.0      6 1759 1759-06-02 02:00:00  blood  126 #> 252   50.0      6 1759 1759-06-02 02:00:00   bone  126 #> 253   51.0      7 1759 1759-07-02 12:00:00  blood  127 #> 254   51.0      7 1759 1759-07-02 12:00:00   bone  127 #> 255   71.3      8 1759 1759-08-01 21:59:59  blood  128 #> 256   71.3      8 1759 1759-08-01 21:59:59   bone  128 #> 257   77.2      9 1759 1759-09-01 08:00:00  blood  129 #> 258   77.2      9 1759 1759-09-01 08:00:00   bone  129 #> 259   59.7     10 1759 1759-10-01 18:00:00  blood  130 #> 260   59.7     10 1759 1759-10-01 18:00:00   bone  130 #> 261   46.3     11 1759 1759-11-01 03:59:59  blood  131 #> 262   46.3     11 1759 1759-11-01 03:59:59   bone  131 #> 263   57.0     12 1759 1759-12-01 14:00:00  blood  132 #> 264   57.0     12 1759 1759-12-01 14:00:00   bone  132 #> 265   67.3      1 1760 1760-01-01 00:00:00  blood  133 #> 266   67.3      1 1760 1760-01-01 00:00:00   bone  133 #> 267   59.5      2 1760 1760-01-31 11:59:59  blood  134 #> 268   59.5      2 1760 1760-01-31 11:59:59   bone  134 #> 269   74.7      3 1760 1760-03-02 00:00:00  blood  135 #> 270   74.7      3 1760 1760-03-02 00:00:00   bone  135 #> 271   58.3      4 1760 1760-04-01 12:00:00  blood  136 #> 272   58.3      4 1760 1760-04-01 12:00:00   bone  136 #> 273   72.0      5 1760 1760-05-01 23:59:59  blood  137 #> 274   72.0      5 1760 1760-05-01 23:59:59   bone  137 #> 275   48.3      6 1760 1760-06-01 12:00:00  blood  138 #> 276   48.3      6 1760 1760-06-01 12:00:00   bone  138 #> 277   66.0      7 1760 1760-07-02 00:00:00  blood  139 #> 278   66.0      7 1760 1760-07-02 00:00:00   bone  139 #> 279   75.6      8 1760 1760-08-01 11:59:59  blood  140 #> 280   75.6      8 1760 1760-08-01 11:59:59   bone  140 #> 281   61.3      9 1760 1760-09-01 00:00:00  blood  141 #> 282   61.3      9 1760 1760-09-01 00:00:00   bone  141 #> 283   50.6     10 1760 1760-10-01 12:00:00  blood  142 #> 284   50.6     10 1760 1760-10-01 12:00:00   bone  142 #> 285   59.7     11 1760 1760-10-31 23:59:59  blood  143 #> 286   59.7     11 1760 1760-10-31 23:59:59   bone  143 #> 287   61.0     12 1760 1760-12-01 12:00:00  blood  144 #> 288   61.0     12 1760 1760-12-01 12:00:00   bone  144 #> 289   70.0      1 1761 1761-01-01 00:00:00  blood  145 #> 290   70.0      1 1761 1761-01-01 00:00:00   bone  145 #> 291   91.0      2 1761 1761-01-31 09:59:59  blood  146 #> 292   91.0      2 1761 1761-01-31 09:59:59   bone  146 #> 293   80.7      3 1761 1761-03-02 20:00:00  blood  147 #> 294   80.7      3 1761 1761-03-02 20:00:00   bone  147 #> 295   71.7      4 1761 1761-04-02 06:00:00  blood  148 #> 296   71.7      4 1761 1761-04-02 06:00:00   bone  148 #> 297  107.2      5 1761 1761-05-02 15:59:59  blood  149 #> 298  107.2      5 1761 1761-05-02 15:59:59   bone  149 #> 299   99.3      6 1761 1761-06-02 02:00:00  blood  150 #> 300   99.3      6 1761 1761-06-02 02:00:00   bone  150 #> 301   94.1      7 1761 1761-07-02 12:00:00  blood  151 #> 302   94.1      7 1761 1761-07-02 12:00:00   bone  151 #> 303   91.1      8 1761 1761-08-01 21:59:59  blood  152 #> 304   91.1      8 1761 1761-08-01 21:59:59   bone  152 #> 305  100.7      9 1761 1761-09-01 08:00:00  blood  153 #> 306  100.7      9 1761 1761-09-01 08:00:00   bone  153 #> 307   88.7     10 1761 1761-10-01 18:00:00  blood  154 #> 308   88.7     10 1761 1761-10-01 18:00:00   bone  154 #> 309   89.7     11 1761 1761-11-01 03:59:59  blood  155 #> 310   89.7     11 1761 1761-11-01 03:59:59   bone  155 #> 311   46.0     12 1761 1761-12-01 14:00:00  blood  156 #> 312   46.0     12 1761 1761-12-01 14:00:00   bone  156 #> 313   43.8      1 1762 1762-01-01 00:00:00  blood  157 #> 314   43.8      1 1762 1762-01-01 00:00:00   bone  157 #> 315   72.8      2 1762 1762-01-31 09:59:59  blood  158 #> 316   72.8      2 1762 1762-01-31 09:59:59   bone  158 #> 317   45.7      3 1762 1762-03-02 20:00:00  blood  159 #> 318   45.7      3 1762 1762-03-02 20:00:00   bone  159 #> 319   60.2      4 1762 1762-04-02 06:00:00  blood  160 #> 320   60.2      4 1762 1762-04-02 06:00:00   bone  160 #> 321   39.9      5 1762 1762-05-02 15:59:59  blood  161 #> 322   39.9      5 1762 1762-05-02 15:59:59   bone  161 #> 323   77.1      6 1762 1762-06-02 02:00:00  blood  162 #> 324   77.1      6 1762 1762-06-02 02:00:00   bone  162 #> 325   33.8      7 1762 1762-07-02 12:00:00  blood  163 #> 326   33.8      7 1762 1762-07-02 12:00:00   bone  163 #> 327   67.7      8 1762 1762-08-01 21:59:59  blood  164 #> 328   67.7      8 1762 1762-08-01 21:59:59   bone  164 #> 329   68.5      9 1762 1762-09-01 08:00:00  blood  165 #> 330   68.5      9 1762 1762-09-01 08:00:00   bone  165 #> 331   69.3     10 1762 1762-10-01 18:00:00  blood  166 #> 332   69.3     10 1762 1762-10-01 18:00:00   bone  166 #> 333   77.8     11 1762 1762-11-01 03:59:59  blood  167 #> 334   77.8     11 1762 1762-11-01 03:59:59   bone  167 #> 335   77.2     12 1762 1762-12-01 14:00:00  blood  168 #> 336   77.2     12 1762 1762-12-01 14:00:00   bone  168 #> 337   56.5      1 1763 1763-01-01 00:00:00  blood  169 #> 338   56.5      1 1763 1763-01-01 00:00:00   bone  169 #> 339   31.9      2 1763 1763-01-31 09:59:59  blood  170 #> 340   31.9      2 1763 1763-01-31 09:59:59   bone  170 #> 341   34.2      3 1763 1763-03-02 20:00:00  blood  171 #> 342   34.2      3 1763 1763-03-02 20:00:00   bone  171 #> 343   32.9      4 1763 1763-04-02 06:00:00  blood  172 #> 344   32.9      4 1763 1763-04-02 06:00:00   bone  172 #> 345   32.7      5 1763 1763-05-02 15:59:59  blood  173 #> 346   32.7      5 1763 1763-05-02 15:59:59   bone  173 #> 347   35.8      6 1763 1763-06-02 02:00:00  blood  174 #> 348   35.8      6 1763 1763-06-02 02:00:00   bone  174 #> 349   54.2      7 1763 1763-07-02 12:00:00  blood  175 #> 350   54.2      7 1763 1763-07-02 12:00:00   bone  175 #> 351   26.5      8 1763 1763-08-01 21:59:59  blood  176 #> 352   26.5      8 1763 1763-08-01 21:59:59   bone  176 #> 353   68.1      9 1763 1763-09-01 08:00:00  blood  177 #> 354   68.1      9 1763 1763-09-01 08:00:00   bone  177 #> 355   46.3     10 1763 1763-10-01 18:00:00  blood  178 #> 356   46.3     10 1763 1763-10-01 18:00:00   bone  178 #> 357   60.9     11 1763 1763-11-01 03:59:59  blood  179 #> 358   60.9     11 1763 1763-11-01 03:59:59   bone  179 #> 359   61.4     12 1763 1763-12-01 14:00:00  blood  180 #> 360   61.4     12 1763 1763-12-01 14:00:00   bone  180 #> 361   59.7      1 1764 1764-01-01 00:00:00  blood  181 #> 362   59.7      1 1764 1764-01-01 00:00:00   bone  181 #> 363   59.7      2 1764 1764-01-31 11:59:59  blood  182 #> 364   59.7      2 1764 1764-01-31 11:59:59   bone  182 #> 365   40.2      3 1764 1764-03-02 00:00:00  blood  183 #> 366   40.2      3 1764 1764-03-02 00:00:00   bone  183 #> 367   34.4      4 1764 1764-04-01 12:00:00  blood  184 #> 368   34.4      4 1764 1764-04-01 12:00:00   bone  184 #> 369   44.3      5 1764 1764-05-01 23:59:59  blood  185 #> 370   44.3      5 1764 1764-05-01 23:59:59   bone  185 #> 371   30.0      6 1764 1764-06-01 12:00:00  blood  186 #> 372   30.0      6 1764 1764-06-01 12:00:00   bone  186 #> 373   30.0      7 1764 1764-07-02 00:00:00  blood  187 #> 374   30.0      7 1764 1764-07-02 00:00:00   bone  187 #> 375   30.0      8 1764 1764-08-01 11:59:59  blood  188 #> 376   30.0      8 1764 1764-08-01 11:59:59   bone  188 #> 377   28.2      9 1764 1764-09-01 00:00:00  blood  189 #> 378   28.2      9 1764 1764-09-01 00:00:00   bone  189 #> 379   28.0     10 1764 1764-10-01 12:00:00  blood  190 #> 380   28.0     10 1764 1764-10-01 12:00:00   bone  190 #> 381   26.0     11 1764 1764-10-31 23:59:59  blood  191 #> 382   26.0     11 1764 1764-10-31 23:59:59   bone  191 #> 383   25.7     12 1764 1764-12-01 12:00:00  blood  192 #> 384   25.7     12 1764 1764-12-01 12:00:00   bone  192 #> 385   24.0      1 1765 1765-01-01 00:00:00  blood  193 #> 386   24.0      1 1765 1765-01-01 00:00:00   bone  193 #> 387   26.0      2 1765 1765-01-31 09:59:59  blood  194 #> 388   26.0      2 1765 1765-01-31 09:59:59   bone  194 #> 389   25.0      3 1765 1765-03-02 20:00:00  blood  195 #> 390   25.0      3 1765 1765-03-02 20:00:00   bone  195 #> 391   22.0      4 1765 1765-04-02 06:00:00  blood  196 #> 392   22.0      4 1765 1765-04-02 06:00:00   bone  196 #> 393   20.2      5 1765 1765-05-02 15:59:59  blood  197 #> 394   20.2      5 1765 1765-05-02 15:59:59   bone  197 #> 395   20.0      6 1765 1765-06-02 02:00:00  blood  198 #> 396   20.0      6 1765 1765-06-02 02:00:00   bone  198 #> 397   27.0      7 1765 1765-07-02 12:00:00  blood  199 #> 398   27.0      7 1765 1765-07-02 12:00:00   bone  199 #> 399   29.7      8 1765 1765-08-01 21:59:59  blood  200 #> 400   29.7      8 1765 1765-08-01 21:59:59   bone  200 #> 401   16.0      9 1765 1765-09-01 08:00:00  blood  201 #> 402   16.0      9 1765 1765-09-01 08:00:00   bone  201 #> 403   14.0     10 1765 1765-10-01 18:00:00  blood  202 #> 404   14.0     10 1765 1765-10-01 18:00:00   bone  202 #> 405   14.0     11 1765 1765-11-01 03:59:59  blood  203 #> 406   14.0     11 1765 1765-11-01 03:59:59   bone  203 #> 407   13.0     12 1765 1765-12-01 14:00:00  blood  204 #> 408   13.0     12 1765 1765-12-01 14:00:00   bone  204 #> 409   12.0      1 1766 1766-01-01 00:00:00  blood  205 #> 410   12.0      1 1766 1766-01-01 00:00:00   bone  205 #> 411   11.0      2 1766 1766-01-31 09:59:59  blood  206 #> 412   11.0      2 1766 1766-01-31 09:59:59   bone  206 #> 413   36.6      3 1766 1766-03-02 20:00:00  blood  207 #> 414   36.6      3 1766 1766-03-02 20:00:00   bone  207 #> 415    6.0      4 1766 1766-04-02 06:00:00  blood  208 #> 416    6.0      4 1766 1766-04-02 06:00:00   bone  208 #> 417   26.8      5 1766 1766-05-02 15:59:59  blood  209 #> 418   26.8      5 1766 1766-05-02 15:59:59   bone  209 #> 419    3.0      6 1766 1766-06-02 02:00:00  blood  210 #> 420    3.0      6 1766 1766-06-02 02:00:00   bone  210 #> 421    3.3      7 1766 1766-07-02 12:00:00  blood  211 #> 422    3.3      7 1766 1766-07-02 12:00:00   bone  211 #> 423    4.0      8 1766 1766-08-01 21:59:59  blood  212 #> 424    4.0      8 1766 1766-08-01 21:59:59   bone  212 #> 425    4.3      9 1766 1766-09-01 08:00:00  blood  213 #> 426    4.3      9 1766 1766-09-01 08:00:00   bone  213 #> 427    5.0     10 1766 1766-10-01 18:00:00  blood  214 #> 428    5.0     10 1766 1766-10-01 18:00:00   bone  214 #> 429    5.7     11 1766 1766-11-01 03:59:59  blood  215 #> 430    5.7     11 1766 1766-11-01 03:59:59   bone  215 #> 431   19.2     12 1766 1766-12-01 14:00:00  blood  216 #> 432   19.2     12 1766 1766-12-01 14:00:00   bone  216 #> 433   27.4      1 1767 1767-01-01 00:00:00  blood  217 #> 434   27.4      1 1767 1767-01-01 00:00:00   bone  217 #> 435   30.0      2 1767 1767-01-31 09:59:59  blood  218 #> 436   30.0      2 1767 1767-01-31 09:59:59   bone  218 #> 437   43.0      3 1767 1767-03-02 20:00:00  blood  219 #> 438   43.0      3 1767 1767-03-02 20:00:00   bone  219 #> 439   32.9      4 1767 1767-04-02 06:00:00  blood  220 #> 440   32.9      4 1767 1767-04-02 06:00:00   bone  220 #> 441   29.8      5 1767 1767-05-02 15:59:59  blood  221 #> 442   29.8      5 1767 1767-05-02 15:59:59   bone  221 #> 443   33.3      6 1767 1767-06-02 02:00:00  blood  222 #> 444   33.3      6 1767 1767-06-02 02:00:00   bone  222 #> 445   21.9      7 1767 1767-07-02 12:00:00  blood  223 #> 446   21.9      7 1767 1767-07-02 12:00:00   bone  223 #> 447   40.8      8 1767 1767-08-01 21:59:59  blood  224 #> 448   40.8      8 1767 1767-08-01 21:59:59   bone  224 #> 449   42.7      9 1767 1767-09-01 08:00:00  blood  225 #> 450   42.7      9 1767 1767-09-01 08:00:00   bone  225 #> 451   44.1     10 1767 1767-10-01 18:00:00  blood  226 #> 452   44.1     10 1767 1767-10-01 18:00:00   bone  226 #> 453   54.7     11 1767 1767-11-01 03:59:59  blood  227 #> 454   54.7     11 1767 1767-11-01 03:59:59   bone  227 #> 455   53.3     12 1767 1767-12-01 14:00:00  blood  228 #> 456   53.3     12 1767 1767-12-01 14:00:00   bone  228 #> 457   53.5      1 1768 1768-01-01 00:00:00  blood  229 #> 458   53.5      1 1768 1768-01-01 00:00:00   bone  229 #> 459   66.1      2 1768 1768-01-31 11:59:59  blood  230 #> 460   66.1      2 1768 1768-01-31 11:59:59   bone  230 #> 461   46.3      3 1768 1768-03-02 00:00:00  blood  231 #> 462   46.3      3 1768 1768-03-02 00:00:00   bone  231 #> 463   42.7      4 1768 1768-04-01 12:00:00  blood  232 #> 464   42.7      4 1768 1768-04-01 12:00:00   bone  232 #> 465   77.7      5 1768 1768-05-01 23:59:59  blood  233 #> 466   77.7      5 1768 1768-05-01 23:59:59   bone  233 #> 467   77.4      6 1768 1768-06-01 12:00:00  blood  234 #> 468   77.4      6 1768 1768-06-01 12:00:00   bone  234 #> 469   52.6      7 1768 1768-07-02 00:00:00  blood  235 #> 470   52.6      7 1768 1768-07-02 00:00:00   bone  235 #> 471   66.8      8 1768 1768-08-01 11:59:59  blood  236 #> 472   66.8      8 1768 1768-08-01 11:59:59   bone  236 #> 473   74.8      9 1768 1768-09-01 00:00:00  blood  237 #> 474   74.8      9 1768 1768-09-01 00:00:00   bone  237 #> 475   77.8     10 1768 1768-10-01 12:00:00  blood  238 #> 476   77.8     10 1768 1768-10-01 12:00:00   bone  238 #> 477   90.6     11 1768 1768-10-31 23:59:59  blood  239 #> 478   90.6     11 1768 1768-10-31 23:59:59   bone  239 #> 479  111.8     12 1768 1768-12-01 12:00:00  blood  240 #> 480  111.8     12 1768 1768-12-01 12:00:00   bone  240 #> 481   73.9      1 1769 1769-01-01 00:00:00  blood  241 #> 482   73.9      1 1769 1769-01-01 00:00:00   bone  241 #> 483   64.2      2 1769 1769-01-31 09:59:59  blood  242 #> 484   64.2      2 1769 1769-01-31 09:59:59   bone  242 #> 485   64.3      3 1769 1769-03-02 20:00:00  blood  243 #> 486   64.3      3 1769 1769-03-02 20:00:00   bone  243 #> 487   96.7      4 1769 1769-04-02 06:00:00  blood  244 #> 488   96.7      4 1769 1769-04-02 06:00:00   bone  244 #> 489   73.6      5 1769 1769-05-02 15:59:59  blood  245 #> 490   73.6      5 1769 1769-05-02 15:59:59   bone  245 #> 491   94.4      6 1769 1769-06-02 02:00:00  blood  246 #> 492   94.4      6 1769 1769-06-02 02:00:00   bone  246 #> 493  118.6      7 1769 1769-07-02 12:00:00  blood  247 #> 494  118.6      7 1769 1769-07-02 12:00:00   bone  247 #> 495  120.3      8 1769 1769-08-01 21:59:59  blood  248 #> 496  120.3      8 1769 1769-08-01 21:59:59   bone  248 #> 497  148.8      9 1769 1769-09-01 08:00:00  blood  249 #> 498  148.8      9 1769 1769-09-01 08:00:00   bone  249 #> 499  158.2     10 1769 1769-10-01 18:00:00  blood  250 #> 500  158.2     10 1769 1769-10-01 18:00:00   bone  250 #> 501  148.1     11 1769 1769-11-01 03:59:59  blood  251 #> 502  148.1     11 1769 1769-11-01 03:59:59   bone  251 #> 503  112.0     12 1769 1769-12-01 14:00:00  blood  252 #> 504  112.0     12 1769 1769-12-01 14:00:00   bone  252 #> 505  104.0      1 1770 1770-01-01 00:00:00  blood  253 #> 506  104.0      1 1770 1770-01-01 00:00:00   bone  253 #> 507  142.5      2 1770 1770-01-31 09:59:59  blood  254 #> 508  142.5      2 1770 1770-01-31 09:59:59   bone  254 #> 509   80.1      3 1770 1770-03-02 20:00:00  blood  255 #> 510   80.1      3 1770 1770-03-02 20:00:00   bone  255 #> 511   51.0      4 1770 1770-04-02 06:00:00  blood  256 #> 512   51.0      4 1770 1770-04-02 06:00:00   bone  256 #> 513   70.1      5 1770 1770-05-02 15:59:59  blood  257 #> 514   70.1      5 1770 1770-05-02 15:59:59   bone  257 #> 515   83.3      6 1770 1770-06-02 02:00:00  blood  258 #> 516   83.3      6 1770 1770-06-02 02:00:00   bone  258 #> 517  109.8      7 1770 1770-07-02 12:00:00  blood  259 #> 518  109.8      7 1770 1770-07-02 12:00:00   bone  259 #> 519  126.3      8 1770 1770-08-01 21:59:59  blood  260 #> 520  126.3      8 1770 1770-08-01 21:59:59   bone  260 #> 521  104.4      9 1770 1770-09-01 08:00:00  blood  261 #> 522  104.4      9 1770 1770-09-01 08:00:00   bone  261 #> 523  103.6     10 1770 1770-10-01 18:00:00  blood  262 #> 524  103.6     10 1770 1770-10-01 18:00:00   bone  262 #> 525  132.2     11 1770 1770-11-01 03:59:59  blood  263 #> 526  132.2     11 1770 1770-11-01 03:59:59   bone  263 #> 527  102.3     12 1770 1770-12-01 14:00:00  blood  264 #> 528  102.3     12 1770 1770-12-01 14:00:00   bone  264 #> 529   36.0      1 1771 1771-01-01 00:00:00  blood  265 #> 530   36.0      1 1771 1771-01-01 00:00:00   bone  265 #> 531   46.2      2 1771 1771-01-31 09:59:59  blood  266 #> 532   46.2      2 1771 1771-01-31 09:59:59   bone  266 #> 533   46.7      3 1771 1771-03-02 20:00:00  blood  267 #> 534   46.7      3 1771 1771-03-02 20:00:00   bone  267 #> 535   64.9      4 1771 1771-04-02 06:00:00  blood  268 #> 536   64.9      4 1771 1771-04-02 06:00:00   bone  268 #> 537  152.7      5 1771 1771-05-02 15:59:59  blood  269 #> 538  152.7      5 1771 1771-05-02 15:59:59   bone  269 #> 539  119.5      6 1771 1771-06-02 02:00:00  blood  270 #> 540  119.5      6 1771 1771-06-02 02:00:00   bone  270 #> 541   67.7      7 1771 1771-07-02 12:00:00  blood  271 #> 542   67.7      7 1771 1771-07-02 12:00:00   bone  271 #> 543   58.5      8 1771 1771-08-01 21:59:59  blood  272 #> 544   58.5      8 1771 1771-08-01 21:59:59   bone  272 #> 545  101.4      9 1771 1771-09-01 08:00:00  blood  273 #> 546  101.4      9 1771 1771-09-01 08:00:00   bone  273 #> 547   90.0     10 1771 1771-10-01 18:00:00  blood  274 #> 548   90.0     10 1771 1771-10-01 18:00:00   bone  274 #> 549   99.7     11 1771 1771-11-01 03:59:59  blood  275 #> 550   99.7     11 1771 1771-11-01 03:59:59   bone  275 #> 551   95.7     12 1771 1771-12-01 14:00:00  blood  276 #> 552   95.7     12 1771 1771-12-01 14:00:00   bone  276 #> 553  100.9      1 1772 1772-01-01 00:00:00  blood  277 #> 554  100.9      1 1772 1772-01-01 00:00:00   bone  277 #> 555   90.8      2 1772 1772-01-31 11:59:59  blood  278 #> 556   90.8      2 1772 1772-01-31 11:59:59   bone  278 #> 557   31.1      3 1772 1772-03-02 00:00:00  blood  279 #> 558   31.1      3 1772 1772-03-02 00:00:00   bone  279 #> 559   92.2      4 1772 1772-04-01 12:00:00  blood  280 #> 560   92.2      4 1772 1772-04-01 12:00:00   bone  280 #> 561   38.0      5 1772 1772-05-01 23:59:59  blood  281 #> 562   38.0      5 1772 1772-05-01 23:59:59   bone  281 #> 563   57.0      6 1772 1772-06-01 12:00:00  blood  282 #> 564   57.0      6 1772 1772-06-01 12:00:00   bone  282 #> 565   77.3      7 1772 1772-07-02 00:00:00  blood  283 #> 566   77.3      7 1772 1772-07-02 00:00:00   bone  283 #> 567   56.2      8 1772 1772-08-01 11:59:59  blood  284 #> 568   56.2      8 1772 1772-08-01 11:59:59   bone  284 #> 569   50.5      9 1772 1772-09-01 00:00:00  blood  285 #> 570   50.5      9 1772 1772-09-01 00:00:00   bone  285 #> 571   78.6     10 1772 1772-10-01 12:00:00  blood  286 #> 572   78.6     10 1772 1772-10-01 12:00:00   bone  286 #> 573   61.3     11 1772 1772-10-31 23:59:59  blood  287 #> 574   61.3     11 1772 1772-10-31 23:59:59   bone  287 #> 575   64.0     12 1772 1772-12-01 12:00:00  blood  288 #> 576   64.0     12 1772 1772-12-01 12:00:00   bone  288 #> 577   54.6      1 1773 1773-01-01 00:00:00  blood  289 #> 578   54.6      1 1773 1773-01-01 00:00:00   bone  289 #> 579   29.0      2 1773 1773-01-31 09:59:59  blood  290 #> 580   29.0      2 1773 1773-01-31 09:59:59   bone  290 #> 581   51.2      3 1773 1773-03-02 20:00:00  blood  291 #> 582   51.2      3 1773 1773-03-02 20:00:00   bone  291 #> 583   32.9      4 1773 1773-04-02 06:00:00  blood  292 #> 584   32.9      4 1773 1773-04-02 06:00:00   bone  292 #> 585   41.1      5 1773 1773-05-02 15:59:59  blood  293 #> 586   41.1      5 1773 1773-05-02 15:59:59   bone  293 #> 587   28.4      6 1773 1773-06-02 02:00:00  blood  294 #> 588   28.4      6 1773 1773-06-02 02:00:00   bone  294 #> 589   27.7      7 1773 1773-07-02 12:00:00  blood  295 #> 590   27.7      7 1773 1773-07-02 12:00:00   bone  295 #> 591   12.7      8 1773 1773-08-01 21:59:59  blood  296 #> 592   12.7      8 1773 1773-08-01 21:59:59   bone  296 #> 593   29.3      9 1773 1773-09-01 08:00:00  blood  297 #> 594   29.3      9 1773 1773-09-01 08:00:00   bone  297 #> 595   26.3     10 1773 1773-10-01 18:00:00  blood  298 #> 596   26.3     10 1773 1773-10-01 18:00:00   bone  298 #> 597   40.9     11 1773 1773-11-01 03:59:59  blood  299 #> 598   40.9     11 1773 1773-11-01 03:59:59   bone  299 #> 599   43.2     12 1773 1773-12-01 14:00:00  blood  300 #> 600   43.2     12 1773 1773-12-01 14:00:00   bone  300 #> 601   46.8      1 1774 1774-01-01 00:00:00  blood  301 #> 602   46.8      1 1774 1774-01-01 00:00:00   bone  301 #> 603   65.4      2 1774 1774-01-31 09:59:59  blood  302 #> 604   65.4      2 1774 1774-01-31 09:59:59   bone  302 #> 605   55.7      3 1774 1774-03-02 20:00:00  blood  303 #> 606   55.7      3 1774 1774-03-02 20:00:00   bone  303 #> 607   43.8      4 1774 1774-04-02 06:00:00  blood  304 #> 608   43.8      4 1774 1774-04-02 06:00:00   bone  304 #> 609   51.3      5 1774 1774-05-02 15:59:59  blood  305 #> 610   51.3      5 1774 1774-05-02 15:59:59   bone  305 #> 611   28.5      6 1774 1774-06-02 02:00:00  blood  306 #> 612   28.5      6 1774 1774-06-02 02:00:00   bone  306 #> 613   17.5      7 1774 1774-07-02 12:00:00  blood  307 #> 614   17.5      7 1774 1774-07-02 12:00:00   bone  307 #> 615    6.6      8 1774 1774-08-01 21:59:59  blood  308 #> 616    6.6      8 1774 1774-08-01 21:59:59   bone  308 #> 617    7.9      9 1774 1774-09-01 08:00:00  blood  309 #> 618    7.9      9 1774 1774-09-01 08:00:00   bone  309 #> 619   14.0     10 1774 1774-10-01 18:00:00  blood  310 #> 620   14.0     10 1774 1774-10-01 18:00:00   bone  310 #> 621   17.7     11 1774 1774-11-01 03:59:59  blood  311 #> 622   17.7     11 1774 1774-11-01 03:59:59   bone  311 #> 623   12.2     12 1774 1774-12-01 14:00:00  blood  312 #> 624   12.2     12 1774 1774-12-01 14:00:00   bone  312 #> 625    4.4      1 1775 1775-01-01 00:00:00  blood  313 #> 626    4.4      1 1775 1775-01-01 00:00:00   bone  313 #> 627    0.0      2 1775 1775-01-31 09:59:59  blood  314 #> 628    0.0      2 1775 1775-01-31 09:59:59   bone  314 #> 629   11.6      3 1775 1775-03-02 20:00:00  blood  315 #> 630   11.6      3 1775 1775-03-02 20:00:00   bone  315 #> 631   11.2      4 1775 1775-04-02 06:00:00  blood  316 #> 632   11.2      4 1775 1775-04-02 06:00:00   bone  316 #> 633    3.9      5 1775 1775-05-02 15:59:59  blood  317 #> 634    3.9      5 1775 1775-05-02 15:59:59   bone  317 #> 635   12.3      6 1775 1775-06-02 02:00:00  blood  318 #> 636   12.3      6 1775 1775-06-02 02:00:00   bone  318 #> 637    1.0      7 1775 1775-07-02 12:00:00  blood  319 #> 638    1.0      7 1775 1775-07-02 12:00:00   bone  319 #> 639    7.9      8 1775 1775-08-01 21:59:59  blood  320 #> 640    7.9      8 1775 1775-08-01 21:59:59   bone  320 #> 641    3.2      9 1775 1775-09-01 08:00:00  blood  321 #> 642    3.2      9 1775 1775-09-01 08:00:00   bone  321 #> 643    5.6     10 1775 1775-10-01 18:00:00  blood  322 #> 644    5.6     10 1775 1775-10-01 18:00:00   bone  322 #> 645   15.1     11 1775 1775-11-01 03:59:59  blood  323 #> 646   15.1     11 1775 1775-11-01 03:59:59   bone  323 #> 647    7.9     12 1775 1775-12-01 14:00:00  blood  324 #> 648    7.9     12 1775 1775-12-01 14:00:00   bone  324 #> 649   21.7      1 1776 1776-01-01 00:00:00  blood  325 #> 650   21.7      1 1776 1776-01-01 00:00:00   bone  325 #> 651   11.6      2 1776 1776-01-31 11:59:59  blood  326 #> 652   11.6      2 1776 1776-01-31 11:59:59   bone  326 #> 653    6.3      3 1776 1776-03-02 00:00:00  blood  327 #> 654    6.3      3 1776 1776-03-02 00:00:00   bone  327 #> 655   21.8      4 1776 1776-04-01 12:00:00  blood  328 #> 656   21.8      4 1776 1776-04-01 12:00:00   bone  328 #> 657   11.2      5 1776 1776-05-01 23:59:59  blood  329 #> 658   11.2      5 1776 1776-05-01 23:59:59   bone  329 #> 659   19.0      6 1776 1776-06-01 12:00:00  blood  330 #> 660   19.0      6 1776 1776-06-01 12:00:00   bone  330 #> 661    1.0      7 1776 1776-07-02 00:00:00  blood  331 #> 662    1.0      7 1776 1776-07-02 00:00:00   bone  331 #> 663   24.2      8 1776 1776-08-01 11:59:59  blood  332 #> 664   24.2      8 1776 1776-08-01 11:59:59   bone  332 #> 665   16.0      9 1776 1776-09-01 00:00:00  blood  333 #> 666   16.0      9 1776 1776-09-01 00:00:00   bone  333 #> 667   30.0     10 1776 1776-10-01 12:00:00  blood  334 #> 668   30.0     10 1776 1776-10-01 12:00:00   bone  334 #> 669   35.0     11 1776 1776-10-31 23:59:59  blood  335 #> 670   35.0     11 1776 1776-10-31 23:59:59   bone  335 #> 671   40.0     12 1776 1776-12-01 12:00:00  blood  336 #> 672   40.0     12 1776 1776-12-01 12:00:00   bone  336 #> 673   45.0      1 1777 1777-01-01 00:00:00  blood  337 #> 674   45.0      1 1777 1777-01-01 00:00:00   bone  337 #> 675   36.5      2 1777 1777-01-31 09:59:59  blood  338 #> 676   36.5      2 1777 1777-01-31 09:59:59   bone  338 #> 677   39.0      3 1777 1777-03-02 20:00:00  blood  339 #> 678   39.0      3 1777 1777-03-02 20:00:00   bone  339 #> 679   95.5      4 1777 1777-04-02 06:00:00  blood  340 #> 680   95.5      4 1777 1777-04-02 06:00:00   bone  340 #> 681   80.3      5 1777 1777-05-02 15:59:59  blood  341 #> 682   80.3      5 1777 1777-05-02 15:59:59   bone  341 #> 683   80.7      6 1777 1777-06-02 02:00:00  blood  342 #> 684   80.7      6 1777 1777-06-02 02:00:00   bone  342 #> 685   95.0      7 1777 1777-07-02 12:00:00  blood  343 #> 686   95.0      7 1777 1777-07-02 12:00:00   bone  343 #> 687  112.0      8 1777 1777-08-01 21:59:59  blood  344 #> 688  112.0      8 1777 1777-08-01 21:59:59   bone  344 #> 689  116.2      9 1777 1777-09-01 08:00:00  blood  345 #> 690  116.2      9 1777 1777-09-01 08:00:00   bone  345 #> 691  106.5     10 1777 1777-10-01 18:00:00  blood  346 #> 692  106.5     10 1777 1777-10-01 18:00:00   bone  346 #> 693  146.0     11 1777 1777-11-01 03:59:59  blood  347 #> 694  146.0     11 1777 1777-11-01 03:59:59   bone  347 #> 695  157.3     12 1777 1777-12-01 14:00:00  blood  348 #> 696  157.3     12 1777 1777-12-01 14:00:00   bone  348 #> 697  177.3      1 1778 1778-01-01 00:00:00  blood  349 #> 698  177.3      1 1778 1778-01-01 00:00:00   bone  349 #> 699  109.3      2 1778 1778-01-31 09:59:59  blood  350 #> 700  109.3      2 1778 1778-01-31 09:59:59   bone  350 #> 701  134.0      3 1778 1778-03-02 20:00:00  blood  351 #> 702  134.0      3 1778 1778-03-02 20:00:00   bone  351 #> 703  145.0      4 1778 1778-04-02 06:00:00  blood  352 #> 704  145.0      4 1778 1778-04-02 06:00:00   bone  352 #> 705  238.9      5 1778 1778-05-02 15:59:59  blood  353 #> 706  238.9      5 1778 1778-05-02 15:59:59   bone  353 #> 707  171.6      6 1778 1778-06-02 02:00:00  blood  354 #> 708  171.6      6 1778 1778-06-02 02:00:00   bone  354 #> 709  153.0      7 1778 1778-07-02 12:00:00  blood  355 #> 710  153.0      7 1778 1778-07-02 12:00:00   bone  355 #> 711  140.0      8 1778 1778-08-01 21:59:59  blood  356 #> 712  140.0      8 1778 1778-08-01 21:59:59   bone  356 #> 713  171.7      9 1778 1778-09-01 08:00:00  blood  357 #> 714  171.7      9 1778 1778-09-01 08:00:00   bone  357 #> 715  156.3     10 1778 1778-10-01 18:00:00  blood  358 #> 716  156.3     10 1778 1778-10-01 18:00:00   bone  358 #> 717  150.3     11 1778 1778-11-01 03:59:59  blood  359 #> 718  150.3     11 1778 1778-11-01 03:59:59   bone  359 #> 719  105.0     12 1778 1778-12-01 14:00:00  blood  360 #> 720  105.0     12 1778 1778-12-01 14:00:00   bone  360 #> 721  114.7      1 1779 1779-01-01 00:00:00  blood  361 #> 722  114.7      1 1779 1779-01-01 00:00:00   bone  361 #> 723  165.7      2 1779 1779-01-31 09:59:59  blood  362 #> 724  165.7      2 1779 1779-01-31 09:59:59   bone  362 #> 725  118.0      3 1779 1779-03-02 20:00:00  blood  363 #> 726  118.0      3 1779 1779-03-02 20:00:00   bone  363 #> 727  145.0      4 1779 1779-04-02 06:00:00  blood  364 #> 728  145.0      4 1779 1779-04-02 06:00:00   bone  364 #> 729  140.0      5 1779 1779-05-02 15:59:59  blood  365 #> 730  140.0      5 1779 1779-05-02 15:59:59   bone  365 #> 731  113.7      6 1779 1779-06-02 02:00:00  blood  366 #> 732  113.7      6 1779 1779-06-02 02:00:00   bone  366 #> 733  143.0      7 1779 1779-07-02 12:00:00  blood  367 #> 734  143.0      7 1779 1779-07-02 12:00:00   bone  367 #> 735  112.0      8 1779 1779-08-01 21:59:59  blood  368 #> 736  112.0      8 1779 1779-08-01 21:59:59   bone  368 #> 737  111.0      9 1779 1779-09-01 08:00:00  blood  369 #> 738  111.0      9 1779 1779-09-01 08:00:00   bone  369 #> 739  124.0     10 1779 1779-10-01 18:00:00  blood  370 #> 740  124.0     10 1779 1779-10-01 18:00:00   bone  370 #> 741  114.0     11 1779 1779-11-01 03:59:59  blood  371 #> 742  114.0     11 1779 1779-11-01 03:59:59   bone  371 #> 743  110.0     12 1779 1779-12-01 14:00:00  blood  372 #> 744  110.0     12 1779 1779-12-01 14:00:00   bone  372 #> 745   70.0      1 1780 1780-01-01 00:00:00  blood  373 #> 746   70.0      1 1780 1780-01-01 00:00:00   bone  373 #> 747   98.0      2 1780 1780-01-31 11:59:59  blood  374 #> 748   98.0      2 1780 1780-01-31 11:59:59   bone  374 #> 749   98.0      3 1780 1780-03-02 00:00:00  blood  375 #> 750   98.0      3 1780 1780-03-02 00:00:00   bone  375 #> 751   95.0      4 1780 1780-04-01 12:00:00  blood  376 #> 752   95.0      4 1780 1780-04-01 12:00:00   bone  376 #> 753  107.2      5 1780 1780-05-01 23:59:59  blood  377 #> 754  107.2      5 1780 1780-05-01 23:59:59   bone  377 #> 755   88.0      6 1780 1780-06-01 12:00:00  blood  378 #> 756   88.0      6 1780 1780-06-01 12:00:00   bone  378 #> 757   86.0      7 1780 1780-07-02 00:00:00  blood  379 #> 758   86.0      7 1780 1780-07-02 00:00:00   bone  379 #> 759   86.0      8 1780 1780-08-01 11:59:59  blood  380 #> 760   86.0      8 1780 1780-08-01 11:59:59   bone  380 #> 761   93.7      9 1780 1780-09-01 00:00:00  blood  381 #> 762   93.7      9 1780 1780-09-01 00:00:00   bone  381 #> 763   77.0     10 1780 1780-10-01 12:00:00  blood  382 #> 764   77.0     10 1780 1780-10-01 12:00:00   bone  382 #> 765   60.0     11 1780 1780-10-31 23:59:59  blood  383 #> 766   60.0     11 1780 1780-10-31 23:59:59   bone  383 #> 767   58.7     12 1780 1780-12-01 12:00:00  blood  384 #> 768   58.7     12 1780 1780-12-01 12:00:00   bone  384 #> 769   98.7      1 1781 1781-01-01 00:00:00  blood  385 #> 770   98.7      1 1781 1781-01-01 00:00:00   bone  385 #> 771   74.7      2 1781 1781-01-31 09:59:59  blood  386 #> 772   74.7      2 1781 1781-01-31 09:59:59   bone  386 #> 773   53.0      3 1781 1781-03-02 20:00:00  blood  387 #> 774   53.0      3 1781 1781-03-02 20:00:00   bone  387 #> 775   68.3      4 1781 1781-04-02 06:00:00  blood  388 #> 776   68.3      4 1781 1781-04-02 06:00:00   bone  388 #> 777  104.7      5 1781 1781-05-02 15:59:59  blood  389 #> 778  104.7      5 1781 1781-05-02 15:59:59   bone  389 #> 779   97.7      6 1781 1781-06-02 02:00:00  blood  390 #> 780   97.7      6 1781 1781-06-02 02:00:00   bone  390 #> 781   73.5      7 1781 1781-07-02 12:00:00  blood  391 #> 782   73.5      7 1781 1781-07-02 12:00:00   bone  391 #> 783   66.0      8 1781 1781-08-01 21:59:59  blood  392 #> 784   66.0      8 1781 1781-08-01 21:59:59   bone  392 #> 785   51.0      9 1781 1781-09-01 08:00:00  blood  393 #> 786   51.0      9 1781 1781-09-01 08:00:00   bone  393 #> 787   27.3     10 1781 1781-10-01 18:00:00  blood  394 #> 788   27.3     10 1781 1781-10-01 18:00:00   bone  394 #> 789   67.0     11 1781 1781-11-01 03:59:59  blood  395 #> 790   67.0     11 1781 1781-11-01 03:59:59   bone  395 #> 791   35.2     12 1781 1781-12-01 14:00:00  blood  396 #> 792   35.2     12 1781 1781-12-01 14:00:00   bone  396 #> 793   54.0      1 1782 1782-01-01 00:00:00  blood  397 #> 794   54.0      1 1782 1782-01-01 00:00:00   bone  397 #> 795   37.5      2 1782 1782-01-31 09:59:59  blood  398 #> 796   37.5      2 1782 1782-01-31 09:59:59   bone  398 #> 797   37.0      3 1782 1782-03-02 20:00:00  blood  399 #> 798   37.0      3 1782 1782-03-02 20:00:00   bone  399 #> 799   41.0      4 1782 1782-04-02 06:00:00  blood  400 #> 800   41.0      4 1782 1782-04-02 06:00:00   bone  400 #> 801   54.3      5 1782 1782-05-02 15:59:59  blood  401 #> 802   54.3      5 1782 1782-05-02 15:59:59   bone  401 #> 803   38.0      6 1782 1782-06-02 02:00:00  blood  402 #> 804   38.0      6 1782 1782-06-02 02:00:00   bone  402 #> 805   37.0      7 1782 1782-07-02 12:00:00  blood  403 #> 806   37.0      7 1782 1782-07-02 12:00:00   bone  403 #> 807   44.0      8 1782 1782-08-01 21:59:59  blood  404 #> 808   44.0      8 1782 1782-08-01 21:59:59   bone  404 #> 809   34.0      9 1782 1782-09-01 08:00:00  blood  405 #> 810   34.0      9 1782 1782-09-01 08:00:00   bone  405 #> 811   23.2     10 1782 1782-10-01 18:00:00  blood  406 #> 812   23.2     10 1782 1782-10-01 18:00:00   bone  406 #> 813   31.5     11 1782 1782-11-01 03:59:59  blood  407 #> 814   31.5     11 1782 1782-11-01 03:59:59   bone  407 #> 815   30.0     12 1782 1782-12-01 14:00:00  blood  408 #> 816   30.0     12 1782 1782-12-01 14:00:00   bone  408 #> 817   28.0      1 1783 1783-01-01 00:00:00  blood  409 #> 818   28.0      1 1783 1783-01-01 00:00:00   bone  409 #> 819   38.7      2 1783 1783-01-31 09:59:59  blood  410 #> 820   38.7      2 1783 1783-01-31 09:59:59   bone  410 #> 821   26.7      3 1783 1783-03-02 20:00:00  blood  411 #> 822   26.7      3 1783 1783-03-02 20:00:00   bone  411 #> 823   28.3      4 1783 1783-04-02 06:00:00  blood  412 #> 824   28.3      4 1783 1783-04-02 06:00:00   bone  412 #> 825   23.0      5 1783 1783-05-02 15:59:59  blood  413 #> 826   23.0      5 1783 1783-05-02 15:59:59   bone  413 #> 827   25.2      6 1783 1783-06-02 02:00:00  blood  414 #> 828   25.2      6 1783 1783-06-02 02:00:00   bone  414 #> 829   32.2      7 1783 1783-07-02 12:00:00  blood  415 #> 830   32.2      7 1783 1783-07-02 12:00:00   bone  415 #> 831   20.0      8 1783 1783-08-01 21:59:59  blood  416 #> 832   20.0      8 1783 1783-08-01 21:59:59   bone  416 #> 833   18.0      9 1783 1783-09-01 08:00:00  blood  417 #> 834   18.0      9 1783 1783-09-01 08:00:00   bone  417 #> 835    8.0     10 1783 1783-10-01 18:00:00  blood  418 #> 836    8.0     10 1783 1783-10-01 18:00:00   bone  418 #> 837   15.0     11 1783 1783-11-01 03:59:59  blood  419 #> 838   15.0     11 1783 1783-11-01 03:59:59   bone  419 #> 839   10.5     12 1783 1783-12-01 14:00:00  blood  420 #> 840   10.5     12 1783 1783-12-01 14:00:00   bone  420 #> 841   13.0      1 1784 1784-01-01 00:00:00  blood  421 #> 842   13.0      1 1784 1784-01-01 00:00:00   bone  421 #> 843    8.0      2 1784 1784-01-31 11:59:59  blood  422 #> 844    8.0      2 1784 1784-01-31 11:59:59   bone  422 #> 845   11.0      3 1784 1784-03-02 00:00:00  blood  423 #> 846   11.0      3 1784 1784-03-02 00:00:00   bone  423 #> 847   10.0      4 1784 1784-04-01 12:00:00  blood  424 #> 848   10.0      4 1784 1784-04-01 12:00:00   bone  424 #> 849    6.0      5 1784 1784-05-01 23:59:59  blood  425 #> 850    6.0      5 1784 1784-05-01 23:59:59   bone  425 #> 851    9.0      6 1784 1784-06-01 12:00:00  blood  426 #> 852    9.0      6 1784 1784-06-01 12:00:00   bone  426 #> 853    6.0      7 1784 1784-07-02 00:00:00  blood  427 #> 854    6.0      7 1784 1784-07-02 00:00:00   bone  427 #> 855   10.0      8 1784 1784-08-01 11:59:59  blood  428 #> 856   10.0      8 1784 1784-08-01 11:59:59   bone  428 #> 857   10.0      9 1784 1784-09-01 00:00:00  blood  429 #> 858   10.0      9 1784 1784-09-01 00:00:00   bone  429 #> 859    8.0     10 1784 1784-10-01 12:00:00  blood  430 #> 860    8.0     10 1784 1784-10-01 12:00:00   bone  430 #> 861   17.0     11 1784 1784-10-31 23:59:59  blood  431 #> 862   17.0     11 1784 1784-10-31 23:59:59   bone  431 #> 863   14.0     12 1784 1784-12-01 12:00:00  blood  432 #> 864   14.0     12 1784 1784-12-01 12:00:00   bone  432 #> 865    6.5      1 1785 1785-01-01 00:00:00  blood  433 #> 866    6.5      1 1785 1785-01-01 00:00:00   bone  433 #> 867    8.0      2 1785 1785-01-31 09:59:59  blood  434 #> 868    8.0      2 1785 1785-01-31 09:59:59   bone  434 #> 869    9.0      3 1785 1785-03-02 20:00:00  blood  435 #> 870    9.0      3 1785 1785-03-02 20:00:00   bone  435 #> 871   15.7      4 1785 1785-04-02 06:00:00  blood  436 #> 872   15.7      4 1785 1785-04-02 06:00:00   bone  436 #> 873   20.7      5 1785 1785-05-02 15:59:59  blood  437 #> 874   20.7      5 1785 1785-05-02 15:59:59   bone  437 #> 875   26.3      6 1785 1785-06-02 02:00:00  blood  438 #> 876   26.3      6 1785 1785-06-02 02:00:00   bone  438 #> 877   36.3      7 1785 1785-07-02 12:00:00  blood  439 #> 878   36.3      7 1785 1785-07-02 12:00:00   bone  439 #> 879   20.0      8 1785 1785-08-01 21:59:59  blood  440 #> 880   20.0      8 1785 1785-08-01 21:59:59   bone  440 #> 881   32.0      9 1785 1785-09-01 08:00:00  blood  441 #> 882   32.0      9 1785 1785-09-01 08:00:00   bone  441 #> 883   47.2     10 1785 1785-10-01 18:00:00  blood  442 #> 884   47.2     10 1785 1785-10-01 18:00:00   bone  442 #> 885   40.2     11 1785 1785-11-01 03:59:59  blood  443 #> 886   40.2     11 1785 1785-11-01 03:59:59   bone  443 #> 887   27.3     12 1785 1785-12-01 14:00:00  blood  444 #> 888   27.3     12 1785 1785-12-01 14:00:00   bone  444 #> 889   37.2      1 1786 1786-01-01 00:00:00  blood  445 #> 890   37.2      1 1786 1786-01-01 00:00:00   bone  445 #> 891   47.6      2 1786 1786-01-31 09:59:59  blood  446 #> 892   47.6      2 1786 1786-01-31 09:59:59   bone  446 #> 893   47.7      3 1786 1786-03-02 20:00:00  blood  447 #> 894   47.7      3 1786 1786-03-02 20:00:00   bone  447 #> 895   85.4      4 1786 1786-04-02 06:00:00  blood  448 #> 896   85.4      4 1786 1786-04-02 06:00:00   bone  448 #> 897   92.3      5 1786 1786-05-02 15:59:59  blood  449 #> 898   92.3      5 1786 1786-05-02 15:59:59   bone  449 #> 899   59.0      6 1786 1786-06-02 02:00:00  blood  450 #> 900   59.0      6 1786 1786-06-02 02:00:00   bone  450 #> 901   83.0      7 1786 1786-07-02 12:00:00  blood  451 #> 902   83.0      7 1786 1786-07-02 12:00:00   bone  451 #> 903   89.7      8 1786 1786-08-01 21:59:59  blood  452 #> 904   89.7      8 1786 1786-08-01 21:59:59   bone  452 #> 905  111.5      9 1786 1786-09-01 08:00:00  blood  453 #> 906  111.5      9 1786 1786-09-01 08:00:00   bone  453 #> 907  112.3     10 1786 1786-10-01 18:00:00  blood  454 #> 908  112.3     10 1786 1786-10-01 18:00:00   bone  454 #> 909  116.0     11 1786 1786-11-01 03:59:59  blood  455 #> 910  116.0     11 1786 1786-11-01 03:59:59   bone  455 #> 911  112.7     12 1786 1786-12-01 14:00:00  blood  456 #> 912  112.7     12 1786 1786-12-01 14:00:00   bone  456 #> 913  134.7      1 1787 1787-01-01 00:00:00  blood  457 #> 914  134.7      1 1787 1787-01-01 00:00:00   bone  457 #> 915  106.0      2 1787 1787-01-31 09:59:59  blood  458 #> 916  106.0      2 1787 1787-01-31 09:59:59   bone  458 #> 917   87.4      3 1787 1787-03-02 20:00:00  blood  459 #> 918   87.4      3 1787 1787-03-02 20:00:00   bone  459 #> 919  127.2      4 1787 1787-04-02 06:00:00  blood  460 #> 920  127.2      4 1787 1787-04-02 06:00:00   bone  460 #> 921  134.8      5 1787 1787-05-02 15:59:59  blood  461 #> 922  134.8      5 1787 1787-05-02 15:59:59   bone  461 #> 923   99.2      6 1787 1787-06-02 02:00:00  blood  462 #> 924   99.2      6 1787 1787-06-02 02:00:00   bone  462 #> 925  128.0      7 1787 1787-07-02 12:00:00  blood  463 #> 926  128.0      7 1787 1787-07-02 12:00:00   bone  463 #> 927  137.2      8 1787 1787-08-01 21:59:59  blood  464 #> 928  137.2      8 1787 1787-08-01 21:59:59   bone  464 #> 929  157.3      9 1787 1787-09-01 08:00:00  blood  465 #> 930  157.3      9 1787 1787-09-01 08:00:00   bone  465 #> 931  157.0     10 1787 1787-10-01 18:00:00  blood  466 #> 932  157.0     10 1787 1787-10-01 18:00:00   bone  466 #> 933  141.5     11 1787 1787-11-01 03:59:59  blood  467 #> 934  141.5     11 1787 1787-11-01 03:59:59   bone  467 #> 935  174.0     12 1787 1787-12-01 14:00:00  blood  468 #> 936  174.0     12 1787 1787-12-01 14:00:00   bone  468 #> 937  138.0      1 1788 1788-01-01 00:00:00  blood  469 #> 938  138.0      1 1788 1788-01-01 00:00:00   bone  469 #> 939  129.2      2 1788 1788-01-31 11:59:59  blood  470 #> 940  129.2      2 1788 1788-01-31 11:59:59   bone  470 #> 941  143.3      3 1788 1788-03-02 00:00:00  blood  471 #> 942  143.3      3 1788 1788-03-02 00:00:00   bone  471 #> 943  108.5      4 1788 1788-04-01 12:00:00  blood  472 #> 944  108.5      4 1788 1788-04-01 12:00:00   bone  472 #> 945  113.0      5 1788 1788-05-01 23:59:59  blood  473 #> 946  113.0      5 1788 1788-05-01 23:59:59   bone  473 #> 947  154.2      6 1788 1788-06-01 12:00:00  blood  474 #> 948  154.2      6 1788 1788-06-01 12:00:00   bone  474 #> 949  141.5      7 1788 1788-07-02 00:00:00  blood  475 #> 950  141.5      7 1788 1788-07-02 00:00:00   bone  475 #> 951  136.0      8 1788 1788-08-01 11:59:59  blood  476 #> 952  136.0      8 1788 1788-08-01 11:59:59   bone  476 #> 953  141.0      9 1788 1788-09-01 00:00:00  blood  477 #> 954  141.0      9 1788 1788-09-01 00:00:00   bone  477 #> 955  142.0     10 1788 1788-10-01 12:00:00  blood  478 #> 956  142.0     10 1788 1788-10-01 12:00:00   bone  478 #> 957   94.7     11 1788 1788-10-31 23:59:59  blood  479 #> 958   94.7     11 1788 1788-10-31 23:59:59   bone  479 #> 959  129.5     12 1788 1788-12-01 12:00:00  blood  480 #> 960  129.5     12 1788 1788-12-01 12:00:00   bone  480 #> 961  114.0      1 1789 1789-01-01 00:00:00  blood  481 #> 962  114.0      1 1789 1789-01-01 00:00:00   bone  481 #> 963  125.3      2 1789 1789-01-31 09:59:59  blood  482 #> 964  125.3      2 1789 1789-01-31 09:59:59   bone  482 #> 965  120.0      3 1789 1789-03-02 20:00:00  blood  483 #> 966  120.0      3 1789 1789-03-02 20:00:00   bone  483 #> 967  123.3      4 1789 1789-04-02 06:00:00  blood  484 #> 968  123.3      4 1789 1789-04-02 06:00:00   bone  484 #> 969  123.5      5 1789 1789-05-02 15:59:59  blood  485 #> 970  123.5      5 1789 1789-05-02 15:59:59   bone  485 #> 971  120.0      6 1789 1789-06-02 02:00:00  blood  486 #> 972  120.0      6 1789 1789-06-02 02:00:00   bone  486 #> 973  117.0      7 1789 1789-07-02 12:00:00  blood  487 #> 974  117.0      7 1789 1789-07-02 12:00:00   bone  487 #> 975  103.0      8 1789 1789-08-01 21:59:59  blood  488 #> 976  103.0      8 1789 1789-08-01 21:59:59   bone  488 #> 977  112.0      9 1789 1789-09-01 08:00:00  blood  489 #> 978  112.0      9 1789 1789-09-01 08:00:00   bone  489 #> 979   89.7     10 1789 1789-10-01 18:00:00  blood  490 #> 980   89.7     10 1789 1789-10-01 18:00:00   bone  490 #> 981  134.0     11 1789 1789-11-01 03:59:59  blood  491 #> 982  134.0     11 1789 1789-11-01 03:59:59   bone  491 #> 983  135.5     12 1789 1789-12-01 14:00:00  blood  492 #> 984  135.5     12 1789 1789-12-01 14:00:00   bone  492 #> 985  103.0      1 1790 1790-01-01 00:00:00  blood  493 #> 986  103.0      1 1790 1790-01-01 00:00:00   bone  493 #> 987  127.5      2 1790 1790-01-31 09:59:59  blood  494 #> 988  127.5      2 1790 1790-01-31 09:59:59   bone  494 #> 989   96.3      3 1790 1790-03-02 20:00:00  blood  495 #> 990   96.3      3 1790 1790-03-02 20:00:00   bone  495 #> 991   94.0      4 1790 1790-04-02 06:00:00  blood  496 #> 992   94.0      4 1790 1790-04-02 06:00:00   bone  496 #> 993   93.0      5 1790 1790-05-02 15:59:59  blood  497 #> 994   93.0      5 1790 1790-05-02 15:59:59   bone  497 #> 995   91.0      6 1790 1790-06-02 02:00:00  blood  498 #> 996   91.0      6 1790 1790-06-02 02:00:00   bone  498 #> 997   69.3      7 1790 1790-07-02 12:00:00  blood  499 #> 998   69.3      7 1790 1790-07-02 12:00:00   bone  499 #> 999   87.0      8 1790 1790-08-01 21:59:59  blood  500 #> 1000  87.0      8 1790 1790-08-01 21:59:59   bone  500 #> 1001  77.3      9 1790 1790-09-01 08:00:00  blood  501 #> 1002  77.3      9 1790 1790-09-01 08:00:00   bone  501 #> 1003  84.3     10 1790 1790-10-01 18:00:00  blood  502 #> 1004  84.3     10 1790 1790-10-01 18:00:00   bone  502 #> 1005  82.0     11 1790 1790-11-01 03:59:59  blood  503 #> 1006  82.0     11 1790 1790-11-01 03:59:59   bone  503 #> 1007  74.0     12 1790 1790-12-01 14:00:00  blood  504 #> 1008  74.0     12 1790 1790-12-01 14:00:00   bone  504 #> 1009  72.7      1 1791 1791-01-01 00:00:00  blood  505 #> 1010  72.7      1 1791 1791-01-01 00:00:00   bone  505 #> 1011  62.0      2 1791 1791-01-31 09:59:59  blood  506 #> 1012  62.0      2 1791 1791-01-31 09:59:59   bone  506 #> 1013  74.0      3 1791 1791-03-02 20:00:00  blood  507 #> 1014  74.0      3 1791 1791-03-02 20:00:00   bone  507 #> 1015  77.2      4 1791 1791-04-02 06:00:00  blood  508 #> 1016  77.2      4 1791 1791-04-02 06:00:00   bone  508 #> 1017  73.7      5 1791 1791-05-02 15:59:59  blood  509 #> 1018  73.7      5 1791 1791-05-02 15:59:59   bone  509 #> 1019  64.2      6 1791 1791-06-02 02:00:00  blood  510 #> 1020  64.2      6 1791 1791-06-02 02:00:00   bone  510 #> 1021  71.0      7 1791 1791-07-02 12:00:00  blood  511 #> 1022  71.0      7 1791 1791-07-02 12:00:00   bone  511 #> 1023  43.0      8 1791 1791-08-01 21:59:59  blood  512 #> 1024  43.0      8 1791 1791-08-01 21:59:59   bone  512 #> 1025  66.5      9 1791 1791-09-01 08:00:00  blood  513 #> 1026  66.5      9 1791 1791-09-01 08:00:00   bone  513 #> 1027  61.7     10 1791 1791-10-01 18:00:00  blood  514 #> 1028  61.7     10 1791 1791-10-01 18:00:00   bone  514 #> 1029  67.0     11 1791 1791-11-01 03:59:59  blood  515 #> 1030  67.0     11 1791 1791-11-01 03:59:59   bone  515 #> 1031  66.0     12 1791 1791-12-01 14:00:00  blood  516 #> 1032  66.0     12 1791 1791-12-01 14:00:00   bone  516 #> 1033  58.0      1 1792 1792-01-01 00:00:00  blood  517 #> 1034  58.0      1 1792 1792-01-01 00:00:00   bone  517 #> 1035  64.0      2 1792 1792-01-31 11:59:59  blood  518 #> 1036  64.0      2 1792 1792-01-31 11:59:59   bone  518 #> 1037  63.0      3 1792 1792-03-02 00:00:00  blood  519 #> 1038  63.0      3 1792 1792-03-02 00:00:00   bone  519 #> 1039  75.7      4 1792 1792-04-01 12:00:00  blood  520 #> 1040  75.7      4 1792 1792-04-01 12:00:00   bone  520 #> 1041  62.0      5 1792 1792-05-01 23:59:59  blood  521 #> 1042  62.0      5 1792 1792-05-01 23:59:59   bone  521 #> 1043  61.0      6 1792 1792-06-01 12:00:00  blood  522 #> 1044  61.0      6 1792 1792-06-01 12:00:00   bone  522 #> 1045  45.8      7 1792 1792-07-02 00:00:00  blood  523 #> 1046  45.8      7 1792 1792-07-02 00:00:00   bone  523 #> 1047  60.0      8 1792 1792-08-01 11:59:59  blood  524 #> 1048  60.0      8 1792 1792-08-01 11:59:59   bone  524 #> 1049  59.0      9 1792 1792-09-01 00:00:00  blood  525 #> 1050  59.0      9 1792 1792-09-01 00:00:00   bone  525 #> 1051  59.0     10 1792 1792-10-01 12:00:00  blood  526 #> 1052  59.0     10 1792 1792-10-01 12:00:00   bone  526 #> 1053  57.0     11 1792 1792-10-31 23:59:59  blood  527 #> 1054  57.0     11 1792 1792-10-31 23:59:59   bone  527 #> 1055  56.0     12 1792 1792-12-01 12:00:00  blood  528 #> 1056  56.0     12 1792 1792-12-01 12:00:00   bone  528 #> 1057  56.0      1 1793 1793-01-01 00:00:00  blood  529 #> 1058  56.0      1 1793 1793-01-01 00:00:00   bone  529 #> 1059  55.0      2 1793 1793-01-31 09:59:59  blood  530 #> 1060  55.0      2 1793 1793-01-31 09:59:59   bone  530 #> 1061  55.5      3 1793 1793-03-02 20:00:00  blood  531 #> 1062  55.5      3 1793 1793-03-02 20:00:00   bone  531 #> 1063  53.0      4 1793 1793-04-02 06:00:00  blood  532 #> 1064  53.0      4 1793 1793-04-02 06:00:00   bone  532 #> 1065  52.3      5 1793 1793-05-02 15:59:59  blood  533 #> 1066  52.3      5 1793 1793-05-02 15:59:59   bone  533 #> 1067  51.0      6 1793 1793-06-02 02:00:00  blood  534 #> 1068  51.0      6 1793 1793-06-02 02:00:00   bone  534 #> 1069  50.0      7 1793 1793-07-02 12:00:00  blood  535 #> 1070  50.0      7 1793 1793-07-02 12:00:00   bone  535 #> 1071  29.3      8 1793 1793-08-01 21:59:59  blood  536 #> 1072  29.3      8 1793 1793-08-01 21:59:59   bone  536 #> 1073  24.0      9 1793 1793-09-01 08:00:00  blood  537 #> 1074  24.0      9 1793 1793-09-01 08:00:00   bone  537 #> 1075  47.0     10 1793 1793-10-01 18:00:00  blood  538 #> 1076  47.0     10 1793 1793-10-01 18:00:00   bone  538 #> 1077  44.0     11 1793 1793-11-01 03:59:59  blood  539 #> 1078  44.0     11 1793 1793-11-01 03:59:59   bone  539 #> 1079  45.7     12 1793 1793-12-01 14:00:00  blood  540 #> 1080  45.7     12 1793 1793-12-01 14:00:00   bone  540 #> 1081  45.0      1 1794 1794-01-01 00:00:00  blood  541 #> 1082  45.0      1 1794 1794-01-01 00:00:00   bone  541 #> 1083  44.0      2 1794 1794-01-31 09:59:59  blood  542 #> 1084  44.0      2 1794 1794-01-31 09:59:59   bone  542 #> 1085  38.0      3 1794 1794-03-02 20:00:00  blood  543 #> 1086  38.0      3 1794 1794-03-02 20:00:00   bone  543 #> 1087  28.4      4 1794 1794-04-02 06:00:00  blood  544 #> 1088  28.4      4 1794 1794-04-02 06:00:00   bone  544 #> 1089  55.7      5 1794 1794-05-02 15:59:59  blood  545 #> 1090  55.7      5 1794 1794-05-02 15:59:59   bone  545 #> 1091  41.5      6 1794 1794-06-02 02:00:00  blood  546 #> 1092  41.5      6 1794 1794-06-02 02:00:00   bone  546 #> 1093  41.0      7 1794 1794-07-02 12:00:00  blood  547 #> 1094  41.0      7 1794 1794-07-02 12:00:00   bone  547 #> 1095  40.0      8 1794 1794-08-01 21:59:59  blood  548 #> 1096  40.0      8 1794 1794-08-01 21:59:59   bone  548 #> 1097  11.1      9 1794 1794-09-01 08:00:00  blood  549 #> 1098  11.1      9 1794 1794-09-01 08:00:00   bone  549 #> 1099  28.5     10 1794 1794-10-01 18:00:00  blood  550 #> 1100  28.5     10 1794 1794-10-01 18:00:00   bone  550 #> 1101  67.4     11 1794 1794-11-01 03:59:59  blood  551 #> 1102  67.4     11 1794 1794-11-01 03:59:59   bone  551 #> 1103  51.4     12 1794 1794-12-01 14:00:00  blood  552 #> 1104  51.4     12 1794 1794-12-01 14:00:00   bone  552 #> 1105  21.4      1 1795 1795-01-01 00:00:00  blood  553 #> 1106  21.4      1 1795 1795-01-01 00:00:00   bone  553 #> 1107  39.9      2 1795 1795-01-31 09:59:59  blood  554 #> 1108  39.9      2 1795 1795-01-31 09:59:59   bone  554 #> 1109  12.6      3 1795 1795-03-02 20:00:00  blood  555 #> 1110  12.6      3 1795 1795-03-02 20:00:00   bone  555 #> 1111  18.6      4 1795 1795-04-02 06:00:00  blood  556 #> 1112  18.6      4 1795 1795-04-02 06:00:00   bone  556 #> 1113  31.0      5 1795 1795-05-02 15:59:59  blood  557 #> 1114  31.0      5 1795 1795-05-02 15:59:59   bone  557 #> 1115  17.1      6 1795 1795-06-02 02:00:00  blood  558 #> 1116  17.1      6 1795 1795-06-02 02:00:00   bone  558 #> 1117  12.9      7 1795 1795-07-02 12:00:00  blood  559 #> 1118  12.9      7 1795 1795-07-02 12:00:00   bone  559 #> 1119  25.7      8 1795 1795-08-01 21:59:59  blood  560 #> 1120  25.7      8 1795 1795-08-01 21:59:59   bone  560 #> 1121  13.5      9 1795 1795-09-01 08:00:00  blood  561 #> 1122  13.5      9 1795 1795-09-01 08:00:00   bone  561 #> 1123  19.5     10 1795 1795-10-01 18:00:00  blood  562 #> 1124  19.5     10 1795 1795-10-01 18:00:00   bone  562 #> 1125  25.0     11 1795 1795-11-01 03:59:59  blood  563 #> 1126  25.0     11 1795 1795-11-01 03:59:59   bone  563 #> 1127  18.0     12 1795 1795-12-01 14:00:00  blood  564 #> 1128  18.0     12 1795 1795-12-01 14:00:00   bone  564 #> 1129  22.0      1 1796 1796-01-01 00:00:00  blood  565 #> 1130  22.0      1 1796 1796-01-01 00:00:00   bone  565 #> 1131  23.8      2 1796 1796-01-31 11:59:59  blood  566 #> 1132  23.8      2 1796 1796-01-31 11:59:59   bone  566 #> 1133  15.7      3 1796 1796-03-02 00:00:00  blood  567 #> 1134  15.7      3 1796 1796-03-02 00:00:00   bone  567 #> 1135  31.7      4 1796 1796-04-01 12:00:00  blood  568 #> 1136  31.7      4 1796 1796-04-01 12:00:00   bone  568 #> 1137  21.0      5 1796 1796-05-01 23:59:59  blood  569 #> 1138  21.0      5 1796 1796-05-01 23:59:59   bone  569 #> 1139   6.7      6 1796 1796-06-01 12:00:00  blood  570 #> 1140   6.7      6 1796 1796-06-01 12:00:00   bone  570 #> 1141  26.9      7 1796 1796-07-02 00:00:00  blood  571 #> 1142  26.9      7 1796 1796-07-02 00:00:00   bone  571 #> 1143   1.5      8 1796 1796-08-01 11:59:59  blood  572 #> 1144   1.5      8 1796 1796-08-01 11:59:59   bone  572 #> 1145  18.4      9 1796 1796-09-01 00:00:00  blood  573 #> 1146  18.4      9 1796 1796-09-01 00:00:00   bone  573 #> 1147  11.0     10 1796 1796-10-01 12:00:00  blood  574 #> 1148  11.0     10 1796 1796-10-01 12:00:00   bone  574 #> 1149   8.4     11 1796 1796-10-31 23:59:59  blood  575 #> 1150   8.4     11 1796 1796-10-31 23:59:59   bone  575 #> 1151   5.1     12 1796 1796-12-01 12:00:00  blood  576 #> 1152   5.1     12 1796 1796-12-01 12:00:00   bone  576 #> 1153  14.4      1 1797 1797-01-01 00:00:00  blood  577 #> 1154  14.4      1 1797 1797-01-01 00:00:00   bone  577 #> 1155   4.2      2 1797 1797-01-31 09:59:59  blood  578 #> 1156   4.2      2 1797 1797-01-31 09:59:59   bone  578 #> 1157   4.0      3 1797 1797-03-02 20:00:00  blood  579 #> 1158   4.0      3 1797 1797-03-02 20:00:00   bone  579 #> 1159   4.0      4 1797 1797-04-02 06:00:00  blood  580 #> 1160   4.0      4 1797 1797-04-02 06:00:00   bone  580 #> 1161   7.3      5 1797 1797-05-02 15:59:59  blood  581 #> 1162   7.3      5 1797 1797-05-02 15:59:59   bone  581 #> 1163  11.1      6 1797 1797-06-02 02:00:00  blood  582 #> 1164  11.1      6 1797 1797-06-02 02:00:00   bone  582 #> 1165   4.3      7 1797 1797-07-02 12:00:00  blood  583 #> 1166   4.3      7 1797 1797-07-02 12:00:00   bone  583 #> 1167   6.0      8 1797 1797-08-01 21:59:59  blood  584 #> 1168   6.0      8 1797 1797-08-01 21:59:59   bone  584 #> 1169   5.7      9 1797 1797-09-01 08:00:00  blood  585 #> 1170   5.7      9 1797 1797-09-01 08:00:00   bone  585 #> 1171   6.9     10 1797 1797-10-01 18:00:00  blood  586 #> 1172   6.9     10 1797 1797-10-01 18:00:00   bone  586 #> 1173   5.8     11 1797 1797-11-01 03:59:59  blood  587 #> 1174   5.8     11 1797 1797-11-01 03:59:59   bone  587 #> 1175   3.0     12 1797 1797-12-01 14:00:00  blood  588 #> 1176   3.0     12 1797 1797-12-01 14:00:00   bone  588 #> 1177   2.0      1 1798 1798-01-01 00:00:00  blood  589 #> 1178   2.0      1 1798 1798-01-01 00:00:00   bone  589 #> 1179   4.0      2 1798 1798-01-31 09:59:59  blood  590 #> 1180   4.0      2 1798 1798-01-31 09:59:59   bone  590 #> 1181  12.4      3 1798 1798-03-02 20:00:00  blood  591 #> 1182  12.4      3 1798 1798-03-02 20:00:00   bone  591 #> 1183   1.1      4 1798 1798-04-02 06:00:00  blood  592 #> 1184   1.1      4 1798 1798-04-02 06:00:00   bone  592 #> 1185   0.0      5 1798 1798-05-02 15:59:59  blood  593 #> 1186   0.0      5 1798 1798-05-02 15:59:59   bone  593 #> 1187   0.0      6 1798 1798-06-02 02:00:00  blood  594 #> 1188   0.0      6 1798 1798-06-02 02:00:00   bone  594 #> 1189   0.0      7 1798 1798-07-02 12:00:00  blood  595 #> 1190   0.0      7 1798 1798-07-02 12:00:00   bone  595 #> 1191   3.0      8 1798 1798-08-01 21:59:59  blood  596 #> 1192   3.0      8 1798 1798-08-01 21:59:59   bone  596 #> 1193   2.4      9 1798 1798-09-01 08:00:00  blood  597 #> 1194   2.4      9 1798 1798-09-01 08:00:00   bone  597 #> 1195   1.5     10 1798 1798-10-01 18:00:00  blood  598 #> 1196   1.5     10 1798 1798-10-01 18:00:00   bone  598 #> 1197  12.5     11 1798 1798-11-01 03:59:59  blood  599 #> 1198  12.5     11 1798 1798-11-01 03:59:59   bone  599 #> 1199   9.9     12 1798 1798-12-01 14:00:00  blood  600 #> 1200   9.9     12 1798 1798-12-01 14:00:00   bone  600 #> 1201   1.6      1 1799 1799-01-01 00:00:00  blood  601 #> 1202   1.6      1 1799 1799-01-01 00:00:00   bone  601 #> 1203  12.6      2 1799 1799-01-31 09:59:59  blood  602 #> 1204  12.6      2 1799 1799-01-31 09:59:59   bone  602 #> 1205  21.7      3 1799 1799-03-02 20:00:00  blood  603 #> 1206  21.7      3 1799 1799-03-02 20:00:00   bone  603 #> 1207   8.4      4 1799 1799-04-02 06:00:00  blood  604 #> 1208   8.4      4 1799 1799-04-02 06:00:00   bone  604 #> 1209   8.2      5 1799 1799-05-02 15:59:59  blood  605 #> 1210   8.2      5 1799 1799-05-02 15:59:59   bone  605 #> 1211  10.6      6 1799 1799-06-02 02:00:00  blood  606 #> 1212  10.6      6 1799 1799-06-02 02:00:00   bone  606 #> 1213   2.1      7 1799 1799-07-02 12:00:00  blood  607 #> 1214   2.1      7 1799 1799-07-02 12:00:00   bone  607 #> 1215   0.0      8 1799 1799-08-01 21:59:59  blood  608 #> 1216   0.0      8 1799 1799-08-01 21:59:59   bone  608 #> 1217   0.0      9 1799 1799-09-01 08:00:00  blood  609 #> 1218   0.0      9 1799 1799-09-01 08:00:00   bone  609 #> 1219   4.6     10 1799 1799-10-01 18:00:00  blood  610 #> 1220   4.6     10 1799 1799-10-01 18:00:00   bone  610 #> 1221   2.7     11 1799 1799-11-01 03:59:59  blood  611 #> 1222   2.7     11 1799 1799-11-01 03:59:59   bone  611 #> 1223   8.6     12 1799 1799-12-01 14:00:00  blood  612 #> 1224   8.6     12 1799 1799-12-01 14:00:00   bone  612 #> 1225   6.9      1 1800 1800-01-01 00:00:00  blood  613 #> 1226   6.9      1 1800 1800-01-01 00:00:00   bone  613 #> 1227   9.3      2 1800 1800-01-31 09:59:59  blood  614 #> 1228   9.3      2 1800 1800-01-31 09:59:59   bone  614 #> 1229  13.9      3 1800 1800-03-02 20:00:00  blood  615 #> 1230  13.9      3 1800 1800-03-02 20:00:00   bone  615 #> 1231   0.0      4 1800 1800-04-02 06:00:00  blood  616 #> 1232   0.0      4 1800 1800-04-02 06:00:00   bone  616 #> 1233   5.0      5 1800 1800-05-02 15:59:59  blood  617 #> 1234   5.0      5 1800 1800-05-02 15:59:59   bone  617 #> 1235  23.7      6 1800 1800-06-02 02:00:00  blood  618 #> 1236  23.7      6 1800 1800-06-02 02:00:00   bone  618 #> 1237  21.0      7 1800 1800-07-02 12:00:00  blood  619 #> 1238  21.0      7 1800 1800-07-02 12:00:00   bone  619 #> 1239  19.5      8 1800 1800-08-01 21:59:59  blood  620 #> 1240  19.5      8 1800 1800-08-01 21:59:59   bone  620 #> 1241  11.5      9 1800 1800-09-01 08:00:00  blood  621 #> 1242  11.5      9 1800 1800-09-01 08:00:00   bone  621 #> 1243  12.3     10 1800 1800-10-01 18:00:00  blood  622 #> 1244  12.3     10 1800 1800-10-01 18:00:00   bone  622 #> 1245  10.5     11 1800 1800-11-01 03:59:59  blood  623 #> 1246  10.5     11 1800 1800-11-01 03:59:59   bone  623 #> 1247  40.1     12 1800 1800-12-01 14:00:00  blood  624 #> 1248  40.1     12 1800 1800-12-01 14:00:00   bone  624 #> 1249  27.0      1 1801 1801-01-01 00:00:00  blood  625 #> 1250  27.0      1 1801 1801-01-01 00:00:00   bone  625 #> 1251  29.0      2 1801 1801-01-31 09:59:59  blood  626 #> 1252  29.0      2 1801 1801-01-31 09:59:59   bone  626 #> 1253  30.0      3 1801 1801-03-02 20:00:00  blood  627 #> 1254  30.0      3 1801 1801-03-02 20:00:00   bone  627 #> 1255  31.0      4 1801 1801-04-02 06:00:00  blood  628 #> 1256  31.0      4 1801 1801-04-02 06:00:00   bone  628 #> 1257  32.0      5 1801 1801-05-02 15:59:59  blood  629 #> 1258  32.0      5 1801 1801-05-02 15:59:59   bone  629 #> 1259  31.2      6 1801 1801-06-02 02:00:00  blood  630 #> 1260  31.2      6 1801 1801-06-02 02:00:00   bone  630 #> 1261  35.0      7 1801 1801-07-02 12:00:00  blood  631 #> 1262  35.0      7 1801 1801-07-02 12:00:00   bone  631 #> 1263  38.7      8 1801 1801-08-01 21:59:59  blood  632 #> 1264  38.7      8 1801 1801-08-01 21:59:59   bone  632 #> 1265  33.5      9 1801 1801-09-01 08:00:00  blood  633 #> 1266  33.5      9 1801 1801-09-01 08:00:00   bone  633 #> 1267  32.6     10 1801 1801-10-01 18:00:00  blood  634 #> 1268  32.6     10 1801 1801-10-01 18:00:00   bone  634 #> 1269  39.8     11 1801 1801-11-01 03:59:59  blood  635 #> 1270  39.8     11 1801 1801-11-01 03:59:59   bone  635 #> 1271  48.2     12 1801 1801-12-01 14:00:00  blood  636 #> 1272  48.2     12 1801 1801-12-01 14:00:00   bone  636 #> 1273  47.8      1 1802 1802-01-01 00:00:00  blood  637 #> 1274  47.8      1 1802 1802-01-01 00:00:00   bone  637 #> 1275  47.0      2 1802 1802-01-31 09:59:59  blood  638 #> 1276  47.0      2 1802 1802-01-31 09:59:59   bone  638 #> 1277  40.8      3 1802 1802-03-02 20:00:00  blood  639 #> 1278  40.8      3 1802 1802-03-02 20:00:00   bone  639 #> 1279  42.0      4 1802 1802-04-02 06:00:00  blood  640 #> 1280  42.0      4 1802 1802-04-02 06:00:00   bone  640 #> 1281  44.0      5 1802 1802-05-02 15:59:59  blood  641 #> 1282  44.0      5 1802 1802-05-02 15:59:59   bone  641 #> 1283  46.0      6 1802 1802-06-02 02:00:00  blood  642 #> 1284  46.0      6 1802 1802-06-02 02:00:00   bone  642 #> 1285  48.0      7 1802 1802-07-02 12:00:00  blood  643 #> 1286  48.0      7 1802 1802-07-02 12:00:00   bone  643 #> 1287  50.0      8 1802 1802-08-01 21:59:59  blood  644 #> 1288  50.0      8 1802 1802-08-01 21:59:59   bone  644 #> 1289  51.8      9 1802 1802-09-01 08:00:00  blood  645 #> 1290  51.8      9 1802 1802-09-01 08:00:00   bone  645 #> 1291  38.5     10 1802 1802-10-01 18:00:00  blood  646 #> 1292  38.5     10 1802 1802-10-01 18:00:00   bone  646 #> 1293  34.5     11 1802 1802-11-01 03:59:59  blood  647 #> 1294  34.5     11 1802 1802-11-01 03:59:59   bone  647 #> 1295  50.0     12 1802 1802-12-01 14:00:00  blood  648 #> 1296  50.0     12 1802 1802-12-01 14:00:00   bone  648 #> 1297  50.0      1 1803 1803-01-01 00:00:00  blood  649 #> 1298  50.0      1 1803 1803-01-01 00:00:00   bone  649 #> 1299  50.8      2 1803 1803-01-31 09:59:59  blood  650 #> 1300  50.8      2 1803 1803-01-31 09:59:59   bone  650 #> 1301  29.5      3 1803 1803-03-02 20:00:00  blood  651 #> 1302  29.5      3 1803 1803-03-02 20:00:00   bone  651 #> 1303  25.0      4 1803 1803-04-02 06:00:00  blood  652 #> 1304  25.0      4 1803 1803-04-02 06:00:00   bone  652 #> 1305  44.3      5 1803 1803-05-02 15:59:59  blood  653 #> 1306  44.3      5 1803 1803-05-02 15:59:59   bone  653 #> 1307  36.0      6 1803 1803-06-02 02:00:00  blood  654 #> 1308  36.0      6 1803 1803-06-02 02:00:00   bone  654 #> 1309  48.3      7 1803 1803-07-02 12:00:00  blood  655 #> 1310  48.3      7 1803 1803-07-02 12:00:00   bone  655 #> 1311  34.1      8 1803 1803-08-01 21:59:59  blood  656 #> 1312  34.1      8 1803 1803-08-01 21:59:59   bone  656 #> 1313  45.3      9 1803 1803-09-01 08:00:00  blood  657 #> 1314  45.3      9 1803 1803-09-01 08:00:00   bone  657 #> 1315  54.3     10 1803 1803-10-01 18:00:00  blood  658 #> 1316  54.3     10 1803 1803-10-01 18:00:00   bone  658 #> 1317  51.0     11 1803 1803-11-01 03:59:59  blood  659 #> 1318  51.0     11 1803 1803-11-01 03:59:59   bone  659 #> 1319  48.0     12 1803 1803-12-01 14:00:00  blood  660 #> 1320  48.0     12 1803 1803-12-01 14:00:00   bone  660 #> 1321  45.3      1 1804 1804-01-01 00:00:00  blood  661 #> 1322  45.3      1 1804 1804-01-01 00:00:00   bone  661 #> 1323  48.3      2 1804 1804-01-31 11:59:59  blood  662 #> 1324  48.3      2 1804 1804-01-31 11:59:59   bone  662 #> 1325  48.0      3 1804 1804-03-02 00:00:00  blood  663 #> 1326  48.0      3 1804 1804-03-02 00:00:00   bone  663 #> 1327  50.6      4 1804 1804-04-01 12:00:00  blood  664 #> 1328  50.6      4 1804 1804-04-01 12:00:00   bone  664 #> 1329  33.4      5 1804 1804-05-01 23:59:59  blood  665 #> 1330  33.4      5 1804 1804-05-01 23:59:59   bone  665 #> 1331  34.8      6 1804 1804-06-01 12:00:00  blood  666 #> 1332  34.8      6 1804 1804-06-01 12:00:00   bone  666 #> 1333  29.8      7 1804 1804-07-02 00:00:00  blood  667 #> 1334  29.8      7 1804 1804-07-02 00:00:00   bone  667 #> 1335  43.1      8 1804 1804-08-01 11:59:59  blood  668 #> 1336  43.1      8 1804 1804-08-01 11:59:59   bone  668 #> 1337  53.0      9 1804 1804-09-01 00:00:00  blood  669 #> 1338  53.0      9 1804 1804-09-01 00:00:00   bone  669 #> 1339  62.3     10 1804 1804-10-01 12:00:00  blood  670 #> 1340  62.3     10 1804 1804-10-01 12:00:00   bone  670 #> 1341  61.0     11 1804 1804-10-31 23:59:59  blood  671 #> 1342  61.0     11 1804 1804-10-31 23:59:59   bone  671 #> 1343  60.0     12 1804 1804-12-01 12:00:00  blood  672 #> 1344  60.0     12 1804 1804-12-01 12:00:00   bone  672 #> 1345  61.0      1 1805 1805-01-01 00:00:00  blood  673 #> 1346  61.0      1 1805 1805-01-01 00:00:00   bone  673 #> 1347  44.1      2 1805 1805-01-31 09:59:59  blood  674 #> 1348  44.1      2 1805 1805-01-31 09:59:59   bone  674 #> 1349  51.4      3 1805 1805-03-02 20:00:00  blood  675 #> 1350  51.4      3 1805 1805-03-02 20:00:00   bone  675 #> 1351  37.5      4 1805 1805-04-02 06:00:00  blood  676 #> 1352  37.5      4 1805 1805-04-02 06:00:00   bone  676 #> 1353  39.0      5 1805 1805-05-02 15:59:59  blood  677 #> 1354  39.0      5 1805 1805-05-02 15:59:59   bone  677 #> 1355  40.5      6 1805 1805-06-02 02:00:00  blood  678 #> 1356  40.5      6 1805 1805-06-02 02:00:00   bone  678 #> 1357  37.6      7 1805 1805-07-02 12:00:00  blood  679 #> 1358  37.6      7 1805 1805-07-02 12:00:00   bone  679 #> 1359  42.7      8 1805 1805-08-01 21:59:59  blood  680 #> 1360  42.7      8 1805 1805-08-01 21:59:59   bone  680 #> 1361  44.4      9 1805 1805-09-01 08:00:00  blood  681 #> 1362  44.4      9 1805 1805-09-01 08:00:00   bone  681 #> 1363  29.4     10 1805 1805-10-01 18:00:00  blood  682 #> 1364  29.4     10 1805 1805-10-01 18:00:00   bone  682 #> 1365  41.0     11 1805 1805-11-01 03:59:59  blood  683 #> 1366  41.0     11 1805 1805-11-01 03:59:59   bone  683 #> 1367  38.3     12 1805 1805-12-01 14:00:00  blood  684 #> 1368  38.3     12 1805 1805-12-01 14:00:00   bone  684 #> 1369  39.0      1 1806 1806-01-01 00:00:00  blood  685 #> 1370  39.0      1 1806 1806-01-01 00:00:00   bone  685 #> 1371  29.6      2 1806 1806-01-31 09:59:59  blood  686 #> 1372  29.6      2 1806 1806-01-31 09:59:59   bone  686 #> 1373  32.7      3 1806 1806-03-02 20:00:00  blood  687 #> 1374  32.7      3 1806 1806-03-02 20:00:00   bone  687 #> 1375  27.7      4 1806 1806-04-02 06:00:00  blood  688 #> 1376  27.7      4 1806 1806-04-02 06:00:00   bone  688 #> 1377  26.4      5 1806 1806-05-02 15:59:59  blood  689 #> 1378  26.4      5 1806 1806-05-02 15:59:59   bone  689 #> 1379  25.6      6 1806 1806-06-02 02:00:00  blood  690 #> 1380  25.6      6 1806 1806-06-02 02:00:00   bone  690 #> 1381  30.0      7 1806 1806-07-02 12:00:00  blood  691 #> 1382  30.0      7 1806 1806-07-02 12:00:00   bone  691 #> 1383  26.3      8 1806 1806-08-01 21:59:59  blood  692 #> 1384  26.3      8 1806 1806-08-01 21:59:59   bone  692 #> 1385  24.0      9 1806 1806-09-01 08:00:00  blood  693 #> 1386  24.0      9 1806 1806-09-01 08:00:00   bone  693 #> 1387  27.0     10 1806 1806-10-01 18:00:00  blood  694 #> 1388  27.0     10 1806 1806-10-01 18:00:00   bone  694 #> 1389  25.0     11 1806 1806-11-01 03:59:59  blood  695 #> 1390  25.0     11 1806 1806-11-01 03:59:59   bone  695 #> 1391  24.0     12 1806 1806-12-01 14:00:00  blood  696 #> 1392  24.0     12 1806 1806-12-01 14:00:00   bone  696 #> 1393  12.0      1 1807 1807-01-01 00:00:00  blood  697 #> 1394  12.0      1 1807 1807-01-01 00:00:00   bone  697 #> 1395  12.2      2 1807 1807-01-31 09:59:59  blood  698 #> 1396  12.2      2 1807 1807-01-31 09:59:59   bone  698 #> 1397   9.6      3 1807 1807-03-02 20:00:00  blood  699 #> 1398   9.6      3 1807 1807-03-02 20:00:00   bone  699 #> 1399  23.8      4 1807 1807-04-02 06:00:00  blood  700 #> 1400  23.8      4 1807 1807-04-02 06:00:00   bone  700 #> 1401  10.0      5 1807 1807-05-02 15:59:59  blood  701 #> 1402  10.0      5 1807 1807-05-02 15:59:59   bone  701 #> 1403  12.0      6 1807 1807-06-02 02:00:00  blood  702 #> 1404  12.0      6 1807 1807-06-02 02:00:00   bone  702 #> 1405  12.7      7 1807 1807-07-02 12:00:00  blood  703 #> 1406  12.7      7 1807 1807-07-02 12:00:00   bone  703 #> 1407  12.0      8 1807 1807-08-01 21:59:59  blood  704 #> 1408  12.0      8 1807 1807-08-01 21:59:59   bone  704 #> 1409   5.7      9 1807 1807-09-01 08:00:00  blood  705 #> 1410   5.7      9 1807 1807-09-01 08:00:00   bone  705 #> 1411   8.0     10 1807 1807-10-01 18:00:00  blood  706 #> 1412   8.0     10 1807 1807-10-01 18:00:00   bone  706 #> 1413   2.6     11 1807 1807-11-01 03:59:59  blood  707 #> 1414   2.6     11 1807 1807-11-01 03:59:59   bone  707 #> 1415   0.0     12 1807 1807-12-01 14:00:00  blood  708 #> 1416   0.0     12 1807 1807-12-01 14:00:00   bone  708 #> 1417   0.0      1 1808 1808-01-01 00:00:00  blood  709 #> 1418   0.0      1 1808 1808-01-01 00:00:00   bone  709 #> 1419   4.5      2 1808 1808-01-31 11:59:59  blood  710 #> 1420   4.5      2 1808 1808-01-31 11:59:59   bone  710 #> 1421   0.0      3 1808 1808-03-02 00:00:00  blood  711 #> 1422   0.0      3 1808 1808-03-02 00:00:00   bone  711 #> 1423  12.3      4 1808 1808-04-01 12:00:00  blood  712 #> 1424  12.3      4 1808 1808-04-01 12:00:00   bone  712 #> 1425  13.5      5 1808 1808-05-01 23:59:59  blood  713 #> 1426  13.5      5 1808 1808-05-01 23:59:59   bone  713 #> 1427  13.5      6 1808 1808-06-01 12:00:00  blood  714 #> 1428  13.5      6 1808 1808-06-01 12:00:00   bone  714 #> 1429   6.7      7 1808 1808-07-02 00:00:00  blood  715 #> 1430   6.7      7 1808 1808-07-02 00:00:00   bone  715 #> 1431   8.0      8 1808 1808-08-01 11:59:59  blood  716 #> 1432   8.0      8 1808 1808-08-01 11:59:59   bone  716 #> 1433  11.7      9 1808 1808-09-01 00:00:00  blood  717 #> 1434  11.7      9 1808 1808-09-01 00:00:00   bone  717 #> 1435   4.7     10 1808 1808-10-01 12:00:00  blood  718 #> 1436   4.7     10 1808 1808-10-01 12:00:00   bone  718 #> 1437  10.5     11 1808 1808-10-31 23:59:59  blood  719 #> 1438  10.5     11 1808 1808-10-31 23:59:59   bone  719 #> 1439  12.3     12 1808 1808-12-01 12:00:00  blood  720 #> 1440  12.3     12 1808 1808-12-01 12:00:00   bone  720 #> 1441   7.2      1 1809 1809-01-01 00:00:00  blood  721 #> 1442   7.2      1 1809 1809-01-01 00:00:00   bone  721 #> 1443   9.2      2 1809 1809-01-31 09:59:59  blood  722 #> 1444   9.2      2 1809 1809-01-31 09:59:59   bone  722 #> 1445   0.9      3 1809 1809-03-02 20:00:00  blood  723 #> 1446   0.9      3 1809 1809-03-02 20:00:00   bone  723 #> 1447   2.5      4 1809 1809-04-02 06:00:00  blood  724 #> 1448   2.5      4 1809 1809-04-02 06:00:00   bone  724 #> 1449   2.0      5 1809 1809-05-02 15:59:59  blood  725 #> 1450   2.0      5 1809 1809-05-02 15:59:59   bone  725 #> 1451   7.7      6 1809 1809-06-02 02:00:00  blood  726 #> 1452   7.7      6 1809 1809-06-02 02:00:00   bone  726 #> 1453   0.3      7 1809 1809-07-02 12:00:00  blood  727 #> 1454   0.3      7 1809 1809-07-02 12:00:00   bone  727 #> 1455   0.2      8 1809 1809-08-01 21:59:59  blood  728 #> 1456   0.2      8 1809 1809-08-01 21:59:59   bone  728 #> 1457   0.4      9 1809 1809-09-01 08:00:00  blood  729 #> 1458   0.4      9 1809 1809-09-01 08:00:00   bone  729 #> 1459   0.0     10 1809 1809-10-01 18:00:00  blood  730 #> 1460   0.0     10 1809 1809-10-01 18:00:00   bone  730 #> 1461   0.0     11 1809 1809-11-01 03:59:59  blood  731 #> 1462   0.0     11 1809 1809-11-01 03:59:59   bone  731 #> 1463   0.0     12 1809 1809-12-01 14:00:00  blood  732 #> 1464   0.0     12 1809 1809-12-01 14:00:00   bone  732 #> 1465   0.0      1 1810 1810-01-01 00:00:00  blood  733 #> 1466   0.0      1 1810 1810-01-01 00:00:00   bone  733 #> 1467   0.0      2 1810 1810-01-31 09:59:59  blood  734 #> 1468   0.0      2 1810 1810-01-31 09:59:59   bone  734 #> 1469   0.0      3 1810 1810-03-02 20:00:00  blood  735 #> 1470   0.0      3 1810 1810-03-02 20:00:00   bone  735 #> 1471   0.0      4 1810 1810-04-02 06:00:00  blood  736 #> 1472   0.0      4 1810 1810-04-02 06:00:00   bone  736 #> 1473   0.0      5 1810 1810-05-02 15:59:59  blood  737 #> 1474   0.0      5 1810 1810-05-02 15:59:59   bone  737 #> 1475   0.0      6 1810 1810-06-02 02:00:00  blood  738 #> 1476   0.0      6 1810 1810-06-02 02:00:00   bone  738 #> 1477   0.0      7 1810 1810-07-02 12:00:00  blood  739 #> 1478   0.0      7 1810 1810-07-02 12:00:00   bone  739 #> 1479   0.0      8 1810 1810-08-01 21:59:59  blood  740 #> 1480   0.0      8 1810 1810-08-01 21:59:59   bone  740 #> 1481   0.0      9 1810 1810-09-01 08:00:00  blood  741 #> 1482   0.0      9 1810 1810-09-01 08:00:00   bone  741 #> 1483   0.0     10 1810 1810-10-01 18:00:00  blood  742 #> 1484   0.0     10 1810 1810-10-01 18:00:00   bone  742 #> 1485   0.0     11 1810 1810-11-01 03:59:59  blood  743 #> 1486   0.0     11 1810 1810-11-01 03:59:59   bone  743 #> 1487   0.0     12 1810 1810-12-01 14:00:00  blood  744 #> 1488   0.0     12 1810 1810-12-01 14:00:00   bone  744 #> 1489   0.0      1 1811 1811-01-01 00:00:00  blood  745 #> 1490   0.0      1 1811 1811-01-01 00:00:00   bone  745 #> 1491   0.0      2 1811 1811-01-31 09:59:59  blood  746 #> 1492   0.0      2 1811 1811-01-31 09:59:59   bone  746 #> 1493   0.0      3 1811 1811-03-02 20:00:00  blood  747 #> 1494   0.0      3 1811 1811-03-02 20:00:00   bone  747 #> 1495   0.0      4 1811 1811-04-02 06:00:00  blood  748 #> 1496   0.0      4 1811 1811-04-02 06:00:00   bone  748 #> 1497   0.0      5 1811 1811-05-02 15:59:59  blood  749 #> 1498   0.0      5 1811 1811-05-02 15:59:59   bone  749 #> 1499   0.0      6 1811 1811-06-02 02:00:00  blood  750 #> 1500   0.0      6 1811 1811-06-02 02:00:00   bone  750 #> 1501   6.6      7 1811 1811-07-02 12:00:00  blood  751 #> 1502   6.6      7 1811 1811-07-02 12:00:00   bone  751 #> 1503   0.0      8 1811 1811-08-01 21:59:59  blood  752 #> 1504   0.0      8 1811 1811-08-01 21:59:59   bone  752 #> 1505   2.4      9 1811 1811-09-01 08:00:00  blood  753 #> 1506   2.4      9 1811 1811-09-01 08:00:00   bone  753 #> 1507   6.1     10 1811 1811-10-01 18:00:00  blood  754 #> 1508   6.1     10 1811 1811-10-01 18:00:00   bone  754 #> 1509   0.8     11 1811 1811-11-01 03:59:59  blood  755 #> 1510   0.8     11 1811 1811-11-01 03:59:59   bone  755 #> 1511   1.1     12 1811 1811-12-01 14:00:00  blood  756 #> 1512   1.1     12 1811 1811-12-01 14:00:00   bone  756 #> 1513  11.3      1 1812 1812-01-01 00:00:00  blood  757 #> 1514  11.3      1 1812 1812-01-01 00:00:00   bone  757 #> 1515   1.9      2 1812 1812-01-31 11:59:59  blood  758 #> 1516   1.9      2 1812 1812-01-31 11:59:59   bone  758 #> 1517   0.7      3 1812 1812-03-02 00:00:00  blood  759 #> 1518   0.7      3 1812 1812-03-02 00:00:00   bone  759 #> 1519   0.0      4 1812 1812-04-01 12:00:00  blood  760 #> 1520   0.0      4 1812 1812-04-01 12:00:00   bone  760 #> 1521   1.0      5 1812 1812-05-01 23:59:59  blood  761 #> 1522   1.0      5 1812 1812-05-01 23:59:59   bone  761 #> 1523   1.3      6 1812 1812-06-01 12:00:00  blood  762 #> 1524   1.3      6 1812 1812-06-01 12:00:00   bone  762 #> 1525   0.5      7 1812 1812-07-02 00:00:00  blood  763 #> 1526   0.5      7 1812 1812-07-02 00:00:00   bone  763 #> 1527  15.6      8 1812 1812-08-01 11:59:59  blood  764 #> 1528  15.6      8 1812 1812-08-01 11:59:59   bone  764 #> 1529   5.2      9 1812 1812-09-01 00:00:00  blood  765 #> 1530   5.2      9 1812 1812-09-01 00:00:00   bone  765 #> 1531   3.9     10 1812 1812-10-01 12:00:00  blood  766 #> 1532   3.9     10 1812 1812-10-01 12:00:00   bone  766 #> 1533   7.9     11 1812 1812-10-31 23:59:59  blood  767 #> 1534   7.9     11 1812 1812-10-31 23:59:59   bone  767 #> 1535  10.1     12 1812 1812-12-01 12:00:00  blood  768 #> 1536  10.1     12 1812 1812-12-01 12:00:00   bone  768 #> 1537   0.0      1 1813 1813-01-01 00:00:00  blood  769 #> 1538   0.0      1 1813 1813-01-01 00:00:00   bone  769 #> 1539  10.3      2 1813 1813-01-31 09:59:59  blood  770 #> 1540  10.3      2 1813 1813-01-31 09:59:59   bone  770 #> 1541   1.9      3 1813 1813-03-02 20:00:00  blood  771 #> 1542   1.9      3 1813 1813-03-02 20:00:00   bone  771 #> 1543  16.6      4 1813 1813-04-02 06:00:00  blood  772 #> 1544  16.6      4 1813 1813-04-02 06:00:00   bone  772 #> 1545   5.5      5 1813 1813-05-02 15:59:59  blood  773 #> 1546   5.5      5 1813 1813-05-02 15:59:59   bone  773 #> 1547  11.2      6 1813 1813-06-02 02:00:00  blood  774 #> 1548  11.2      6 1813 1813-06-02 02:00:00   bone  774 #> 1549  18.3      7 1813 1813-07-02 12:00:00  blood  775 #> 1550  18.3      7 1813 1813-07-02 12:00:00   bone  775 #> 1551   8.4      8 1813 1813-08-01 21:59:59  blood  776 #> 1552   8.4      8 1813 1813-08-01 21:59:59   bone  776 #> 1553  15.3      9 1813 1813-09-01 08:00:00  blood  777 #> 1554  15.3      9 1813 1813-09-01 08:00:00   bone  777 #> 1555  27.8     10 1813 1813-10-01 18:00:00  blood  778 #> 1556  27.8     10 1813 1813-10-01 18:00:00   bone  778 #> 1557  16.7     11 1813 1813-11-01 03:59:59  blood  779 #> 1558  16.7     11 1813 1813-11-01 03:59:59   bone  779 #> 1559  14.3     12 1813 1813-12-01 14:00:00  blood  780 #> 1560  14.3     12 1813 1813-12-01 14:00:00   bone  780 #> 1561  22.2      1 1814 1814-01-01 00:00:00  blood  781 #> 1562  22.2      1 1814 1814-01-01 00:00:00   bone  781 #> 1563  12.0      2 1814 1814-01-31 09:59:59  blood  782 #> 1564  12.0      2 1814 1814-01-31 09:59:59   bone  782 #> 1565   5.7      3 1814 1814-03-02 20:00:00  blood  783 #> 1566   5.7      3 1814 1814-03-02 20:00:00   bone  783 #> 1567  23.8      4 1814 1814-04-02 06:00:00  blood  784 #> 1568  23.8      4 1814 1814-04-02 06:00:00   bone  784 #> 1569   5.8      5 1814 1814-05-02 15:59:59  blood  785 #> 1570   5.8      5 1814 1814-05-02 15:59:59   bone  785 #> 1571  14.9      6 1814 1814-06-02 02:00:00  blood  786 #> 1572  14.9      6 1814 1814-06-02 02:00:00   bone  786 #> 1573  18.5      7 1814 1814-07-02 12:00:00  blood  787 #> 1574  18.5      7 1814 1814-07-02 12:00:00   bone  787 #> 1575   2.3      8 1814 1814-08-01 21:59:59  blood  788 #> 1576   2.3      8 1814 1814-08-01 21:59:59   bone  788 #> 1577   8.1      9 1814 1814-09-01 08:00:00  blood  789 #> 1578   8.1      9 1814 1814-09-01 08:00:00   bone  789 #> 1579  19.3     10 1814 1814-10-01 18:00:00  blood  790 #> 1580  19.3     10 1814 1814-10-01 18:00:00   bone  790 #> 1581  14.5     11 1814 1814-11-01 03:59:59  blood  791 #> 1582  14.5     11 1814 1814-11-01 03:59:59   bone  791 #> 1583  20.1     12 1814 1814-12-01 14:00:00  blood  792 #> 1584  20.1     12 1814 1814-12-01 14:00:00   bone  792 #> 1585  19.2      1 1815 1815-01-01 00:00:00  blood  793 #> 1586  19.2      1 1815 1815-01-01 00:00:00   bone  793 #> 1587  32.2      2 1815 1815-01-31 09:59:59  blood  794 #> 1588  32.2      2 1815 1815-01-31 09:59:59   bone  794 #> 1589  26.2      3 1815 1815-03-02 20:00:00  blood  795 #> 1590  26.2      3 1815 1815-03-02 20:00:00   bone  795 #> 1591  31.6      4 1815 1815-04-02 06:00:00  blood  796 #> 1592  31.6      4 1815 1815-04-02 06:00:00   bone  796 #> 1593   9.8      5 1815 1815-05-02 15:59:59  blood  797 #> 1594   9.8      5 1815 1815-05-02 15:59:59   bone  797 #> 1595  55.9      6 1815 1815-06-02 02:00:00  blood  798 #> 1596  55.9      6 1815 1815-06-02 02:00:00   bone  798 #> 1597  35.5      7 1815 1815-07-02 12:00:00  blood  799 #> 1598  35.5      7 1815 1815-07-02 12:00:00   bone  799 #> 1599  47.2      8 1815 1815-08-01 21:59:59  blood  800 #> 1600  47.2      8 1815 1815-08-01 21:59:59   bone  800 #> 1601  31.5      9 1815 1815-09-01 08:00:00  blood  801 #> 1602  31.5      9 1815 1815-09-01 08:00:00   bone  801 #> 1603  33.5     10 1815 1815-10-01 18:00:00  blood  802 #> 1604  33.5     10 1815 1815-10-01 18:00:00   bone  802 #> 1605  37.2     11 1815 1815-11-01 03:59:59  blood  803 #> 1606  37.2     11 1815 1815-11-01 03:59:59   bone  803 #> 1607  65.0     12 1815 1815-12-01 14:00:00  blood  804 #> 1608  65.0     12 1815 1815-12-01 14:00:00   bone  804 #> 1609  26.3      1 1816 1816-01-01 00:00:00  blood  805 #> 1610  26.3      1 1816 1816-01-01 00:00:00   bone  805 #> 1611  68.8      2 1816 1816-01-31 11:59:59  blood  806 #> 1612  68.8      2 1816 1816-01-31 11:59:59   bone  806 #> 1613  73.7      3 1816 1816-03-02 00:00:00  blood  807 #> 1614  73.7      3 1816 1816-03-02 00:00:00   bone  807 #> 1615  58.8      4 1816 1816-04-01 12:00:00  blood  808 #> 1616  58.8      4 1816 1816-04-01 12:00:00   bone  808 #> 1617  44.3      5 1816 1816-05-01 23:59:59  blood  809 #> 1618  44.3      5 1816 1816-05-01 23:59:59   bone  809 #> 1619  43.6      6 1816 1816-06-01 12:00:00  blood  810 #> 1620  43.6      6 1816 1816-06-01 12:00:00   bone  810 #> 1621  38.8      7 1816 1816-07-02 00:00:00  blood  811 #> 1622  38.8      7 1816 1816-07-02 00:00:00   bone  811 #> 1623  23.2      8 1816 1816-08-01 11:59:59  blood  812 #> 1624  23.2      8 1816 1816-08-01 11:59:59   bone  812 #> 1625  47.8      9 1816 1816-09-01 00:00:00  blood  813 #> 1626  47.8      9 1816 1816-09-01 00:00:00   bone  813 #> 1627  56.4     10 1816 1816-10-01 12:00:00  blood  814 #> 1628  56.4     10 1816 1816-10-01 12:00:00   bone  814 #> 1629  38.1     11 1816 1816-10-31 23:59:59  blood  815 #> 1630  38.1     11 1816 1816-10-31 23:59:59   bone  815 #> 1631  29.9     12 1816 1816-12-01 12:00:00  blood  816 #> 1632  29.9     12 1816 1816-12-01 12:00:00   bone  816 #> 1633  36.4      1 1817 1817-01-01 00:00:00  blood  817 #> 1634  36.4      1 1817 1817-01-01 00:00:00   bone  817 #> 1635  57.9      2 1817 1817-01-31 09:59:59  blood  818 #> 1636  57.9      2 1817 1817-01-31 09:59:59   bone  818 #> 1637  96.2      3 1817 1817-03-02 20:00:00  blood  819 #> 1638  96.2      3 1817 1817-03-02 20:00:00   bone  819 #> 1639  26.4      4 1817 1817-04-02 06:00:00  blood  820 #> 1640  26.4      4 1817 1817-04-02 06:00:00   bone  820 #> 1641  21.2      5 1817 1817-05-02 15:59:59  blood  821 #> 1642  21.2      5 1817 1817-05-02 15:59:59   bone  821 #> 1643  40.0      6 1817 1817-06-02 02:00:00  blood  822 #> 1644  40.0      6 1817 1817-06-02 02:00:00   bone  822 #> 1645  50.0      7 1817 1817-07-02 12:00:00  blood  823 #> 1646  50.0      7 1817 1817-07-02 12:00:00   bone  823 #> 1647  45.0      8 1817 1817-08-01 21:59:59  blood  824 #> 1648  45.0      8 1817 1817-08-01 21:59:59   bone  824 #> 1649  36.7      9 1817 1817-09-01 08:00:00  blood  825 #> 1650  36.7      9 1817 1817-09-01 08:00:00   bone  825 #> 1651  25.6     10 1817 1817-10-01 18:00:00  blood  826 #> 1652  25.6     10 1817 1817-10-01 18:00:00   bone  826 #> 1653  28.9     11 1817 1817-11-01 03:59:59  blood  827 #> 1654  28.9     11 1817 1817-11-01 03:59:59   bone  827 #> 1655  28.4     12 1817 1817-12-01 14:00:00  blood  828 #> 1656  28.4     12 1817 1817-12-01 14:00:00   bone  828 #> 1657  34.9      1 1818 1818-01-01 00:00:00  blood  829 #> 1658  34.9      1 1818 1818-01-01 00:00:00   bone  829 #> 1659  22.4      2 1818 1818-01-31 09:59:59  blood  830 #> 1660  22.4      2 1818 1818-01-31 09:59:59   bone  830 #> 1661  25.4      3 1818 1818-03-02 20:00:00  blood  831 #> 1662  25.4      3 1818 1818-03-02 20:00:00   bone  831 #> 1663  34.5      4 1818 1818-04-02 06:00:00  blood  832 #> 1664  34.5      4 1818 1818-04-02 06:00:00   bone  832 #> 1665  53.1      5 1818 1818-05-02 15:59:59  blood  833 #> 1666  53.1      5 1818 1818-05-02 15:59:59   bone  833 #> 1667  36.4      6 1818 1818-06-02 02:00:00  blood  834 #> 1668  36.4      6 1818 1818-06-02 02:00:00   bone  834 #> 1669  28.0      7 1818 1818-07-02 12:00:00  blood  835 #> 1670  28.0      7 1818 1818-07-02 12:00:00   bone  835 #> 1671  31.5      8 1818 1818-08-01 21:59:59  blood  836 #> 1672  31.5      8 1818 1818-08-01 21:59:59   bone  836 #> 1673  26.1      9 1818 1818-09-01 08:00:00  blood  837 #> 1674  26.1      9 1818 1818-09-01 08:00:00   bone  837 #> 1675  31.7     10 1818 1818-10-01 18:00:00  blood  838 #> 1676  31.7     10 1818 1818-10-01 18:00:00   bone  838 #> 1677  10.9     11 1818 1818-11-01 03:59:59  blood  839 #> 1678  10.9     11 1818 1818-11-01 03:59:59   bone  839 #> 1679  25.8     12 1818 1818-12-01 14:00:00  blood  840 #> 1680  25.8     12 1818 1818-12-01 14:00:00   bone  840 #> 1681  32.5      1 1819 1819-01-01 00:00:00  blood  841 #> 1682  32.5      1 1819 1819-01-01 00:00:00   bone  841 #> 1683  20.7      2 1819 1819-01-31 09:59:59  blood  842 #> 1684  20.7      2 1819 1819-01-31 09:59:59   bone  842 #> 1685   3.7      3 1819 1819-03-02 20:00:00  blood  843 #> 1686   3.7      3 1819 1819-03-02 20:00:00   bone  843 #> 1687  20.2      4 1819 1819-04-02 06:00:00  blood  844 #> 1688  20.2      4 1819 1819-04-02 06:00:00   bone  844 #> 1689  19.6      5 1819 1819-05-02 15:59:59  blood  845 #> 1690  19.6      5 1819 1819-05-02 15:59:59   bone  845 #> 1691  35.0      6 1819 1819-06-02 02:00:00  blood  846 #> 1692  35.0      6 1819 1819-06-02 02:00:00   bone  846 #> 1693  31.4      7 1819 1819-07-02 12:00:00  blood  847 #> 1694  31.4      7 1819 1819-07-02 12:00:00   bone  847 #> 1695  26.1      8 1819 1819-08-01 21:59:59  blood  848 #> 1696  26.1      8 1819 1819-08-01 21:59:59   bone  848 #> 1697  14.9      9 1819 1819-09-01 08:00:00  blood  849 #> 1698  14.9      9 1819 1819-09-01 08:00:00   bone  849 #> 1699  27.5     10 1819 1819-10-01 18:00:00  blood  850 #> 1700  27.5     10 1819 1819-10-01 18:00:00   bone  850 #> 1701  25.1     11 1819 1819-11-01 03:59:59  blood  851 #> 1702  25.1     11 1819 1819-11-01 03:59:59   bone  851 #> 1703  30.6     12 1819 1819-12-01 14:00:00  blood  852 #> 1704  30.6     12 1819 1819-12-01 14:00:00   bone  852 #> 1705  19.2      1 1820 1820-01-01 00:00:00  blood  853 #> 1706  19.2      1 1820 1820-01-01 00:00:00   bone  853 #> 1707  26.6      2 1820 1820-01-31 11:59:59  blood  854 #> 1708  26.6      2 1820 1820-01-31 11:59:59   bone  854 #> 1709   4.5      3 1820 1820-03-02 00:00:00  blood  855 #> 1710   4.5      3 1820 1820-03-02 00:00:00   bone  855 #> 1711  19.4      4 1820 1820-04-01 12:00:00  blood  856 #> 1712  19.4      4 1820 1820-04-01 12:00:00   bone  856 #> 1713  29.3      5 1820 1820-05-01 23:59:59  blood  857 #> 1714  29.3      5 1820 1820-05-01 23:59:59   bone  857 #> 1715  10.8      6 1820 1820-06-01 12:00:00  blood  858 #> 1716  10.8      6 1820 1820-06-01 12:00:00   bone  858 #> 1717  20.6      7 1820 1820-07-02 00:00:00  blood  859 #> 1718  20.6      7 1820 1820-07-02 00:00:00   bone  859 #> 1719  25.9      8 1820 1820-08-01 11:59:59  blood  860 #> 1720  25.9      8 1820 1820-08-01 11:59:59   bone  860 #> 1721   5.2      9 1820 1820-09-01 00:00:00  blood  861 #> 1722   5.2      9 1820 1820-09-01 00:00:00   bone  861 #> 1723   9.0     10 1820 1820-10-01 12:00:00  blood  862 #> 1724   9.0     10 1820 1820-10-01 12:00:00   bone  862 #> 1725   7.9     11 1820 1820-10-31 23:59:59  blood  863 #> 1726   7.9     11 1820 1820-10-31 23:59:59   bone  863 #> 1727   9.7     12 1820 1820-12-01 12:00:00  blood  864 #> 1728   9.7     12 1820 1820-12-01 12:00:00   bone  864 #> 1729  21.5      1 1821 1821-01-01 00:00:00  blood  865 #> 1730  21.5      1 1821 1821-01-01 00:00:00   bone  865 #> 1731   4.3      2 1821 1821-01-31 09:59:59  blood  866 #> 1732   4.3      2 1821 1821-01-31 09:59:59   bone  866 #> 1733   5.7      3 1821 1821-03-02 20:00:00  blood  867 #> 1734   5.7      3 1821 1821-03-02 20:00:00   bone  867 #> 1735   9.2      4 1821 1821-04-02 06:00:00  blood  868 #> 1736   9.2      4 1821 1821-04-02 06:00:00   bone  868 #> 1737   1.7      5 1821 1821-05-02 15:59:59  blood  869 #> 1738   1.7      5 1821 1821-05-02 15:59:59   bone  869 #> 1739   1.8      6 1821 1821-06-02 02:00:00  blood  870 #> 1740   1.8      6 1821 1821-06-02 02:00:00   bone  870 #> 1741   2.5      7 1821 1821-07-02 12:00:00  blood  871 #> 1742   2.5      7 1821 1821-07-02 12:00:00   bone  871 #> 1743   4.8      8 1821 1821-08-01 21:59:59  blood  872 #> 1744   4.8      8 1821 1821-08-01 21:59:59   bone  872 #> 1745   4.4      9 1821 1821-09-01 08:00:00  blood  873 #> 1746   4.4      9 1821 1821-09-01 08:00:00   bone  873 #> 1747  18.8     10 1821 1821-10-01 18:00:00  blood  874 #> 1748  18.8     10 1821 1821-10-01 18:00:00   bone  874 #> 1749   4.4     11 1821 1821-11-01 03:59:59  blood  875 #> 1750   4.4     11 1821 1821-11-01 03:59:59   bone  875 #> 1751   0.0     12 1821 1821-12-01 14:00:00  blood  876 #> 1752   0.0     12 1821 1821-12-01 14:00:00   bone  876 #> 1753   0.0      1 1822 1822-01-01 00:00:00  blood  877 #> 1754   0.0      1 1822 1822-01-01 00:00:00   bone  877 #> 1755   0.9      2 1822 1822-01-31 09:59:59  blood  878 #> 1756   0.9      2 1822 1822-01-31 09:59:59   bone  878 #> 1757  16.1      3 1822 1822-03-02 20:00:00  blood  879 #> 1758  16.1      3 1822 1822-03-02 20:00:00   bone  879 #> 1759  13.5      4 1822 1822-04-02 06:00:00  blood  880 #> 1760  13.5      4 1822 1822-04-02 06:00:00   bone  880 #> 1761   1.5      5 1822 1822-05-02 15:59:59  blood  881 #> 1762   1.5      5 1822 1822-05-02 15:59:59   bone  881 #> 1763   5.6      6 1822 1822-06-02 02:00:00  blood  882 #> 1764   5.6      6 1822 1822-06-02 02:00:00   bone  882 #> 1765   7.9      7 1822 1822-07-02 12:00:00  blood  883 #> 1766   7.9      7 1822 1822-07-02 12:00:00   bone  883 #> 1767   2.1      8 1822 1822-08-01 21:59:59  blood  884 #> 1768   2.1      8 1822 1822-08-01 21:59:59   bone  884 #> 1769   0.0      9 1822 1822-09-01 08:00:00  blood  885 #> 1770   0.0      9 1822 1822-09-01 08:00:00   bone  885 #> 1771   0.4     10 1822 1822-10-01 18:00:00  blood  886 #> 1772   0.4     10 1822 1822-10-01 18:00:00   bone  886 #> 1773   0.0     11 1822 1822-11-01 03:59:59  blood  887 #> 1774   0.0     11 1822 1822-11-01 03:59:59   bone  887 #> 1775   0.0     12 1822 1822-12-01 14:00:00  blood  888 #> 1776   0.0     12 1822 1822-12-01 14:00:00   bone  888 #> 1777   0.0      1 1823 1823-01-01 00:00:00  blood  889 #> 1778   0.0      1 1823 1823-01-01 00:00:00   bone  889 #> 1779   0.0      2 1823 1823-01-31 09:59:59  blood  890 #> 1780   0.0      2 1823 1823-01-31 09:59:59   bone  890 #> 1781   0.6      3 1823 1823-03-02 20:00:00  blood  891 #> 1782   0.6      3 1823 1823-03-02 20:00:00   bone  891 #> 1783   0.0      4 1823 1823-04-02 06:00:00  blood  892 #> 1784   0.0      4 1823 1823-04-02 06:00:00   bone  892 #> 1785   0.0      5 1823 1823-05-02 15:59:59  blood  893 #> 1786   0.0      5 1823 1823-05-02 15:59:59   bone  893 #> 1787   0.0      6 1823 1823-06-02 02:00:00  blood  894 #> 1788   0.0      6 1823 1823-06-02 02:00:00   bone  894 #> 1789   0.5      7 1823 1823-07-02 12:00:00  blood  895 #> 1790   0.5      7 1823 1823-07-02 12:00:00   bone  895 #> 1791   0.0      8 1823 1823-08-01 21:59:59  blood  896 #> 1792   0.0      8 1823 1823-08-01 21:59:59   bone  896 #> 1793   0.0      9 1823 1823-09-01 08:00:00  blood  897 #> 1794   0.0      9 1823 1823-09-01 08:00:00   bone  897 #> 1795   0.0     10 1823 1823-10-01 18:00:00  blood  898 #> 1796   0.0     10 1823 1823-10-01 18:00:00   bone  898 #> 1797   0.0     11 1823 1823-11-01 03:59:59  blood  899 #> 1798   0.0     11 1823 1823-11-01 03:59:59   bone  899 #> 1799  20.4     12 1823 1823-12-01 14:00:00  blood  900 #> 1800  20.4     12 1823 1823-12-01 14:00:00   bone  900 #> 1801  21.6      1 1824 1824-01-01 00:00:00  blood  901 #> 1802  21.6      1 1824 1824-01-01 00:00:00   bone  901 #> 1803  10.8      2 1824 1824-01-31 11:59:59  blood  902 #> 1804  10.8      2 1824 1824-01-31 11:59:59   bone  902 #> 1805   0.0      3 1824 1824-03-02 00:00:00  blood  903 #> 1806   0.0      3 1824 1824-03-02 00:00:00   bone  903 #> 1807  19.4      4 1824 1824-04-01 12:00:00  blood  904 #> 1808  19.4      4 1824 1824-04-01 12:00:00   bone  904 #> 1809   2.8      5 1824 1824-05-01 23:59:59  blood  905 #> 1810   2.8      5 1824 1824-05-01 23:59:59   bone  905 #> 1811   0.0      6 1824 1824-06-01 12:00:00  blood  906 #> 1812   0.0      6 1824 1824-06-01 12:00:00   bone  906 #> 1813   0.0      7 1824 1824-07-02 00:00:00  blood  907 #> 1814   0.0      7 1824 1824-07-02 00:00:00   bone  907 #> 1815   1.4      8 1824 1824-08-01 11:59:59  blood  908 #> 1816   1.4      8 1824 1824-08-01 11:59:59   bone  908 #> 1817  20.5      9 1824 1824-09-01 00:00:00  blood  909 #> 1818  20.5      9 1824 1824-09-01 00:00:00   bone  909 #> 1819  25.2     10 1824 1824-10-01 12:00:00  blood  910 #> 1820  25.2     10 1824 1824-10-01 12:00:00   bone  910 #> 1821   0.0     11 1824 1824-10-31 23:59:59  blood  911 #> 1822   0.0     11 1824 1824-10-31 23:59:59   bone  911 #> 1823   0.8     12 1824 1824-12-01 12:00:00  blood  912 #> 1824   0.8     12 1824 1824-12-01 12:00:00   bone  912 #> 1825   5.0      1 1825 1825-01-01 00:00:00  blood  913 #> 1826   5.0      1 1825 1825-01-01 00:00:00   bone  913 #> 1827  15.5      2 1825 1825-01-31 09:59:59  blood  914 #> 1828  15.5      2 1825 1825-01-31 09:59:59   bone  914 #> 1829  22.4      3 1825 1825-03-02 20:00:00  blood  915 #> 1830  22.4      3 1825 1825-03-02 20:00:00   bone  915 #> 1831   3.8      4 1825 1825-04-02 06:00:00  blood  916 #> 1832   3.8      4 1825 1825-04-02 06:00:00   bone  916 #> 1833  15.4      5 1825 1825-05-02 15:59:59  blood  917 #> 1834  15.4      5 1825 1825-05-02 15:59:59   bone  917 #> 1835  15.4      6 1825 1825-06-02 02:00:00  blood  918 #> 1836  15.4      6 1825 1825-06-02 02:00:00   bone  918 #> 1837  30.9      7 1825 1825-07-02 12:00:00  blood  919 #> 1838  30.9      7 1825 1825-07-02 12:00:00   bone  919 #> 1839  25.4      8 1825 1825-08-01 21:59:59  blood  920 #> 1840  25.4      8 1825 1825-08-01 21:59:59   bone  920 #> 1841  15.7      9 1825 1825-09-01 08:00:00  blood  921 #> 1842  15.7      9 1825 1825-09-01 08:00:00   bone  921 #> 1843  15.6     10 1825 1825-10-01 18:00:00  blood  922 #> 1844  15.6     10 1825 1825-10-01 18:00:00   bone  922 #> 1845  11.7     11 1825 1825-11-01 03:59:59  blood  923 #> 1846  11.7     11 1825 1825-11-01 03:59:59   bone  923 #> 1847  22.0     12 1825 1825-12-01 14:00:00  blood  924 #> 1848  22.0     12 1825 1825-12-01 14:00:00   bone  924 #> 1849  17.7      1 1826 1826-01-01 00:00:00  blood  925 #> 1850  17.7      1 1826 1826-01-01 00:00:00   bone  925 #> 1851  18.2      2 1826 1826-01-31 09:59:59  blood  926 #> 1852  18.2      2 1826 1826-01-31 09:59:59   bone  926 #> 1853  36.7      3 1826 1826-03-02 20:00:00  blood  927 #> 1854  36.7      3 1826 1826-03-02 20:00:00   bone  927 #> 1855  24.0      4 1826 1826-04-02 06:00:00  blood  928 #> 1856  24.0      4 1826 1826-04-02 06:00:00   bone  928 #> 1857  32.4      5 1826 1826-05-02 15:59:59  blood  929 #> 1858  32.4      5 1826 1826-05-02 15:59:59   bone  929 #> 1859  37.1      6 1826 1826-06-02 02:00:00  blood  930 #> 1860  37.1      6 1826 1826-06-02 02:00:00   bone  930 #> 1861  52.5      7 1826 1826-07-02 12:00:00  blood  931 #> 1862  52.5      7 1826 1826-07-02 12:00:00   bone  931 #> 1863  39.6      8 1826 1826-08-01 21:59:59  blood  932 #> 1864  39.6      8 1826 1826-08-01 21:59:59   bone  932 #> 1865  18.9      9 1826 1826-09-01 08:00:00  blood  933 #> 1866  18.9      9 1826 1826-09-01 08:00:00   bone  933 #> 1867  50.6     10 1826 1826-10-01 18:00:00  blood  934 #> 1868  50.6     10 1826 1826-10-01 18:00:00   bone  934 #> 1869  39.5     11 1826 1826-11-01 03:59:59  blood  935 #> 1870  39.5     11 1826 1826-11-01 03:59:59   bone  935 #> 1871  68.1     12 1826 1826-12-01 14:00:00  blood  936 #> 1872  68.1     12 1826 1826-12-01 14:00:00   bone  936 #> 1873  34.6      1 1827 1827-01-01 00:00:00  blood  937 #> 1874  34.6      1 1827 1827-01-01 00:00:00   bone  937 #> 1875  47.4      2 1827 1827-01-31 09:59:59  blood  938 #> 1876  47.4      2 1827 1827-01-31 09:59:59   bone  938 #> 1877  57.8      3 1827 1827-03-02 20:00:00  blood  939 #> 1878  57.8      3 1827 1827-03-02 20:00:00   bone  939 #> 1879  46.0      4 1827 1827-04-02 06:00:00  blood  940 #> 1880  46.0      4 1827 1827-04-02 06:00:00   bone  940 #> 1881  56.3      5 1827 1827-05-02 15:59:59  blood  941 #> 1882  56.3      5 1827 1827-05-02 15:59:59   bone  941 #> 1883  56.7      6 1827 1827-06-02 02:00:00  blood  942 #> 1884  56.7      6 1827 1827-06-02 02:00:00   bone  942 #> 1885  42.9      7 1827 1827-07-02 12:00:00  blood  943 #> 1886  42.9      7 1827 1827-07-02 12:00:00   bone  943 #> 1887  53.7      8 1827 1827-08-01 21:59:59  blood  944 #> 1888  53.7      8 1827 1827-08-01 21:59:59   bone  944 #> 1889  49.6      9 1827 1827-09-01 08:00:00  blood  945 #> 1890  49.6      9 1827 1827-09-01 08:00:00   bone  945 #> 1891  57.2     10 1827 1827-10-01 18:00:00  blood  946 #> 1892  57.2     10 1827 1827-10-01 18:00:00   bone  946 #> 1893  48.2     11 1827 1827-11-01 03:59:59  blood  947 #> 1894  48.2     11 1827 1827-11-01 03:59:59   bone  947 #> 1895  46.1     12 1827 1827-12-01 14:00:00  blood  948 #> 1896  46.1     12 1827 1827-12-01 14:00:00   bone  948 #> 1897  52.8      1 1828 1828-01-01 00:00:00  blood  949 #> 1898  52.8      1 1828 1828-01-01 00:00:00   bone  949 #> 1899  64.4      2 1828 1828-01-31 11:59:59  blood  950 #> 1900  64.4      2 1828 1828-01-31 11:59:59   bone  950 #> 1901  65.0      3 1828 1828-03-02 00:00:00  blood  951 #> 1902  65.0      3 1828 1828-03-02 00:00:00   bone  951 #> 1903  61.1      4 1828 1828-04-01 12:00:00  blood  952 #> 1904  61.1      4 1828 1828-04-01 12:00:00   bone  952 #> 1905  89.1      5 1828 1828-05-01 23:59:59  blood  953 #> 1906  89.1      5 1828 1828-05-01 23:59:59   bone  953 #> 1907  98.0      6 1828 1828-06-01 12:00:00  blood  954 #> 1908  98.0      6 1828 1828-06-01 12:00:00   bone  954 #> 1909  54.3      7 1828 1828-07-02 00:00:00  blood  955 #> 1910  54.3      7 1828 1828-07-02 00:00:00   bone  955 #> 1911  76.4      8 1828 1828-08-01 11:59:59  blood  956 #> 1912  76.4      8 1828 1828-08-01 11:59:59   bone  956 #> 1913  50.4      9 1828 1828-09-01 00:00:00  blood  957 #> 1914  50.4      9 1828 1828-09-01 00:00:00   bone  957 #> 1915  54.7     10 1828 1828-10-01 12:00:00  blood  958 #> 1916  54.7     10 1828 1828-10-01 12:00:00   bone  958 #> 1917  57.0     11 1828 1828-10-31 23:59:59  blood  959 #> 1918  57.0     11 1828 1828-10-31 23:59:59   bone  959 #> 1919  46.6     12 1828 1828-12-01 12:00:00  blood  960 #> 1920  46.6     12 1828 1828-12-01 12:00:00   bone  960 #> 1921  43.0      1 1829 1829-01-01 00:00:00  blood  961 #> 1922  43.0      1 1829 1829-01-01 00:00:00   bone  961 #> 1923  49.4      2 1829 1829-01-31 09:59:59  blood  962 #> 1924  49.4      2 1829 1829-01-31 09:59:59   bone  962 #> 1925  72.3      3 1829 1829-03-02 20:00:00  blood  963 #> 1926  72.3      3 1829 1829-03-02 20:00:00   bone  963 #> 1927  95.0      4 1829 1829-04-02 06:00:00  blood  964 #> 1928  95.0      4 1829 1829-04-02 06:00:00   bone  964 #> 1929  67.5      5 1829 1829-05-02 15:59:59  blood  965 #> 1930  67.5      5 1829 1829-05-02 15:59:59   bone  965 #> 1931  73.9      6 1829 1829-06-02 02:00:00  blood  966 #> 1932  73.9      6 1829 1829-06-02 02:00:00   bone  966 #> 1933  90.8      7 1829 1829-07-02 12:00:00  blood  967 #> 1934  90.8      7 1829 1829-07-02 12:00:00   bone  967 #> 1935  78.3      8 1829 1829-08-01 21:59:59  blood  968 #> 1936  78.3      8 1829 1829-08-01 21:59:59   bone  968 #> 1937  52.8      9 1829 1829-09-01 08:00:00  blood  969 #> 1938  52.8      9 1829 1829-09-01 08:00:00   bone  969 #> 1939  57.2     10 1829 1829-10-01 18:00:00  blood  970 #> 1940  57.2     10 1829 1829-10-01 18:00:00   bone  970 #> 1941  67.6     11 1829 1829-11-01 03:59:59  blood  971 #> 1942  67.6     11 1829 1829-11-01 03:59:59   bone  971 #> 1943  56.5     12 1829 1829-12-01 14:00:00  blood  972 #> 1944  56.5     12 1829 1829-12-01 14:00:00   bone  972 #> 1945  52.2      1 1830 1830-01-01 00:00:00  blood  973 #> 1946  52.2      1 1830 1830-01-01 00:00:00   bone  973 #> 1947  72.1      2 1830 1830-01-31 09:59:59  blood  974 #> 1948  72.1      2 1830 1830-01-31 09:59:59   bone  974 #> 1949  84.6      3 1830 1830-03-02 20:00:00  blood  975 #> 1950  84.6      3 1830 1830-03-02 20:00:00   bone  975 #> 1951 107.1      4 1830 1830-04-02 06:00:00  blood  976 #> 1952 107.1      4 1830 1830-04-02 06:00:00   bone  976 #> 1953  66.3      5 1830 1830-05-02 15:59:59  blood  977 #> 1954  66.3      5 1830 1830-05-02 15:59:59   bone  977 #> 1955  65.1      6 1830 1830-06-02 02:00:00  blood  978 #> 1956  65.1      6 1830 1830-06-02 02:00:00   bone  978 #> 1957  43.9      7 1830 1830-07-02 12:00:00  blood  979 #> 1958  43.9      7 1830 1830-07-02 12:00:00   bone  979 #> 1959  50.7      8 1830 1830-08-01 21:59:59  blood  980 #> 1960  50.7      8 1830 1830-08-01 21:59:59   bone  980 #> 1961  62.1      9 1830 1830-09-01 08:00:00  blood  981 #> 1962  62.1      9 1830 1830-09-01 08:00:00   bone  981 #> 1963  84.4     10 1830 1830-10-01 18:00:00  blood  982 #> 1964  84.4     10 1830 1830-10-01 18:00:00   bone  982 #> 1965  81.2     11 1830 1830-11-01 03:59:59  blood  983 #> 1966  81.2     11 1830 1830-11-01 03:59:59   bone  983 #> 1967  82.1     12 1830 1830-12-01 14:00:00  blood  984 #> 1968  82.1     12 1830 1830-12-01 14:00:00   bone  984 #> 1969  47.5      1 1831 1831-01-01 00:00:00  blood  985 #> 1970  47.5      1 1831 1831-01-01 00:00:00   bone  985 #> 1971  50.1      2 1831 1831-01-31 09:59:59  blood  986 #> 1972  50.1      2 1831 1831-01-31 09:59:59   bone  986 #> 1973  93.4      3 1831 1831-03-02 20:00:00  blood  987 #> 1974  93.4      3 1831 1831-03-02 20:00:00   bone  987 #> 1975  54.6      4 1831 1831-04-02 06:00:00  blood  988 #> 1976  54.6      4 1831 1831-04-02 06:00:00   bone  988 #> 1977  38.1      5 1831 1831-05-02 15:59:59  blood  989 #> 1978  38.1      5 1831 1831-05-02 15:59:59   bone  989 #> 1979  33.4      6 1831 1831-06-02 02:00:00  blood  990 #> 1980  33.4      6 1831 1831-06-02 02:00:00   bone  990 #> 1981  45.2      7 1831 1831-07-02 12:00:00  blood  991 #> 1982  45.2      7 1831 1831-07-02 12:00:00   bone  991 #> 1983  54.9      8 1831 1831-08-01 21:59:59  blood  992 #> 1984  54.9      8 1831 1831-08-01 21:59:59   bone  992 #> 1985  37.9      9 1831 1831-09-01 08:00:00  blood  993 #> 1986  37.9      9 1831 1831-09-01 08:00:00   bone  993 #> 1987  46.2     10 1831 1831-10-01 18:00:00  blood  994 #> 1988  46.2     10 1831 1831-10-01 18:00:00   bone  994 #> 1989  43.5     11 1831 1831-11-01 03:59:59  blood  995 #> 1990  43.5     11 1831 1831-11-01 03:59:59   bone  995 #> 1991  28.9     12 1831 1831-12-01 14:00:00  blood  996 #> 1992  28.9     12 1831 1831-12-01 14:00:00   bone  996 #> 1993  30.9      1 1832 1832-01-01 00:00:00  blood  997 #> 1994  30.9      1 1832 1832-01-01 00:00:00   bone  997 #> 1995  55.5      2 1832 1832-01-31 11:59:59  blood  998 #> 1996  55.5      2 1832 1832-01-31 11:59:59   bone  998 #> 1997  55.1      3 1832 1832-03-02 00:00:00  blood  999 #> 1998  55.1      3 1832 1832-03-02 00:00:00   bone  999 #> 1999  26.9      4 1832 1832-04-01 12:00:00  blood 1000 #> 2000  26.9      4 1832 1832-04-01 12:00:00   bone 1000 #> 2001  41.3      5 1832 1832-05-01 23:59:59  blood 1001 #> 2002  41.3      5 1832 1832-05-01 23:59:59   bone 1001 #> 2003  26.7      6 1832 1832-06-01 12:00:00  blood 1002 #> 2004  26.7      6 1832 1832-06-01 12:00:00   bone 1002 #> 2005  13.9      7 1832 1832-07-02 00:00:00  blood 1003 #> 2006  13.9      7 1832 1832-07-02 00:00:00   bone 1003 #> 2007   8.9      8 1832 1832-08-01 11:59:59  blood 1004 #> 2008   8.9      8 1832 1832-08-01 11:59:59   bone 1004 #> 2009   8.2      9 1832 1832-09-01 00:00:00  blood 1005 #> 2010   8.2      9 1832 1832-09-01 00:00:00   bone 1005 #> 2011  21.1     10 1832 1832-10-01 12:00:00  blood 1006 #> 2012  21.1     10 1832 1832-10-01 12:00:00   bone 1006 #> 2013  14.3     11 1832 1832-10-31 23:59:59  blood 1007 #> 2014  14.3     11 1832 1832-10-31 23:59:59   bone 1007 #> 2015  27.5     12 1832 1832-12-01 12:00:00  blood 1008 #> 2016  27.5     12 1832 1832-12-01 12:00:00   bone 1008 #> 2017  11.3      1 1833 1833-01-01 00:00:00  blood 1009 #> 2018  11.3      1 1833 1833-01-01 00:00:00   bone 1009 #> 2019  14.9      2 1833 1833-01-31 09:59:59  blood 1010 #> 2020  14.9      2 1833 1833-01-31 09:59:59   bone 1010 #> 2021  11.8      3 1833 1833-03-02 20:00:00  blood 1011 #> 2022  11.8      3 1833 1833-03-02 20:00:00   bone 1011 #> 2023   2.8      4 1833 1833-04-02 06:00:00  blood 1012 #> 2024   2.8      4 1833 1833-04-02 06:00:00   bone 1012 #> 2025  12.9      5 1833 1833-05-02 15:59:59  blood 1013 #> 2026  12.9      5 1833 1833-05-02 15:59:59   bone 1013 #> 2027   1.0      6 1833 1833-06-02 02:00:00  blood 1014 #> 2028   1.0      6 1833 1833-06-02 02:00:00   bone 1014 #> 2029   7.0      7 1833 1833-07-02 12:00:00  blood 1015 #> 2030   7.0      7 1833 1833-07-02 12:00:00   bone 1015 #> 2031   5.7      8 1833 1833-08-01 21:59:59  blood 1016 #> 2032   5.7      8 1833 1833-08-01 21:59:59   bone 1016 #> 2033  11.6      9 1833 1833-09-01 08:00:00  blood 1017 #> 2034  11.6      9 1833 1833-09-01 08:00:00   bone 1017 #> 2035   7.5     10 1833 1833-10-01 18:00:00  blood 1018 #> 2036   7.5     10 1833 1833-10-01 18:00:00   bone 1018 #> 2037   5.9     11 1833 1833-11-01 03:59:59  blood 1019 #> 2038   5.9     11 1833 1833-11-01 03:59:59   bone 1019 #> 2039   9.9     12 1833 1833-12-01 14:00:00  blood 1020 #> 2040   9.9     12 1833 1833-12-01 14:00:00   bone 1020 #> 2041   4.9      1 1834 1834-01-01 00:00:00  blood 1021 #> 2042   4.9      1 1834 1834-01-01 00:00:00   bone 1021 #> 2043  18.1      2 1834 1834-01-31 09:59:59  blood 1022 #> 2044  18.1      2 1834 1834-01-31 09:59:59   bone 1022 #> 2045   3.9      3 1834 1834-03-02 20:00:00  blood 1023 #> 2046   3.9      3 1834 1834-03-02 20:00:00   bone 1023 #> 2047   1.4      4 1834 1834-04-02 06:00:00  blood 1024 #> 2048   1.4      4 1834 1834-04-02 06:00:00   bone 1024 #> 2049   8.8      5 1834 1834-05-02 15:59:59  blood 1025 #> 2050   8.8      5 1834 1834-05-02 15:59:59   bone 1025 #> 2051   7.8      6 1834 1834-06-02 02:00:00  blood 1026 #> 2052   7.8      6 1834 1834-06-02 02:00:00   bone 1026 #> 2053   8.7      7 1834 1834-07-02 12:00:00  blood 1027 #> 2054   8.7      7 1834 1834-07-02 12:00:00   bone 1027 #> 2055   4.0      8 1834 1834-08-01 21:59:59  blood 1028 #> 2056   4.0      8 1834 1834-08-01 21:59:59   bone 1028 #> 2057  11.5      9 1834 1834-09-01 08:00:00  blood 1029 #> 2058  11.5      9 1834 1834-09-01 08:00:00   bone 1029 #> 2059  24.8     10 1834 1834-10-01 18:00:00  blood 1030 #> 2060  24.8     10 1834 1834-10-01 18:00:00   bone 1030 #> 2061  30.5     11 1834 1834-11-01 03:59:59  blood 1031 #> 2062  30.5     11 1834 1834-11-01 03:59:59   bone 1031 #> 2063  34.5     12 1834 1834-12-01 14:00:00  blood 1032 #> 2064  34.5     12 1834 1834-12-01 14:00:00   bone 1032 #> 2065   7.5      1 1835 1835-01-01 00:00:00  blood 1033 #> 2066   7.5      1 1835 1835-01-01 00:00:00   bone 1033 #> 2067  24.5      2 1835 1835-01-31 09:59:59  blood 1034 #> 2068  24.5      2 1835 1835-01-31 09:59:59   bone 1034 #> 2069  19.7      3 1835 1835-03-02 20:00:00  blood 1035 #> 2070  19.7      3 1835 1835-03-02 20:00:00   bone 1035 #> 2071  61.5      4 1835 1835-04-02 06:00:00  blood 1036 #> 2072  61.5      4 1835 1835-04-02 06:00:00   bone 1036 #> 2073  43.6      5 1835 1835-05-02 15:59:59  blood 1037 #> 2074  43.6      5 1835 1835-05-02 15:59:59   bone 1037 #> 2075  33.2      6 1835 1835-06-02 02:00:00  blood 1038 #> 2076  33.2      6 1835 1835-06-02 02:00:00   bone 1038 #> 2077  59.8      7 1835 1835-07-02 12:00:00  blood 1039 #> 2078  59.8      7 1835 1835-07-02 12:00:00   bone 1039 #> 2079  59.0      8 1835 1835-08-01 21:59:59  blood 1040 #> 2080  59.0      8 1835 1835-08-01 21:59:59   bone 1040 #> 2081 100.8      9 1835 1835-09-01 08:00:00  blood 1041 #> 2082 100.8      9 1835 1835-09-01 08:00:00   bone 1041 #> 2083  95.2     10 1835 1835-10-01 18:00:00  blood 1042 #> 2084  95.2     10 1835 1835-10-01 18:00:00   bone 1042 #> 2085 100.0     11 1835 1835-11-01 03:59:59  blood 1043 #> 2086 100.0     11 1835 1835-11-01 03:59:59   bone 1043 #> 2087  77.5     12 1835 1835-12-01 14:00:00  blood 1044 #> 2088  77.5     12 1835 1835-12-01 14:00:00   bone 1044 #> 2089  88.6      1 1836 1836-01-01 00:00:00  blood 1045 #> 2090  88.6      1 1836 1836-01-01 00:00:00   bone 1045 #> 2091 107.6      2 1836 1836-01-31 11:59:59  blood 1046 #> 2092 107.6      2 1836 1836-01-31 11:59:59   bone 1046 #> 2093  98.1      3 1836 1836-03-02 00:00:00  blood 1047 #> 2094  98.1      3 1836 1836-03-02 00:00:00   bone 1047 #> 2095 142.9      4 1836 1836-04-01 12:00:00  blood 1048 #> 2096 142.9      4 1836 1836-04-01 12:00:00   bone 1048 #> 2097 111.4      5 1836 1836-05-01 23:59:59  blood 1049 #> 2098 111.4      5 1836 1836-05-01 23:59:59   bone 1049 #> 2099 124.7      6 1836 1836-06-01 12:00:00  blood 1050 #> 2100 124.7      6 1836 1836-06-01 12:00:00   bone 1050 #> 2101 116.7      7 1836 1836-07-02 00:00:00  blood 1051 #> 2102 116.7      7 1836 1836-07-02 00:00:00   bone 1051 #> 2103 107.8      8 1836 1836-08-01 11:59:59  blood 1052 #> 2104 107.8      8 1836 1836-08-01 11:59:59   bone 1052 #> 2105  95.1      9 1836 1836-09-01 00:00:00  blood 1053 #> 2106  95.1      9 1836 1836-09-01 00:00:00   bone 1053 #> 2107 137.4     10 1836 1836-10-01 12:00:00  blood 1054 #> 2108 137.4     10 1836 1836-10-01 12:00:00   bone 1054 #> 2109 120.9     11 1836 1836-10-31 23:59:59  blood 1055 #> 2110 120.9     11 1836 1836-10-31 23:59:59   bone 1055 #> 2111 206.2     12 1836 1836-12-01 12:00:00  blood 1056 #> 2112 206.2     12 1836 1836-12-01 12:00:00   bone 1056 #> 2113 188.0      1 1837 1837-01-01 00:00:00  blood 1057 #> 2114 188.0      1 1837 1837-01-01 00:00:00   bone 1057 #> 2115 175.6      2 1837 1837-01-31 09:59:59  blood 1058 #> 2116 175.6      2 1837 1837-01-31 09:59:59   bone 1058 #> 2117 134.6      3 1837 1837-03-02 20:00:00  blood 1059 #> 2118 134.6      3 1837 1837-03-02 20:00:00   bone 1059 #> 2119 138.2      4 1837 1837-04-02 06:00:00  blood 1060 #> 2120 138.2      4 1837 1837-04-02 06:00:00   bone 1060 #> 2121 111.3      5 1837 1837-05-02 15:59:59  blood 1061 #> 2122 111.3      5 1837 1837-05-02 15:59:59   bone 1061 #> 2123 158.0      6 1837 1837-06-02 02:00:00  blood 1062 #> 2124 158.0      6 1837 1837-06-02 02:00:00   bone 1062 #> 2125 162.8      7 1837 1837-07-02 12:00:00  blood 1063 #> 2126 162.8      7 1837 1837-07-02 12:00:00   bone 1063 #> 2127 134.0      8 1837 1837-08-01 21:59:59  blood 1064 #> 2128 134.0      8 1837 1837-08-01 21:59:59   bone 1064 #> 2129  96.3      9 1837 1837-09-01 08:00:00  blood 1065 #> 2130  96.3      9 1837 1837-09-01 08:00:00   bone 1065 #> 2131 123.7     10 1837 1837-10-01 18:00:00  blood 1066 #> 2132 123.7     10 1837 1837-10-01 18:00:00   bone 1066 #> 2133 107.0     11 1837 1837-11-01 03:59:59  blood 1067 #> 2134 107.0     11 1837 1837-11-01 03:59:59   bone 1067 #> 2135 129.8     12 1837 1837-12-01 14:00:00  blood 1068 #> 2136 129.8     12 1837 1837-12-01 14:00:00   bone 1068 #> 2137 144.9      1 1838 1838-01-01 00:00:00  blood 1069 #> 2138 144.9      1 1838 1838-01-01 00:00:00   bone 1069 #> 2139  84.8      2 1838 1838-01-31 09:59:59  blood 1070 #> 2140  84.8      2 1838 1838-01-31 09:59:59   bone 1070 #> 2141 140.8      3 1838 1838-03-02 20:00:00  blood 1071 #> 2142 140.8      3 1838 1838-03-02 20:00:00   bone 1071 #> 2143 126.6      4 1838 1838-04-02 06:00:00  blood 1072 #> 2144 126.6      4 1838 1838-04-02 06:00:00   bone 1072 #> 2145 137.6      5 1838 1838-05-02 15:59:59  blood 1073 #> 2146 137.6      5 1838 1838-05-02 15:59:59   bone 1073 #> 2147  94.5      6 1838 1838-06-02 02:00:00  blood 1074 #> 2148  94.5      6 1838 1838-06-02 02:00:00   bone 1074 #> 2149 108.2      7 1838 1838-07-02 12:00:00  blood 1075 #> 2150 108.2      7 1838 1838-07-02 12:00:00   bone 1075 #> 2151  78.8      8 1838 1838-08-01 21:59:59  blood 1076 #> 2152  78.8      8 1838 1838-08-01 21:59:59   bone 1076 #> 2153  73.6      9 1838 1838-09-01 08:00:00  blood 1077 #> 2154  73.6      9 1838 1838-09-01 08:00:00   bone 1077 #> 2155  90.8     10 1838 1838-10-01 18:00:00  blood 1078 #> 2156  90.8     10 1838 1838-10-01 18:00:00   bone 1078 #> 2157  77.4     11 1838 1838-11-01 03:59:59  blood 1079 #> 2158  77.4     11 1838 1838-11-01 03:59:59   bone 1079 #> 2159  79.8     12 1838 1838-12-01 14:00:00  blood 1080 #> 2160  79.8     12 1838 1838-12-01 14:00:00   bone 1080 #> 2161 107.6      1 1839 1839-01-01 00:00:00  blood 1081 #> 2162 107.6      1 1839 1839-01-01 00:00:00   bone 1081 #> 2163 102.5      2 1839 1839-01-31 09:59:59  blood 1082 #> 2164 102.5      2 1839 1839-01-31 09:59:59   bone 1082 #> 2165  77.7      3 1839 1839-03-02 20:00:00  blood 1083 #> 2166  77.7      3 1839 1839-03-02 20:00:00   bone 1083 #> 2167  61.8      4 1839 1839-04-02 06:00:00  blood 1084 #> 2168  61.8      4 1839 1839-04-02 06:00:00   bone 1084 #> 2169  53.8      5 1839 1839-05-02 15:59:59  blood 1085 #> 2170  53.8      5 1839 1839-05-02 15:59:59   bone 1085 #> 2171  54.6      6 1839 1839-06-02 02:00:00  blood 1086 #> 2172  54.6      6 1839 1839-06-02 02:00:00   bone 1086 #> 2173  84.7      7 1839 1839-07-02 12:00:00  blood 1087 #> 2174  84.7      7 1839 1839-07-02 12:00:00   bone 1087 #> 2175 131.2      8 1839 1839-08-01 21:59:59  blood 1088 #> 2176 131.2      8 1839 1839-08-01 21:59:59   bone 1088 #> 2177 132.7      9 1839 1839-09-01 08:00:00  blood 1089 #> 2178 132.7      9 1839 1839-09-01 08:00:00   bone 1089 #> 2179  90.8     10 1839 1839-10-01 18:00:00  blood 1090 #> 2180  90.8     10 1839 1839-10-01 18:00:00   bone 1090 #> 2181  68.8     11 1839 1839-11-01 03:59:59  blood 1091 #> 2182  68.8     11 1839 1839-11-01 03:59:59   bone 1091 #> 2183  63.6     12 1839 1839-12-01 14:00:00  blood 1092 #> 2184  63.6     12 1839 1839-12-01 14:00:00   bone 1092 #> 2185  81.2      1 1840 1840-01-01 00:00:00  blood 1093 #> 2186  81.2      1 1840 1840-01-01 00:00:00   bone 1093 #> 2187  87.7      2 1840 1840-01-31 11:59:59  blood 1094 #> 2188  87.7      2 1840 1840-01-31 11:59:59   bone 1094 #> 2189  55.5      3 1840 1840-03-02 00:00:00  blood 1095 #> 2190  55.5      3 1840 1840-03-02 00:00:00   bone 1095 #> 2191  65.9      4 1840 1840-04-01 12:00:00  blood 1096 #> 2192  65.9      4 1840 1840-04-01 12:00:00   bone 1096 #> 2193  69.2      5 1840 1840-05-01 23:59:59  blood 1097 #> 2194  69.2      5 1840 1840-05-01 23:59:59   bone 1097 #> 2195  48.5      6 1840 1840-06-01 12:00:00  blood 1098 #> 2196  48.5      6 1840 1840-06-01 12:00:00   bone 1098 #> 2197  60.7      7 1840 1840-07-02 00:00:00  blood 1099 #> 2198  60.7      7 1840 1840-07-02 00:00:00   bone 1099 #> 2199  57.8      8 1840 1840-08-01 11:59:59  blood 1100 #> 2200  57.8      8 1840 1840-08-01 11:59:59   bone 1100 #> 2201  74.0      9 1840 1840-09-01 00:00:00  blood 1101 #> 2202  74.0      9 1840 1840-09-01 00:00:00   bone 1101 #> 2203  49.8     10 1840 1840-10-01 12:00:00  blood 1102 #> 2204  49.8     10 1840 1840-10-01 12:00:00   bone 1102 #> 2205  54.3     11 1840 1840-10-31 23:59:59  blood 1103 #> 2206  54.3     11 1840 1840-10-31 23:59:59   bone 1103 #> 2207  53.7     12 1840 1840-12-01 12:00:00  blood 1104 #> 2208  53.7     12 1840 1840-12-01 12:00:00   bone 1104 #> 2209  24.0      1 1841 1841-01-01 00:00:00  blood 1105 #> 2210  24.0      1 1841 1841-01-01 00:00:00   bone 1105 #> 2211  29.9      2 1841 1841-01-31 09:59:59  blood 1106 #> 2212  29.9      2 1841 1841-01-31 09:59:59   bone 1106 #> 2213  29.7      3 1841 1841-03-02 20:00:00  blood 1107 #> 2214  29.7      3 1841 1841-03-02 20:00:00   bone 1107 #> 2215  42.6      4 1841 1841-04-02 06:00:00  blood 1108 #> 2216  42.6      4 1841 1841-04-02 06:00:00   bone 1108 #> 2217  67.4      5 1841 1841-05-02 15:59:59  blood 1109 #> 2218  67.4      5 1841 1841-05-02 15:59:59   bone 1109 #> 2219  55.7      6 1841 1841-06-02 02:00:00  blood 1110 #> 2220  55.7      6 1841 1841-06-02 02:00:00   bone 1110 #> 2221  30.8      7 1841 1841-07-02 12:00:00  blood 1111 #> 2222  30.8      7 1841 1841-07-02 12:00:00   bone 1111 #> 2223  39.3      8 1841 1841-08-01 21:59:59  blood 1112 #> 2224  39.3      8 1841 1841-08-01 21:59:59   bone 1112 #> 2225  35.1      9 1841 1841-09-01 08:00:00  blood 1113 #> 2226  35.1      9 1841 1841-09-01 08:00:00   bone 1113 #> 2227  28.5     10 1841 1841-10-01 18:00:00  blood 1114 #> 2228  28.5     10 1841 1841-10-01 18:00:00   bone 1114 #> 2229  19.8     11 1841 1841-11-01 03:59:59  blood 1115 #> 2230  19.8     11 1841 1841-11-01 03:59:59   bone 1115 #> 2231  38.8     12 1841 1841-12-01 14:00:00  blood 1116 #> 2232  38.8     12 1841 1841-12-01 14:00:00   bone 1116 #> 2233  20.4      1 1842 1842-01-01 00:00:00  blood 1117 #> 2234  20.4      1 1842 1842-01-01 00:00:00   bone 1117 #> 2235  22.1      2 1842 1842-01-31 09:59:59  blood 1118 #> 2236  22.1      2 1842 1842-01-31 09:59:59   bone 1118 #> 2237  21.7      3 1842 1842-03-02 20:00:00  blood 1119 #> 2238  21.7      3 1842 1842-03-02 20:00:00   bone 1119 #> 2239  26.9      4 1842 1842-04-02 06:00:00  blood 1120 #> 2240  26.9      4 1842 1842-04-02 06:00:00   bone 1120 #> 2241  24.9      5 1842 1842-05-02 15:59:59  blood 1121 #> 2242  24.9      5 1842 1842-05-02 15:59:59   bone 1121 #> 2243  20.5      6 1842 1842-06-02 02:00:00  blood 1122 #> 2244  20.5      6 1842 1842-06-02 02:00:00   bone 1122 #> 2245  12.6      7 1842 1842-07-02 12:00:00  blood 1123 #> 2246  12.6      7 1842 1842-07-02 12:00:00   bone 1123 #> 2247  26.5      8 1842 1842-08-01 21:59:59  blood 1124 #> 2248  26.5      8 1842 1842-08-01 21:59:59   bone 1124 #> 2249  18.5      9 1842 1842-09-01 08:00:00  blood 1125 #> 2250  18.5      9 1842 1842-09-01 08:00:00   bone 1125 #> 2251  38.1     10 1842 1842-10-01 18:00:00  blood 1126 #> 2252  38.1     10 1842 1842-10-01 18:00:00   bone 1126 #> 2253  40.5     11 1842 1842-11-01 03:59:59  blood 1127 #> 2254  40.5     11 1842 1842-11-01 03:59:59   bone 1127 #> 2255  17.6     12 1842 1842-12-01 14:00:00  blood 1128 #> 2256  17.6     12 1842 1842-12-01 14:00:00   bone 1128 #> 2257  13.3      1 1843 1843-01-01 00:00:00  blood 1129 #> 2258  13.3      1 1843 1843-01-01 00:00:00   bone 1129 #> 2259   3.5      2 1843 1843-01-31 09:59:59  blood 1130 #> 2260   3.5      2 1843 1843-01-31 09:59:59   bone 1130 #> 2261   8.3      3 1843 1843-03-02 20:00:00  blood 1131 #> 2262   8.3      3 1843 1843-03-02 20:00:00   bone 1131 #> 2263   8.8      4 1843 1843-04-02 06:00:00  blood 1132 #> 2264   8.8      4 1843 1843-04-02 06:00:00   bone 1132 #> 2265  21.1      5 1843 1843-05-02 15:59:59  blood 1133 #> 2266  21.1      5 1843 1843-05-02 15:59:59   bone 1133 #> 2267  10.5      6 1843 1843-06-02 02:00:00  blood 1134 #> 2268  10.5      6 1843 1843-06-02 02:00:00   bone 1134 #> 2269   9.5      7 1843 1843-07-02 12:00:00  blood 1135 #> 2270   9.5      7 1843 1843-07-02 12:00:00   bone 1135 #> 2271  11.8      8 1843 1843-08-01 21:59:59  blood 1136 #> 2272  11.8      8 1843 1843-08-01 21:59:59   bone 1136 #> 2273   4.2      9 1843 1843-09-01 08:00:00  blood 1137 #> 2274   4.2      9 1843 1843-09-01 08:00:00   bone 1137 #> 2275   5.3     10 1843 1843-10-01 18:00:00  blood 1138 #> 2276   5.3     10 1843 1843-10-01 18:00:00   bone 1138 #> 2277  19.1     11 1843 1843-11-01 03:59:59  blood 1139 #> 2278  19.1     11 1843 1843-11-01 03:59:59   bone 1139 #> 2279  12.7     12 1843 1843-12-01 14:00:00  blood 1140 #> 2280  12.7     12 1843 1843-12-01 14:00:00   bone 1140 #> 2281   9.4      1 1844 1844-01-01 00:00:00  blood 1141 #> 2282   9.4      1 1844 1844-01-01 00:00:00   bone 1141 #> 2283  14.7      2 1844 1844-01-31 11:59:59  blood 1142 #> 2284  14.7      2 1844 1844-01-31 11:59:59   bone 1142 #> 2285  13.6      3 1844 1844-03-02 00:00:00  blood 1143 #> 2286  13.6      3 1844 1844-03-02 00:00:00   bone 1143 #> 2287  20.8      4 1844 1844-04-01 12:00:00  blood 1144 #> 2288  20.8      4 1844 1844-04-01 12:00:00   bone 1144 #> 2289  12.0      5 1844 1844-05-01 23:59:59  blood 1145 #> 2290  12.0      5 1844 1844-05-01 23:59:59   bone 1145 #> 2291   3.7      6 1844 1844-06-01 12:00:00  blood 1146 #> 2292   3.7      6 1844 1844-06-01 12:00:00   bone 1146 #> 2293  21.2      7 1844 1844-07-02 00:00:00  blood 1147 #> 2294  21.2      7 1844 1844-07-02 00:00:00   bone 1147 #> 2295  23.9      8 1844 1844-08-01 11:59:59  blood 1148 #> 2296  23.9      8 1844 1844-08-01 11:59:59   bone 1148 #> 2297   6.9      9 1844 1844-09-01 00:00:00  blood 1149 #> 2298   6.9      9 1844 1844-09-01 00:00:00   bone 1149 #> 2299  21.5     10 1844 1844-10-01 12:00:00  blood 1150 #> 2300  21.5     10 1844 1844-10-01 12:00:00   bone 1150 #> 2301  10.7     11 1844 1844-10-31 23:59:59  blood 1151 #> 2302  10.7     11 1844 1844-10-31 23:59:59   bone 1151 #> 2303  21.6     12 1844 1844-12-01 12:00:00  blood 1152 #> 2304  21.6     12 1844 1844-12-01 12:00:00   bone 1152 #> 2305  25.7      1 1845 1845-01-01 00:00:00  blood 1153 #> 2306  25.7      1 1845 1845-01-01 00:00:00   bone 1153 #> 2307  43.6      2 1845 1845-01-31 09:59:59  blood 1154 #> 2308  43.6      2 1845 1845-01-31 09:59:59   bone 1154 #> 2309  43.3      3 1845 1845-03-02 20:00:00  blood 1155 #> 2310  43.3      3 1845 1845-03-02 20:00:00   bone 1155 #> 2311  56.9      4 1845 1845-04-02 06:00:00  blood 1156 #> 2312  56.9      4 1845 1845-04-02 06:00:00   bone 1156 #> 2313  47.8      5 1845 1845-05-02 15:59:59  blood 1157 #> 2314  47.8      5 1845 1845-05-02 15:59:59   bone 1157 #> 2315  31.1      6 1845 1845-06-02 02:00:00  blood 1158 #> 2316  31.1      6 1845 1845-06-02 02:00:00   bone 1158 #> 2317  30.6      7 1845 1845-07-02 12:00:00  blood 1159 #> 2318  30.6      7 1845 1845-07-02 12:00:00   bone 1159 #> 2319  32.3      8 1845 1845-08-01 21:59:59  blood 1160 #> 2320  32.3      8 1845 1845-08-01 21:59:59   bone 1160 #> 2321  29.6      9 1845 1845-09-01 08:00:00  blood 1161 #> 2322  29.6      9 1845 1845-09-01 08:00:00   bone 1161 #> 2323  40.7     10 1845 1845-10-01 18:00:00  blood 1162 #> 2324  40.7     10 1845 1845-10-01 18:00:00   bone 1162 #> 2325  39.4     11 1845 1845-11-01 03:59:59  blood 1163 #> 2326  39.4     11 1845 1845-11-01 03:59:59   bone 1163 #> 2327  59.7     12 1845 1845-12-01 14:00:00  blood 1164 #> 2328  59.7     12 1845 1845-12-01 14:00:00   bone 1164 #> 2329  38.7      1 1846 1846-01-01 00:00:00  blood 1165 #> 2330  38.7      1 1846 1846-01-01 00:00:00   bone 1165 #> 2331  51.0      2 1846 1846-01-31 09:59:59  blood 1166 #> 2332  51.0      2 1846 1846-01-31 09:59:59   bone 1166 #> 2333  63.9      3 1846 1846-03-02 20:00:00  blood 1167 #> 2334  63.9      3 1846 1846-03-02 20:00:00   bone 1167 #> 2335  69.2      4 1846 1846-04-02 06:00:00  blood 1168 #> 2336  69.2      4 1846 1846-04-02 06:00:00   bone 1168 #> 2337  59.9      5 1846 1846-05-02 15:59:59  blood 1169 #> 2338  59.9      5 1846 1846-05-02 15:59:59   bone 1169 #> 2339  65.1      6 1846 1846-06-02 02:00:00  blood 1170 #> 2340  65.1      6 1846 1846-06-02 02:00:00   bone 1170 #> 2341  46.5      7 1846 1846-07-02 12:00:00  blood 1171 #> 2342  46.5      7 1846 1846-07-02 12:00:00   bone 1171 #> 2343  54.8      8 1846 1846-08-01 21:59:59  blood 1172 #> 2344  54.8      8 1846 1846-08-01 21:59:59   bone 1172 #> 2345 107.1      9 1846 1846-09-01 08:00:00  blood 1173 #> 2346 107.1      9 1846 1846-09-01 08:00:00   bone 1173 #> 2347  55.9     10 1846 1846-10-01 18:00:00  blood 1174 #> 2348  55.9     10 1846 1846-10-01 18:00:00   bone 1174 #> 2349  60.4     11 1846 1846-11-01 03:59:59  blood 1175 #> 2350  60.4     11 1846 1846-11-01 03:59:59   bone 1175 #> 2351  65.5     12 1846 1846-12-01 14:00:00  blood 1176 #> 2352  65.5     12 1846 1846-12-01 14:00:00   bone 1176 #> 2353  62.6      1 1847 1847-01-01 00:00:00  blood 1177 #> 2354  62.6      1 1847 1847-01-01 00:00:00   bone 1177 #> 2355  44.9      2 1847 1847-01-31 09:59:59  blood 1178 #> 2356  44.9      2 1847 1847-01-31 09:59:59   bone 1178 #> 2357  85.7      3 1847 1847-03-02 20:00:00  blood 1179 #> 2358  85.7      3 1847 1847-03-02 20:00:00   bone 1179 #> 2359  44.7      4 1847 1847-04-02 06:00:00  blood 1180 #> 2360  44.7      4 1847 1847-04-02 06:00:00   bone 1180 #> 2361  75.4      5 1847 1847-05-02 15:59:59  blood 1181 #> 2362  75.4      5 1847 1847-05-02 15:59:59   bone 1181 #> 2363  85.3      6 1847 1847-06-02 02:00:00  blood 1182 #> 2364  85.3      6 1847 1847-06-02 02:00:00   bone 1182 #> 2365  52.2      7 1847 1847-07-02 12:00:00  blood 1183 #> 2366  52.2      7 1847 1847-07-02 12:00:00   bone 1183 #> 2367 140.6      8 1847 1847-08-01 21:59:59  blood 1184 #> 2368 140.6      8 1847 1847-08-01 21:59:59   bone 1184 #> 2369 161.2      9 1847 1847-09-01 08:00:00  blood 1185 #> 2370 161.2      9 1847 1847-09-01 08:00:00   bone 1185 #> 2371 180.4     10 1847 1847-10-01 18:00:00  blood 1186 #> 2372 180.4     10 1847 1847-10-01 18:00:00   bone 1186 #> 2373 138.9     11 1847 1847-11-01 03:59:59  blood 1187 #> 2374 138.9     11 1847 1847-11-01 03:59:59   bone 1187 #> 2375 109.6     12 1847 1847-12-01 14:00:00  blood 1188 #> 2376 109.6     12 1847 1847-12-01 14:00:00   bone 1188 #> 2377 159.1      1 1848 1848-01-01 00:00:00  blood 1189 #> 2378 159.1      1 1848 1848-01-01 00:00:00   bone 1189 #> 2379 111.8      2 1848 1848-01-31 11:59:59  blood 1190 #> 2380 111.8      2 1848 1848-01-31 11:59:59   bone 1190 #> 2381 108.9      3 1848 1848-03-02 00:00:00  blood 1191 #> 2382 108.9      3 1848 1848-03-02 00:00:00   bone 1191 #> 2383 107.1      4 1848 1848-04-01 12:00:00  blood 1192 #> 2384 107.1      4 1848 1848-04-01 12:00:00   bone 1192 #> 2385 102.2      5 1848 1848-05-01 23:59:59  blood 1193 #> 2386 102.2      5 1848 1848-05-01 23:59:59   bone 1193 #> 2387 123.8      6 1848 1848-06-01 12:00:00  blood 1194 #> 2388 123.8      6 1848 1848-06-01 12:00:00   bone 1194 #> 2389 139.2      7 1848 1848-07-02 00:00:00  blood 1195 #> 2390 139.2      7 1848 1848-07-02 00:00:00   bone 1195 #> 2391 132.5      8 1848 1848-08-01 11:59:59  blood 1196 #> 2392 132.5      8 1848 1848-08-01 11:59:59   bone 1196 #> 2393 100.3      9 1848 1848-09-01 00:00:00  blood 1197 #> 2394 100.3      9 1848 1848-09-01 00:00:00   bone 1197 #> 2395 132.4     10 1848 1848-10-01 12:00:00  blood 1198 #> 2396 132.4     10 1848 1848-10-01 12:00:00   bone 1198 #> 2397 114.6     11 1848 1848-10-31 23:59:59  blood 1199 #> 2398 114.6     11 1848 1848-10-31 23:59:59   bone 1199 #> 2399 159.9     12 1848 1848-12-01 12:00:00  blood 1200 #> 2400 159.9     12 1848 1848-12-01 12:00:00   bone 1200 #> 2401 156.7      1 1849 1849-01-01 00:00:00  blood 1201 #> 2402 156.7      1 1849 1849-01-01 00:00:00   bone 1201 #> 2403 131.7      2 1849 1849-01-31 09:59:59  blood 1202 #> 2404 131.7      2 1849 1849-01-31 09:59:59   bone 1202 #> 2405  96.5      3 1849 1849-03-02 20:00:00  blood 1203 #> 2406  96.5      3 1849 1849-03-02 20:00:00   bone 1203 #> 2407 102.5      4 1849 1849-04-02 06:00:00  blood 1204 #> 2408 102.5      4 1849 1849-04-02 06:00:00   bone 1204 #> 2409  80.6      5 1849 1849-05-02 15:59:59  blood 1205 #> 2410  80.6      5 1849 1849-05-02 15:59:59   bone 1205 #> 2411  81.2      6 1849 1849-06-02 02:00:00  blood 1206 #> 2412  81.2      6 1849 1849-06-02 02:00:00   bone 1206 #> 2413  78.0      7 1849 1849-07-02 12:00:00  blood 1207 #> 2414  78.0      7 1849 1849-07-02 12:00:00   bone 1207 #> 2415  61.3      8 1849 1849-08-01 21:59:59  blood 1208 #> 2416  61.3      8 1849 1849-08-01 21:59:59   bone 1208 #> 2417  93.7      9 1849 1849-09-01 08:00:00  blood 1209 #> 2418  93.7      9 1849 1849-09-01 08:00:00   bone 1209 #> 2419  71.5     10 1849 1849-10-01 18:00:00  blood 1210 #> 2420  71.5     10 1849 1849-10-01 18:00:00   bone 1210 #> 2421  99.7     11 1849 1849-11-01 03:59:59  blood 1211 #> 2422  99.7     11 1849 1849-11-01 03:59:59   bone 1211 #> 2423  97.0     12 1849 1849-12-01 14:00:00  blood 1212 #> 2424  97.0     12 1849 1849-12-01 14:00:00   bone 1212 #> 2425  78.0      1 1850 1850-01-01 00:00:00  blood 1213 #> 2426  78.0      1 1850 1850-01-01 00:00:00   bone 1213 #> 2427  89.4      2 1850 1850-01-31 09:59:59  blood 1214 #> 2428  89.4      2 1850 1850-01-31 09:59:59   bone 1214 #> 2429  82.6      3 1850 1850-03-02 20:00:00  blood 1215 #> 2430  82.6      3 1850 1850-03-02 20:00:00   bone 1215 #> 2431  44.1      4 1850 1850-04-02 06:00:00  blood 1216 #> 2432  44.1      4 1850 1850-04-02 06:00:00   bone 1216 #> 2433  61.6      5 1850 1850-05-02 15:59:59  blood 1217 #> 2434  61.6      5 1850 1850-05-02 15:59:59   bone 1217 #> 2435  70.0      6 1850 1850-06-02 02:00:00  blood 1218 #> 2436  70.0      6 1850 1850-06-02 02:00:00   bone 1218 #> 2437  39.1      7 1850 1850-07-02 12:00:00  blood 1219 #> 2438  39.1      7 1850 1850-07-02 12:00:00   bone 1219 #> 2439  61.6      8 1850 1850-08-01 21:59:59  blood 1220 #> 2440  61.6      8 1850 1850-08-01 21:59:59   bone 1220 #> 2441  86.2      9 1850 1850-09-01 08:00:00  blood 1221 #> 2442  86.2      9 1850 1850-09-01 08:00:00   bone 1221 #> 2443  71.0     10 1850 1850-10-01 18:00:00  blood 1222 #> 2444  71.0     10 1850 1850-10-01 18:00:00   bone 1222 #> 2445  54.8     11 1850 1850-11-01 03:59:59  blood 1223 #> 2446  54.8     11 1850 1850-11-01 03:59:59   bone 1223 #> 2447  60.0     12 1850 1850-12-01 14:00:00  blood 1224 #> 2448  60.0     12 1850 1850-12-01 14:00:00   bone 1224 #> 2449  75.5      1 1851 1851-01-01 00:00:00  blood 1225 #> 2450  75.5      1 1851 1851-01-01 00:00:00   bone 1225 #> 2451 105.4      2 1851 1851-01-31 09:59:59  blood 1226 #> 2452 105.4      2 1851 1851-01-31 09:59:59   bone 1226 #> 2453  64.6      3 1851 1851-03-02 20:00:00  blood 1227 #> 2454  64.6      3 1851 1851-03-02 20:00:00   bone 1227 #> 2455  56.5      4 1851 1851-04-02 06:00:00  blood 1228 #> 2456  56.5      4 1851 1851-04-02 06:00:00   bone 1228 #> 2457  62.6      5 1851 1851-05-02 15:59:59  blood 1229 #> 2458  62.6      5 1851 1851-05-02 15:59:59   bone 1229 #> 2459  63.2      6 1851 1851-06-02 02:00:00  blood 1230 #> 2460  63.2      6 1851 1851-06-02 02:00:00   bone 1230 #> 2461  36.1      7 1851 1851-07-02 12:00:00  blood 1231 #> 2462  36.1      7 1851 1851-07-02 12:00:00   bone 1231 #> 2463  57.4      8 1851 1851-08-01 21:59:59  blood 1232 #> 2464  57.4      8 1851 1851-08-01 21:59:59   bone 1232 #> 2465  67.9      9 1851 1851-09-01 08:00:00  blood 1233 #> 2466  67.9      9 1851 1851-09-01 08:00:00   bone 1233 #> 2467  62.5     10 1851 1851-10-01 18:00:00  blood 1234 #> 2468  62.5     10 1851 1851-10-01 18:00:00   bone 1234 #> 2469  50.9     11 1851 1851-11-01 03:59:59  blood 1235 #> 2470  50.9     11 1851 1851-11-01 03:59:59   bone 1235 #> 2471  71.4     12 1851 1851-12-01 14:00:00  blood 1236 #> 2472  71.4     12 1851 1851-12-01 14:00:00   bone 1236 #> 2473  68.4      1 1852 1852-01-01 00:00:00  blood 1237 #> 2474  68.4      1 1852 1852-01-01 00:00:00   bone 1237 #> 2475  67.5      2 1852 1852-01-31 11:59:59  blood 1238 #> 2476  67.5      2 1852 1852-01-31 11:59:59   bone 1238 #> 2477  61.2      3 1852 1852-03-02 00:00:00  blood 1239 #> 2478  61.2      3 1852 1852-03-02 00:00:00   bone 1239 #> 2479  65.4      4 1852 1852-04-01 12:00:00  blood 1240 #> 2480  65.4      4 1852 1852-04-01 12:00:00   bone 1240 #> 2481  54.9      5 1852 1852-05-01 23:59:59  blood 1241 #> 2482  54.9      5 1852 1852-05-01 23:59:59   bone 1241 #> 2483  46.9      6 1852 1852-06-01 12:00:00  blood 1242 #> 2484  46.9      6 1852 1852-06-01 12:00:00   bone 1242 #> 2485  42.0      7 1852 1852-07-02 00:00:00  blood 1243 #> 2486  42.0      7 1852 1852-07-02 00:00:00   bone 1243 #> 2487  39.7      8 1852 1852-08-01 11:59:59  blood 1244 #> 2488  39.7      8 1852 1852-08-01 11:59:59   bone 1244 #> 2489  37.5      9 1852 1852-09-01 00:00:00  blood 1245 #> 2490  37.5      9 1852 1852-09-01 00:00:00   bone 1245 #> 2491  67.3     10 1852 1852-10-01 12:00:00  blood 1246 #> 2492  67.3     10 1852 1852-10-01 12:00:00   bone 1246 #> 2493  54.3     11 1852 1852-10-31 23:59:59  blood 1247 #> 2494  54.3     11 1852 1852-10-31 23:59:59   bone 1247 #> 2495  45.4     12 1852 1852-12-01 12:00:00  blood 1248 #> 2496  45.4     12 1852 1852-12-01 12:00:00   bone 1248 #> 2497  41.1      1 1853 1853-01-01 00:00:00  blood 1249 #> 2498  41.1      1 1853 1853-01-01 00:00:00   bone 1249 #> 2499  42.9      2 1853 1853-01-31 09:59:59  blood 1250 #> 2500  42.9      2 1853 1853-01-31 09:59:59   bone 1250 #> 2501  37.7      3 1853 1853-03-02 20:00:00  blood 1251 #> 2502  37.7      3 1853 1853-03-02 20:00:00   bone 1251 #> 2503  47.6      4 1853 1853-04-02 06:00:00  blood 1252 #> 2504  47.6      4 1853 1853-04-02 06:00:00   bone 1252 #> 2505  34.7      5 1853 1853-05-02 15:59:59  blood 1253 #> 2506  34.7      5 1853 1853-05-02 15:59:59   bone 1253 #> 2507  40.0      6 1853 1853-06-02 02:00:00  blood 1254 #> 2508  40.0      6 1853 1853-06-02 02:00:00   bone 1254 #> 2509  45.9      7 1853 1853-07-02 12:00:00  blood 1255 #> 2510  45.9      7 1853 1853-07-02 12:00:00   bone 1255 #> 2511  50.4      8 1853 1853-08-01 21:59:59  blood 1256 #> 2512  50.4      8 1853 1853-08-01 21:59:59   bone 1256 #> 2513  33.5      9 1853 1853-09-01 08:00:00  blood 1257 #> 2514  33.5      9 1853 1853-09-01 08:00:00   bone 1257 #> 2515  42.3     10 1853 1853-10-01 18:00:00  blood 1258 #> 2516  42.3     10 1853 1853-10-01 18:00:00   bone 1258 #> 2517  28.8     11 1853 1853-11-01 03:59:59  blood 1259 #> 2518  28.8     11 1853 1853-11-01 03:59:59   bone 1259 #> 2519  23.4     12 1853 1853-12-01 14:00:00  blood 1260 #> 2520  23.4     12 1853 1853-12-01 14:00:00   bone 1260 #> 2521  15.4      1 1854 1854-01-01 00:00:00  blood 1261 #> 2522  15.4      1 1854 1854-01-01 00:00:00   bone 1261 #> 2523  20.0      2 1854 1854-01-31 09:59:59  blood 1262 #> 2524  20.0      2 1854 1854-01-31 09:59:59   bone 1262 #> 2525  20.7      3 1854 1854-03-02 20:00:00  blood 1263 #> 2526  20.7      3 1854 1854-03-02 20:00:00   bone 1263 #> 2527  26.4      4 1854 1854-04-02 06:00:00  blood 1264 #> 2528  26.4      4 1854 1854-04-02 06:00:00   bone 1264 #> 2529  24.0      5 1854 1854-05-02 15:59:59  blood 1265 #> 2530  24.0      5 1854 1854-05-02 15:59:59   bone 1265 #> 2531  21.1      6 1854 1854-06-02 02:00:00  blood 1266 #> 2532  21.1      6 1854 1854-06-02 02:00:00   bone 1266 #> 2533  18.7      7 1854 1854-07-02 12:00:00  blood 1267 #> 2534  18.7      7 1854 1854-07-02 12:00:00   bone 1267 #> 2535  15.8      8 1854 1854-08-01 21:59:59  blood 1268 #> 2536  15.8      8 1854 1854-08-01 21:59:59   bone 1268 #> 2537  22.4      9 1854 1854-09-01 08:00:00  blood 1269 #> 2538  22.4      9 1854 1854-09-01 08:00:00   bone 1269 #> 2539  12.7     10 1854 1854-10-01 18:00:00  blood 1270 #> 2540  12.7     10 1854 1854-10-01 18:00:00   bone 1270 #> 2541  28.2     11 1854 1854-11-01 03:59:59  blood 1271 #> 2542  28.2     11 1854 1854-11-01 03:59:59   bone 1271 #> 2543  21.4     12 1854 1854-12-01 14:00:00  blood 1272 #> 2544  21.4     12 1854 1854-12-01 14:00:00   bone 1272 #> 2545  12.3      1 1855 1855-01-01 00:00:00  blood 1273 #> 2546  12.3      1 1855 1855-01-01 00:00:00   bone 1273 #> 2547  11.4      2 1855 1855-01-31 09:59:59  blood 1274 #> 2548  11.4      2 1855 1855-01-31 09:59:59   bone 1274 #> 2549  17.4      3 1855 1855-03-02 20:00:00  blood 1275 #> 2550  17.4      3 1855 1855-03-02 20:00:00   bone 1275 #> 2551   4.4      4 1855 1855-04-02 06:00:00  blood 1276 #> 2552   4.4      4 1855 1855-04-02 06:00:00   bone 1276 #> 2553   9.1      5 1855 1855-05-02 15:59:59  blood 1277 #> 2554   9.1      5 1855 1855-05-02 15:59:59   bone 1277 #> 2555   5.3      6 1855 1855-06-02 02:00:00  blood 1278 #> 2556   5.3      6 1855 1855-06-02 02:00:00   bone 1278 #> 2557   0.4      7 1855 1855-07-02 12:00:00  blood 1279 #> 2558   0.4      7 1855 1855-07-02 12:00:00   bone 1279 #> 2559   3.1      8 1855 1855-08-01 21:59:59  blood 1280 #> 2560   3.1      8 1855 1855-08-01 21:59:59   bone 1280 #> 2561   0.0      9 1855 1855-09-01 08:00:00  blood 1281 #> 2562   0.0      9 1855 1855-09-01 08:00:00   bone 1281 #> 2563   9.7     10 1855 1855-10-01 18:00:00  blood 1282 #> 2564   9.7     10 1855 1855-10-01 18:00:00   bone 1282 #> 2565   4.3     11 1855 1855-11-01 03:59:59  blood 1283 #> 2566   4.3     11 1855 1855-11-01 03:59:59   bone 1283 #> 2567   3.1     12 1855 1855-12-01 14:00:00  blood 1284 #> 2568   3.1     12 1855 1855-12-01 14:00:00   bone 1284 #> 2569   0.5      1 1856 1856-01-01 00:00:00  blood 1285 #> 2570   0.5      1 1856 1856-01-01 00:00:00   bone 1285 #> 2571   4.9      2 1856 1856-01-31 11:59:59  blood 1286 #> 2572   4.9      2 1856 1856-01-31 11:59:59   bone 1286 #> 2573   0.4      3 1856 1856-03-02 00:00:00  blood 1287 #> 2574   0.4      3 1856 1856-03-02 00:00:00   bone 1287 #> 2575   6.5      4 1856 1856-04-01 12:00:00  blood 1288 #> 2576   6.5      4 1856 1856-04-01 12:00:00   bone 1288 #> 2577   0.0      5 1856 1856-05-01 23:59:59  blood 1289 #> 2578   0.0      5 1856 1856-05-01 23:59:59   bone 1289 #> 2579   5.0      6 1856 1856-06-01 12:00:00  blood 1290 #> 2580   5.0      6 1856 1856-06-01 12:00:00   bone 1290 #> 2581   4.6      7 1856 1856-07-02 00:00:00  blood 1291 #> 2582   4.6      7 1856 1856-07-02 00:00:00   bone 1291 #> 2583   5.9      8 1856 1856-08-01 11:59:59  blood 1292 #> 2584   5.9      8 1856 1856-08-01 11:59:59   bone 1292 #> 2585   4.4      9 1856 1856-09-01 00:00:00  blood 1293 #> 2586   4.4      9 1856 1856-09-01 00:00:00   bone 1293 #> 2587   4.5     10 1856 1856-10-01 12:00:00  blood 1294 #> 2588   4.5     10 1856 1856-10-01 12:00:00   bone 1294 #> 2589   7.7     11 1856 1856-10-31 23:59:59  blood 1295 #> 2590   7.7     11 1856 1856-10-31 23:59:59   bone 1295 #> 2591   7.2     12 1856 1856-12-01 12:00:00  blood 1296 #> 2592   7.2     12 1856 1856-12-01 12:00:00   bone 1296 #> 2593  13.7      1 1857 1857-01-01 00:00:00  blood 1297 #> 2594  13.7      1 1857 1857-01-01 00:00:00   bone 1297 #> 2595   7.4      2 1857 1857-01-31 09:59:59  blood 1298 #> 2596   7.4      2 1857 1857-01-31 09:59:59   bone 1298 #> 2597   5.2      3 1857 1857-03-02 20:00:00  blood 1299 #> 2598   5.2      3 1857 1857-03-02 20:00:00   bone 1299 #> 2599  11.1      4 1857 1857-04-02 06:00:00  blood 1300 #> 2600  11.1      4 1857 1857-04-02 06:00:00   bone 1300 #> 2601  29.2      5 1857 1857-05-02 15:59:59  blood 1301 #> 2602  29.2      5 1857 1857-05-02 15:59:59   bone 1301 #> 2603  16.0      6 1857 1857-06-02 02:00:00  blood 1302 #> 2604  16.0      6 1857 1857-06-02 02:00:00   bone 1302 #> 2605  22.2      7 1857 1857-07-02 12:00:00  blood 1303 #> 2606  22.2      7 1857 1857-07-02 12:00:00   bone 1303 #> 2607  16.9      8 1857 1857-08-01 21:59:59  blood 1304 #> 2608  16.9      8 1857 1857-08-01 21:59:59   bone 1304 #> 2609  42.4      9 1857 1857-09-01 08:00:00  blood 1305 #> 2610  42.4      9 1857 1857-09-01 08:00:00   bone 1305 #> 2611  40.6     10 1857 1857-10-01 18:00:00  blood 1306 #> 2612  40.6     10 1857 1857-10-01 18:00:00   bone 1306 #> 2613  31.4     11 1857 1857-11-01 03:59:59  blood 1307 #> 2614  31.4     11 1857 1857-11-01 03:59:59   bone 1307 #> 2615  37.2     12 1857 1857-12-01 14:00:00  blood 1308 #> 2616  37.2     12 1857 1857-12-01 14:00:00   bone 1308 #> 2617  39.0      1 1858 1858-01-01 00:00:00  blood 1309 #> 2618  39.0      1 1858 1858-01-01 00:00:00   bone 1309 #> 2619  34.9      2 1858 1858-01-31 09:59:59  blood 1310 #> 2620  34.9      2 1858 1858-01-31 09:59:59   bone 1310 #> 2621  57.5      3 1858 1858-03-02 20:00:00  blood 1311 #> 2622  57.5      3 1858 1858-03-02 20:00:00   bone 1311 #> 2623  38.3      4 1858 1858-04-02 06:00:00  blood 1312 #> 2624  38.3      4 1858 1858-04-02 06:00:00   bone 1312 #> 2625  41.4      5 1858 1858-05-02 15:59:59  blood 1313 #> 2626  41.4      5 1858 1858-05-02 15:59:59   bone 1313 #> 2627  44.5      6 1858 1858-06-02 02:00:00  blood 1314 #> 2628  44.5      6 1858 1858-06-02 02:00:00   bone 1314 #> 2629  56.7      7 1858 1858-07-02 12:00:00  blood 1315 #> 2630  56.7      7 1858 1858-07-02 12:00:00   bone 1315 #> 2631  55.3      8 1858 1858-08-01 21:59:59  blood 1316 #> 2632  55.3      8 1858 1858-08-01 21:59:59   bone 1316 #> 2633  80.1      9 1858 1858-09-01 08:00:00  blood 1317 #> 2634  80.1      9 1858 1858-09-01 08:00:00   bone 1317 #> 2635  91.2     10 1858 1858-10-01 18:00:00  blood 1318 #> 2636  91.2     10 1858 1858-10-01 18:00:00   bone 1318 #> 2637  51.9     11 1858 1858-11-01 03:59:59  blood 1319 #> 2638  51.9     11 1858 1858-11-01 03:59:59   bone 1319 #> 2639  66.9     12 1858 1858-12-01 14:00:00  blood 1320 #> 2640  66.9     12 1858 1858-12-01 14:00:00   bone 1320 #> 2641  83.7      1 1859 1859-01-01 00:00:00  blood 1321 #> 2642  83.7      1 1859 1859-01-01 00:00:00   bone 1321 #> 2643  87.6      2 1859 1859-01-31 09:59:59  blood 1322 #> 2644  87.6      2 1859 1859-01-31 09:59:59   bone 1322 #> 2645  90.3      3 1859 1859-03-02 20:00:00  blood 1323 #> 2646  90.3      3 1859 1859-03-02 20:00:00   bone 1323 #> 2647  85.7      4 1859 1859-04-02 06:00:00  blood 1324 #> 2648  85.7      4 1859 1859-04-02 06:00:00   bone 1324 #> 2649  91.0      5 1859 1859-05-02 15:59:59  blood 1325 #> 2650  91.0      5 1859 1859-05-02 15:59:59   bone 1325 #> 2651  87.1      6 1859 1859-06-02 02:00:00  blood 1326 #> 2652  87.1      6 1859 1859-06-02 02:00:00   bone 1326 #> 2653  95.2      7 1859 1859-07-02 12:00:00  blood 1327 #> 2654  95.2      7 1859 1859-07-02 12:00:00   bone 1327 #> 2655 106.8      8 1859 1859-08-01 21:59:59  blood 1328 #> 2656 106.8      8 1859 1859-08-01 21:59:59   bone 1328 #> 2657 105.8      9 1859 1859-09-01 08:00:00  blood 1329 #> 2658 105.8      9 1859 1859-09-01 08:00:00   bone 1329 #> 2659 114.6     10 1859 1859-10-01 18:00:00  blood 1330 #> 2660 114.6     10 1859 1859-10-01 18:00:00   bone 1330 #> 2661  97.2     11 1859 1859-11-01 03:59:59  blood 1331 #> 2662  97.2     11 1859 1859-11-01 03:59:59   bone 1331 #> 2663  81.0     12 1859 1859-12-01 14:00:00  blood 1332 #> 2664  81.0     12 1859 1859-12-01 14:00:00   bone 1332 #> 2665  81.5      1 1860 1860-01-01 00:00:00  blood 1333 #> 2666  81.5      1 1860 1860-01-01 00:00:00   bone 1333 #> 2667  88.0      2 1860 1860-01-31 12:00:00  blood 1334 #> 2668  88.0      2 1860 1860-01-31 12:00:00   bone 1334 #> 2669  98.9      3 1860 1860-03-02 00:00:00  blood 1335 #> 2670  98.9      3 1860 1860-03-02 00:00:00   bone 1335 #> 2671  71.4      4 1860 1860-04-01 12:00:00  blood 1336 #> 2672  71.4      4 1860 1860-04-01 12:00:00   bone 1336 #> 2673 107.1      5 1860 1860-05-02 00:00:00  blood 1337 #> 2674 107.1      5 1860 1860-05-02 00:00:00   bone 1337 #> 2675 108.6      6 1860 1860-06-01 12:00:00  blood 1338 #> 2676 108.6      6 1860 1860-06-01 12:00:00   bone 1338 #> 2677 116.7      7 1860 1860-07-02 00:00:00  blood 1339 #> 2678 116.7      7 1860 1860-07-02 00:00:00   bone 1339 #> 2679 100.3      8 1860 1860-08-01 12:00:00  blood 1340 #> 2680 100.3      8 1860 1860-08-01 12:00:00   bone 1340 #> 2681  92.2      9 1860 1860-09-01 00:00:00  blood 1341 #> 2682  92.2      9 1860 1860-09-01 00:00:00   bone 1341 #> 2683  90.1     10 1860 1860-10-01 12:00:00  blood 1342 #> 2684  90.1     10 1860 1860-10-01 12:00:00   bone 1342 #> 2685  97.9     11 1860 1860-11-01 00:00:00  blood 1343 #> 2686  97.9     11 1860 1860-11-01 00:00:00   bone 1343 #> 2687  95.6     12 1860 1860-12-01 12:00:00  blood 1344 #> 2688  95.6     12 1860 1860-12-01 12:00:00   bone 1344 #> 2689  62.3      1 1861 1861-01-01 00:00:00  blood 1345 #> 2690  62.3      1 1861 1861-01-01 00:00:00   bone 1345 #> 2691  77.8      2 1861 1861-01-31 10:00:00  blood 1346 #> 2692  77.8      2 1861 1861-01-31 10:00:00   bone 1346 #> 2693 101.0      3 1861 1861-03-02 20:00:00  blood 1347 #> 2694 101.0      3 1861 1861-03-02 20:00:00   bone 1347 #> 2695  98.5      4 1861 1861-04-02 06:00:00  blood 1348 #> 2696  98.5      4 1861 1861-04-02 06:00:00   bone 1348 #> 2697  56.8      5 1861 1861-05-02 16:00:00  blood 1349 #> 2698  56.8      5 1861 1861-05-02 16:00:00   bone 1349 #> 2699  87.8      6 1861 1861-06-02 02:00:00  blood 1350 #> 2700  87.8      6 1861 1861-06-02 02:00:00   bone 1350 #> 2701  78.0      7 1861 1861-07-02 12:00:00  blood 1351 #> 2702  78.0      7 1861 1861-07-02 12:00:00   bone 1351 #> 2703  82.5      8 1861 1861-08-01 22:00:00  blood 1352 #> 2704  82.5      8 1861 1861-08-01 22:00:00   bone 1352 #> 2705  79.9      9 1861 1861-09-01 08:00:00  blood 1353 #> 2706  79.9      9 1861 1861-09-01 08:00:00   bone 1353 #> 2707  67.2     10 1861 1861-10-01 18:00:00  blood 1354 #> 2708  67.2     10 1861 1861-10-01 18:00:00   bone 1354 #> 2709  53.7     11 1861 1861-11-01 04:00:00  blood 1355 #> 2710  53.7     11 1861 1861-11-01 04:00:00   bone 1355 #> 2711  80.5     12 1861 1861-12-01 14:00:00  blood 1356 #> 2712  80.5     12 1861 1861-12-01 14:00:00   bone 1356 #> 2713  63.1      1 1862 1862-01-01 00:00:00  blood 1357 #> 2714  63.1      1 1862 1862-01-01 00:00:00   bone 1357 #> 2715  64.5      2 1862 1862-01-31 10:00:00  blood 1358 #> 2716  64.5      2 1862 1862-01-31 10:00:00   bone 1358 #> 2717  43.6      3 1862 1862-03-02 20:00:00  blood 1359 #> 2718  43.6      3 1862 1862-03-02 20:00:00   bone 1359 #> 2719  53.7      4 1862 1862-04-02 06:00:00  blood 1360 #> 2720  53.7      4 1862 1862-04-02 06:00:00   bone 1360 #> 2721  64.4      5 1862 1862-05-02 16:00:00  blood 1361 #> 2722  64.4      5 1862 1862-05-02 16:00:00   bone 1361 #> 2723  84.0      6 1862 1862-06-02 02:00:00  blood 1362 #> 2724  84.0      6 1862 1862-06-02 02:00:00   bone 1362 #> 2725  73.4      7 1862 1862-07-02 12:00:00  blood 1363 #> 2726  73.4      7 1862 1862-07-02 12:00:00   bone 1363 #> 2727  62.5      8 1862 1862-08-01 22:00:00  blood 1364 #> 2728  62.5      8 1862 1862-08-01 22:00:00   bone 1364 #> 2729  66.6      9 1862 1862-09-01 08:00:00  blood 1365 #> 2730  66.6      9 1862 1862-09-01 08:00:00   bone 1365 #> 2731  42.0     10 1862 1862-10-01 18:00:00  blood 1366 #> 2732  42.0     10 1862 1862-10-01 18:00:00   bone 1366 #> 2733  50.6     11 1862 1862-11-01 04:00:00  blood 1367 #> 2734  50.6     11 1862 1862-11-01 04:00:00   bone 1367 #> 2735  40.9     12 1862 1862-12-01 14:00:00  blood 1368 #> 2736  40.9     12 1862 1862-12-01 14:00:00   bone 1368 #> 2737  48.3      1 1863 1863-01-01 00:00:00  blood 1369 #> 2738  48.3      1 1863 1863-01-01 00:00:00   bone 1369 #> 2739  56.7      2 1863 1863-01-31 10:00:00  blood 1370 #> 2740  56.7      2 1863 1863-01-31 10:00:00   bone 1370 #> 2741  66.4      3 1863 1863-03-02 20:00:00  blood 1371 #> 2742  66.4      3 1863 1863-03-02 20:00:00   bone 1371 #> 2743  40.6      4 1863 1863-04-02 06:00:00  blood 1372 #> 2744  40.6      4 1863 1863-04-02 06:00:00   bone 1372 #> 2745  53.8      5 1863 1863-05-02 16:00:00  blood 1373 #> 2746  53.8      5 1863 1863-05-02 16:00:00   bone 1373 #> 2747  40.8      6 1863 1863-06-02 02:00:00  blood 1374 #> 2748  40.8      6 1863 1863-06-02 02:00:00   bone 1374 #> 2749  32.7      7 1863 1863-07-02 12:00:00  blood 1375 #> 2750  32.7      7 1863 1863-07-02 12:00:00   bone 1375 #> 2751  48.1      8 1863 1863-08-01 22:00:00  blood 1376 #> 2752  48.1      8 1863 1863-08-01 22:00:00   bone 1376 #> 2753  22.0      9 1863 1863-09-01 08:00:00  blood 1377 #> 2754  22.0      9 1863 1863-09-01 08:00:00   bone 1377 #> 2755  39.9     10 1863 1863-10-01 18:00:00  blood 1378 #> 2756  39.9     10 1863 1863-10-01 18:00:00   bone 1378 #> 2757  37.7     11 1863 1863-11-01 04:00:00  blood 1379 #> 2758  37.7     11 1863 1863-11-01 04:00:00   bone 1379 #> 2759  41.2     12 1863 1863-12-01 14:00:00  blood 1380 #> 2760  41.2     12 1863 1863-12-01 14:00:00   bone 1380 #> 2761  57.7      1 1864 1864-01-01 00:00:00  blood 1381 #> 2762  57.7      1 1864 1864-01-01 00:00:00   bone 1381 #> 2763  47.1      2 1864 1864-01-31 12:00:00  blood 1382 #> 2764  47.1      2 1864 1864-01-31 12:00:00   bone 1382 #> 2765  66.3      3 1864 1864-03-02 00:00:00  blood 1383 #> 2766  66.3      3 1864 1864-03-02 00:00:00   bone 1383 #> 2767  35.8      4 1864 1864-04-01 12:00:00  blood 1384 #> 2768  35.8      4 1864 1864-04-01 12:00:00   bone 1384 #> 2769  40.6      5 1864 1864-05-02 00:00:00  blood 1385 #> 2770  40.6      5 1864 1864-05-02 00:00:00   bone 1385 #> 2771  57.8      6 1864 1864-06-01 12:00:00  blood 1386 #> 2772  57.8      6 1864 1864-06-01 12:00:00   bone 1386 #> 2773  54.7      7 1864 1864-07-02 00:00:00  blood 1387 #> 2774  54.7      7 1864 1864-07-02 00:00:00   bone 1387 #> 2775  54.8      8 1864 1864-08-01 12:00:00  blood 1388 #> 2776  54.8      8 1864 1864-08-01 12:00:00   bone 1388 #> 2777  28.5      9 1864 1864-09-01 00:00:00  blood 1389 #> 2778  28.5      9 1864 1864-09-01 00:00:00   bone 1389 #> 2779  33.9     10 1864 1864-10-01 12:00:00  blood 1390 #> 2780  33.9     10 1864 1864-10-01 12:00:00   bone 1390 #> 2781  57.6     11 1864 1864-11-01 00:00:00  blood 1391 #> 2782  57.6     11 1864 1864-11-01 00:00:00   bone 1391 #> 2783  28.6     12 1864 1864-12-01 12:00:00  blood 1392 #> 2784  28.6     12 1864 1864-12-01 12:00:00   bone 1392 #> 2785  48.7      1 1865 1865-01-01 00:00:00  blood 1393 #> 2786  48.7      1 1865 1865-01-01 00:00:00   bone 1393 #> 2787  39.3      2 1865 1865-01-31 10:00:00  blood 1394 #> 2788  39.3      2 1865 1865-01-31 10:00:00   bone 1394 #> 2789  39.5      3 1865 1865-03-02 20:00:00  blood 1395 #> 2790  39.5      3 1865 1865-03-02 20:00:00   bone 1395 #> 2791  29.4      4 1865 1865-04-02 06:00:00  blood 1396 #> 2792  29.4      4 1865 1865-04-02 06:00:00   bone 1396 #> 2793  34.5      5 1865 1865-05-02 16:00:00  blood 1397 #> 2794  34.5      5 1865 1865-05-02 16:00:00   bone 1397 #> 2795  33.6      6 1865 1865-06-02 02:00:00  blood 1398 #> 2796  33.6      6 1865 1865-06-02 02:00:00   bone 1398 #> 2797  26.8      7 1865 1865-07-02 12:00:00  blood 1399 #> 2798  26.8      7 1865 1865-07-02 12:00:00   bone 1399 #> 2799  37.8      8 1865 1865-08-01 22:00:00  blood 1400 #> 2800  37.8      8 1865 1865-08-01 22:00:00   bone 1400 #> 2801  21.6      9 1865 1865-09-01 08:00:00  blood 1401 #> 2802  21.6      9 1865 1865-09-01 08:00:00   bone 1401 #> 2803  17.1     10 1865 1865-10-01 18:00:00  blood 1402 #> 2804  17.1     10 1865 1865-10-01 18:00:00   bone 1402 #> 2805  24.6     11 1865 1865-11-01 04:00:00  blood 1403 #> 2806  24.6     11 1865 1865-11-01 04:00:00   bone 1403 #> 2807  12.8     12 1865 1865-12-01 14:00:00  blood 1404 #> 2808  12.8     12 1865 1865-12-01 14:00:00   bone 1404 #> 2809  31.6      1 1866 1866-01-01 00:00:00  blood 1405 #> 2810  31.6      1 1866 1866-01-01 00:00:00   bone 1405 #> 2811  38.4      2 1866 1866-01-31 10:00:00  blood 1406 #> 2812  38.4      2 1866 1866-01-31 10:00:00   bone 1406 #> 2813  24.6      3 1866 1866-03-02 20:00:00  blood 1407 #> 2814  24.6      3 1866 1866-03-02 20:00:00   bone 1407 #> 2815  17.6      4 1866 1866-04-02 06:00:00  blood 1408 #> 2816  17.6      4 1866 1866-04-02 06:00:00   bone 1408 #> 2817  12.9      5 1866 1866-05-02 16:00:00  blood 1409 #> 2818  12.9      5 1866 1866-05-02 16:00:00   bone 1409 #> 2819  16.5      6 1866 1866-06-02 02:00:00  blood 1410 #> 2820  16.5      6 1866 1866-06-02 02:00:00   bone 1410 #> 2821   9.3      7 1866 1866-07-02 12:00:00  blood 1411 #> 2822   9.3      7 1866 1866-07-02 12:00:00   bone 1411 #> 2823  12.7      8 1866 1866-08-01 22:00:00  blood 1412 #> 2824  12.7      8 1866 1866-08-01 22:00:00   bone 1412 #> 2825   7.3      9 1866 1866-09-01 08:00:00  blood 1413 #> 2826   7.3      9 1866 1866-09-01 08:00:00   bone 1413 #> 2827  14.1     10 1866 1866-10-01 18:00:00  blood 1414 #> 2828  14.1     10 1866 1866-10-01 18:00:00   bone 1414 #> 2829   9.0     11 1866 1866-11-01 04:00:00  blood 1415 #> 2830   9.0     11 1866 1866-11-01 04:00:00   bone 1415 #> 2831   1.5     12 1866 1866-12-01 14:00:00  blood 1416 #> 2832   1.5     12 1866 1866-12-01 14:00:00   bone 1416 #> 2833   0.0      1 1867 1867-01-01 00:00:00  blood 1417 #> 2834   0.0      1 1867 1867-01-01 00:00:00   bone 1417 #> 2835   0.7      2 1867 1867-01-31 10:00:00  blood 1418 #> 2836   0.7      2 1867 1867-01-31 10:00:00   bone 1418 #> 2837   9.2      3 1867 1867-03-02 20:00:00  blood 1419 #> 2838   9.2      3 1867 1867-03-02 20:00:00   bone 1419 #> 2839   5.1      4 1867 1867-04-02 06:00:00  blood 1420 #> 2840   5.1      4 1867 1867-04-02 06:00:00   bone 1420 #> 2841   2.9      5 1867 1867-05-02 16:00:00  blood 1421 #> 2842   2.9      5 1867 1867-05-02 16:00:00   bone 1421 #> 2843   1.5      6 1867 1867-06-02 02:00:00  blood 1422 #> 2844   1.5      6 1867 1867-06-02 02:00:00   bone 1422 #> 2845   5.0      7 1867 1867-07-02 12:00:00  blood 1423 #> 2846   5.0      7 1867 1867-07-02 12:00:00   bone 1423 #> 2847   4.9      8 1867 1867-08-01 22:00:00  blood 1424 #> 2848   4.9      8 1867 1867-08-01 22:00:00   bone 1424 #> 2849   9.8      9 1867 1867-09-01 08:00:00  blood 1425 #> 2850   9.8      9 1867 1867-09-01 08:00:00   bone 1425 #> 2851  13.5     10 1867 1867-10-01 18:00:00  blood 1426 #> 2852  13.5     10 1867 1867-10-01 18:00:00   bone 1426 #> 2853   9.3     11 1867 1867-11-01 04:00:00  blood 1427 #> 2854   9.3     11 1867 1867-11-01 04:00:00   bone 1427 #> 2855  25.2     12 1867 1867-12-01 14:00:00  blood 1428 #> 2856  25.2     12 1867 1867-12-01 14:00:00   bone 1428 #> 2857  15.6      1 1868 1868-01-01 00:00:00  blood 1429 #> 2858  15.6      1 1868 1868-01-01 00:00:00   bone 1429 #> 2859  15.8      2 1868 1868-01-31 12:00:00  blood 1430 #> 2860  15.8      2 1868 1868-01-31 12:00:00   bone 1430 #> 2861  26.5      3 1868 1868-03-02 00:00:00  blood 1431 #> 2862  26.5      3 1868 1868-03-02 00:00:00   bone 1431 #> 2863  36.6      4 1868 1868-04-01 12:00:00  blood 1432 #> 2864  36.6      4 1868 1868-04-01 12:00:00   bone 1432 #> 2865  26.7      5 1868 1868-05-02 00:00:00  blood 1433 #> 2866  26.7      5 1868 1868-05-02 00:00:00   bone 1433 #> 2867  31.1      6 1868 1868-06-01 12:00:00  blood 1434 #> 2868  31.1      6 1868 1868-06-01 12:00:00   bone 1434 #> 2869  28.6      7 1868 1868-07-02 00:00:00  blood 1435 #> 2870  28.6      7 1868 1868-07-02 00:00:00   bone 1435 #> 2871  34.4      8 1868 1868-08-01 12:00:00  blood 1436 #> 2872  34.4      8 1868 1868-08-01 12:00:00   bone 1436 #> 2873  43.8      9 1868 1868-09-01 00:00:00  blood 1437 #> 2874  43.8      9 1868 1868-09-01 00:00:00   bone 1437 #> 2875  61.7     10 1868 1868-10-01 12:00:00  blood 1438 #> 2876  61.7     10 1868 1868-10-01 12:00:00   bone 1438 #> 2877  59.1     11 1868 1868-11-01 00:00:00  blood 1439 #> 2878  59.1     11 1868 1868-11-01 00:00:00   bone 1439 #> 2879  67.6     12 1868 1868-12-01 12:00:00  blood 1440 #> 2880  67.6     12 1868 1868-12-01 12:00:00   bone 1440 #> 2881  60.9      1 1869 1869-01-01 00:00:00  blood 1441 #> 2882  60.9      1 1869 1869-01-01 00:00:00   bone 1441 #> 2883  59.3      2 1869 1869-01-31 10:00:00  blood 1442 #> 2884  59.3      2 1869 1869-01-31 10:00:00   bone 1442 #> 2885  52.7      3 1869 1869-03-02 20:00:00  blood 1443 #> 2886  52.7      3 1869 1869-03-02 20:00:00   bone 1443 #> 2887  41.0      4 1869 1869-04-02 06:00:00  blood 1444 #> 2888  41.0      4 1869 1869-04-02 06:00:00   bone 1444 #> 2889 104.0      5 1869 1869-05-02 16:00:00  blood 1445 #> 2890 104.0      5 1869 1869-05-02 16:00:00   bone 1445 #> 2891 108.4      6 1869 1869-06-02 02:00:00  blood 1446 #> 2892 108.4      6 1869 1869-06-02 02:00:00   bone 1446 #> 2893  59.2      7 1869 1869-07-02 12:00:00  blood 1447 #> 2894  59.2      7 1869 1869-07-02 12:00:00   bone 1447 #> 2895  79.6      8 1869 1869-08-01 22:00:00  blood 1448 #> 2896  79.6      8 1869 1869-08-01 22:00:00   bone 1448 #> 2897  80.6      9 1869 1869-09-01 08:00:00  blood 1449 #> 2898  80.6      9 1869 1869-09-01 08:00:00   bone 1449 #> 2899  59.4     10 1869 1869-10-01 18:00:00  blood 1450 #> 2900  59.4     10 1869 1869-10-01 18:00:00   bone 1450 #> 2901  77.4     11 1869 1869-11-01 04:00:00  blood 1451 #> 2902  77.4     11 1869 1869-11-01 04:00:00   bone 1451 #> 2903 104.3     12 1869 1869-12-01 14:00:00  blood 1452 #> 2904 104.3     12 1869 1869-12-01 14:00:00   bone 1452 #> 2905  77.3      1 1870 1870-01-01 00:00:00  blood 1453 #> 2906  77.3      1 1870 1870-01-01 00:00:00   bone 1453 #> 2907 114.9      2 1870 1870-01-31 10:00:00  blood 1454 #> 2908 114.9      2 1870 1870-01-31 10:00:00   bone 1454 #> 2909 159.4      3 1870 1870-03-02 20:00:00  blood 1455 #> 2910 159.4      3 1870 1870-03-02 20:00:00   bone 1455 #> 2911 160.0      4 1870 1870-04-02 06:00:00  blood 1456 #> 2912 160.0      4 1870 1870-04-02 06:00:00   bone 1456 #> 2913 176.0      5 1870 1870-05-02 16:00:00  blood 1457 #> 2914 176.0      5 1870 1870-05-02 16:00:00   bone 1457 #> 2915 135.6      6 1870 1870-06-02 02:00:00  blood 1458 #> 2916 135.6      6 1870 1870-06-02 02:00:00   bone 1458 #> 2917 132.4      7 1870 1870-07-02 12:00:00  blood 1459 #> 2918 132.4      7 1870 1870-07-02 12:00:00   bone 1459 #> 2919 153.8      8 1870 1870-08-01 22:00:00  blood 1460 #> 2920 153.8      8 1870 1870-08-01 22:00:00   bone 1460 #> 2921 136.0      9 1870 1870-09-01 08:00:00  blood 1461 #> 2922 136.0      9 1870 1870-09-01 08:00:00   bone 1461 #> 2923 146.4     10 1870 1870-10-01 18:00:00  blood 1462 #> 2924 146.4     10 1870 1870-10-01 18:00:00   bone 1462 #> 2925 147.5     11 1870 1870-11-01 04:00:00  blood 1463 #> 2926 147.5     11 1870 1870-11-01 04:00:00   bone 1463 #> 2927 130.0     12 1870 1870-12-01 14:00:00  blood 1464 #> 2928 130.0     12 1870 1870-12-01 14:00:00   bone 1464 #> 2929  88.3      1 1871 1871-01-01 00:00:00  blood 1465 #> 2930  88.3      1 1871 1871-01-01 00:00:00   bone 1465 #> 2931 125.3      2 1871 1871-01-31 10:00:00  blood 1466 #> 2932 125.3      2 1871 1871-01-31 10:00:00   bone 1466 #> 2933 143.2      3 1871 1871-03-02 20:00:00  blood 1467 #> 2934 143.2      3 1871 1871-03-02 20:00:00   bone 1467 #> 2935 162.4      4 1871 1871-04-02 06:00:00  blood 1468 #> 2936 162.4      4 1871 1871-04-02 06:00:00   bone 1468 #> 2937 145.5      5 1871 1871-05-02 16:00:00  blood 1469 #> 2938 145.5      5 1871 1871-05-02 16:00:00   bone 1469 #> 2939  91.7      6 1871 1871-06-02 02:00:00  blood 1470 #> 2940  91.7      6 1871 1871-06-02 02:00:00   bone 1470 #> 2941 103.0      7 1871 1871-07-02 12:00:00  blood 1471 #> 2942 103.0      7 1871 1871-07-02 12:00:00   bone 1471 #> 2943 110.0      8 1871 1871-08-01 22:00:00  blood 1472 #> 2944 110.0      8 1871 1871-08-01 22:00:00   bone 1472 #> 2945  80.3      9 1871 1871-09-01 08:00:00  blood 1473 #> 2946  80.3      9 1871 1871-09-01 08:00:00   bone 1473 #> 2947  89.0     10 1871 1871-10-01 18:00:00  blood 1474 #> 2948  89.0     10 1871 1871-10-01 18:00:00   bone 1474 #> 2949 105.4     11 1871 1871-11-01 04:00:00  blood 1475 #> 2950 105.4     11 1871 1871-11-01 04:00:00   bone 1475 #> 2951  90.3     12 1871 1871-12-01 14:00:00  blood 1476 #> 2952  90.3     12 1871 1871-12-01 14:00:00   bone 1476 #> 2953  79.5      1 1872 1872-01-01 00:00:00  blood 1477 #> 2954  79.5      1 1872 1872-01-01 00:00:00   bone 1477 #> 2955 120.1      2 1872 1872-01-31 12:00:00  blood 1478 #> 2956 120.1      2 1872 1872-01-31 12:00:00   bone 1478 #> 2957  88.4      3 1872 1872-03-02 00:00:00  blood 1479 #> 2958  88.4      3 1872 1872-03-02 00:00:00   bone 1479 #> 2959 102.1      4 1872 1872-04-01 12:00:00  blood 1480 #> 2960 102.1      4 1872 1872-04-01 12:00:00   bone 1480 #> 2961 107.6      5 1872 1872-05-02 00:00:00  blood 1481 #> 2962 107.6      5 1872 1872-05-02 00:00:00   bone 1481 #> 2963 109.9      6 1872 1872-06-01 12:00:00  blood 1482 #> 2964 109.9      6 1872 1872-06-01 12:00:00   bone 1482 #> 2965 105.5      7 1872 1872-07-02 00:00:00  blood 1483 #> 2966 105.5      7 1872 1872-07-02 00:00:00   bone 1483 #> 2967  92.9      8 1872 1872-08-01 12:00:00  blood 1484 #> 2968  92.9      8 1872 1872-08-01 12:00:00   bone 1484 #> 2969 114.6      9 1872 1872-09-01 00:00:00  blood 1485 #> 2970 114.6      9 1872 1872-09-01 00:00:00   bone 1485 #> 2971 103.5     10 1872 1872-10-01 12:00:00  blood 1486 #> 2972 103.5     10 1872 1872-10-01 12:00:00   bone 1486 #> 2973 112.0     11 1872 1872-11-01 00:00:00  blood 1487 #> 2974 112.0     11 1872 1872-11-01 00:00:00   bone 1487 #> 2975  83.9     12 1872 1872-12-01 12:00:00  blood 1488 #> 2976  83.9     12 1872 1872-12-01 12:00:00   bone 1488 #> 2977  86.7      1 1873 1873-01-01 00:00:00  blood 1489 #> 2978  86.7      1 1873 1873-01-01 00:00:00   bone 1489 #> 2979 107.0      2 1873 1873-01-31 10:00:00  blood 1490 #> 2980 107.0      2 1873 1873-01-31 10:00:00   bone 1490 #> 2981  98.3      3 1873 1873-03-02 20:00:00  blood 1491 #> 2982  98.3      3 1873 1873-03-02 20:00:00   bone 1491 #> 2983  76.2      4 1873 1873-04-02 06:00:00  blood 1492 #> 2984  76.2      4 1873 1873-04-02 06:00:00   bone 1492 #> 2985  47.9      5 1873 1873-05-02 16:00:00  blood 1493 #> 2986  47.9      5 1873 1873-05-02 16:00:00   bone 1493 #> 2987  44.8      6 1873 1873-06-02 02:00:00  blood 1494 #> 2988  44.8      6 1873 1873-06-02 02:00:00   bone 1494 #> 2989  66.9      7 1873 1873-07-02 12:00:00  blood 1495 #> 2990  66.9      7 1873 1873-07-02 12:00:00   bone 1495 #> 2991  68.2      8 1873 1873-08-01 22:00:00  blood 1496 #> 2992  68.2      8 1873 1873-08-01 22:00:00   bone 1496 #> 2993  47.5      9 1873 1873-09-01 08:00:00  blood 1497 #> 2994  47.5      9 1873 1873-09-01 08:00:00   bone 1497 #> 2995  47.4     10 1873 1873-10-01 18:00:00  blood 1498 #> 2996  47.4     10 1873 1873-10-01 18:00:00   bone 1498 #> 2997  55.4     11 1873 1873-11-01 04:00:00  blood 1499 #> 2998  55.4     11 1873 1873-11-01 04:00:00   bone 1499 #> 2999  49.2     12 1873 1873-12-01 14:00:00  blood 1500 #> 3000  49.2     12 1873 1873-12-01 14:00:00   bone 1500 #> 3001  60.8      1 1874 1874-01-01 00:00:00  blood 1501 #> 3002  60.8      1 1874 1874-01-01 00:00:00   bone 1501 #> 3003  64.2      2 1874 1874-01-31 10:00:00  blood 1502 #> 3004  64.2      2 1874 1874-01-31 10:00:00   bone 1502 #> 3005  46.4      3 1874 1874-03-02 20:00:00  blood 1503 #> 3006  46.4      3 1874 1874-03-02 20:00:00   bone 1503 #> 3007  32.0      4 1874 1874-04-02 06:00:00  blood 1504 #> 3008  32.0      4 1874 1874-04-02 06:00:00   bone 1504 #> 3009  44.6      5 1874 1874-05-02 16:00:00  blood 1505 #> 3010  44.6      5 1874 1874-05-02 16:00:00   bone 1505 #> 3011  38.2      6 1874 1874-06-02 02:00:00  blood 1506 #> 3012  38.2      6 1874 1874-06-02 02:00:00   bone 1506 #> 3013  67.8      7 1874 1874-07-02 12:00:00  blood 1507 #> 3014  67.8      7 1874 1874-07-02 12:00:00   bone 1507 #> 3015  61.3      8 1874 1874-08-01 22:00:00  blood 1508 #> 3016  61.3      8 1874 1874-08-01 22:00:00   bone 1508 #> 3017  28.0      9 1874 1874-09-01 08:00:00  blood 1509 #> 3018  28.0      9 1874 1874-09-01 08:00:00   bone 1509 #> 3019  34.3     10 1874 1874-10-01 18:00:00  blood 1510 #> 3020  34.3     10 1874 1874-10-01 18:00:00   bone 1510 #> 3021  28.9     11 1874 1874-11-01 04:00:00  blood 1511 #> 3022  28.9     11 1874 1874-11-01 04:00:00   bone 1511 #> 3023  29.3     12 1874 1874-12-01 14:00:00  blood 1512 #> 3024  29.3     12 1874 1874-12-01 14:00:00   bone 1512 #> 3025  14.6      1 1875 1875-01-01 00:00:00  blood 1513 #> 3026  14.6      1 1875 1875-01-01 00:00:00   bone 1513 #> 3027  22.2      2 1875 1875-01-31 10:00:00  blood 1514 #> 3028  22.2      2 1875 1875-01-31 10:00:00   bone 1514 #> 3029  33.8      3 1875 1875-03-02 20:00:00  blood 1515 #> 3030  33.8      3 1875 1875-03-02 20:00:00   bone 1515 #> 3031  29.1      4 1875 1875-04-02 06:00:00  blood 1516 #> 3032  29.1      4 1875 1875-04-02 06:00:00   bone 1516 #> 3033  11.5      5 1875 1875-05-02 16:00:00  blood 1517 #> 3034  11.5      5 1875 1875-05-02 16:00:00   bone 1517 #> 3035  23.9      6 1875 1875-06-02 02:00:00  blood 1518 #> 3036  23.9      6 1875 1875-06-02 02:00:00   bone 1518 #> 3037  12.5      7 1875 1875-07-02 12:00:00  blood 1519 #> 3038  12.5      7 1875 1875-07-02 12:00:00   bone 1519 #> 3039  14.6      8 1875 1875-08-01 22:00:00  blood 1520 #> 3040  14.6      8 1875 1875-08-01 22:00:00   bone 1520 #> 3041   2.4      9 1875 1875-09-01 08:00:00  blood 1521 #> 3042   2.4      9 1875 1875-09-01 08:00:00   bone 1521 #> 3043  12.7     10 1875 1875-10-01 18:00:00  blood 1522 #> 3044  12.7     10 1875 1875-10-01 18:00:00   bone 1522 #> 3045  17.7     11 1875 1875-11-01 04:00:00  blood 1523 #> 3046  17.7     11 1875 1875-11-01 04:00:00   bone 1523 #> 3047   9.9     12 1875 1875-12-01 14:00:00  blood 1524 #> 3048   9.9     12 1875 1875-12-01 14:00:00   bone 1524 #> 3049  14.3      1 1876 1876-01-01 00:00:00  blood 1525 #> 3050  14.3      1 1876 1876-01-01 00:00:00   bone 1525 #> 3051  15.0      2 1876 1876-01-31 12:00:00  blood 1526 #> 3052  15.0      2 1876 1876-01-31 12:00:00   bone 1526 #> 3053  31.2      3 1876 1876-03-02 00:00:00  blood 1527 #> 3054  31.2      3 1876 1876-03-02 00:00:00   bone 1527 #> 3055   2.3      4 1876 1876-04-01 12:00:00  blood 1528 #> 3056   2.3      4 1876 1876-04-01 12:00:00   bone 1528 #> 3057   5.1      5 1876 1876-05-02 00:00:00  blood 1529 #> 3058   5.1      5 1876 1876-05-02 00:00:00   bone 1529 #> 3059   1.6      6 1876 1876-06-01 12:00:00  blood 1530 #> 3060   1.6      6 1876 1876-06-01 12:00:00   bone 1530 #> 3061  15.2      7 1876 1876-07-02 00:00:00  blood 1531 #> 3062  15.2      7 1876 1876-07-02 00:00:00   bone 1531 #> 3063   8.8      8 1876 1876-08-01 12:00:00  blood 1532 #> 3064   8.8      8 1876 1876-08-01 12:00:00   bone 1532 #> 3065   9.9      9 1876 1876-09-01 00:00:00  blood 1533 #> 3066   9.9      9 1876 1876-09-01 00:00:00   bone 1533 #> 3067  14.3     10 1876 1876-10-01 12:00:00  blood 1534 #> 3068  14.3     10 1876 1876-10-01 12:00:00   bone 1534 #> 3069   9.9     11 1876 1876-11-01 00:00:00  blood 1535 #> 3070   9.9     11 1876 1876-11-01 00:00:00   bone 1535 #> 3071   8.2     12 1876 1876-12-01 12:00:00  blood 1536 #> 3072   8.2     12 1876 1876-12-01 12:00:00   bone 1536 #> 3073  24.4      1 1877 1877-01-01 00:00:00  blood 1537 #> 3074  24.4      1 1877 1877-01-01 00:00:00   bone 1537 #> 3075   8.7      2 1877 1877-01-31 10:00:00  blood 1538 #> 3076   8.7      2 1877 1877-01-31 10:00:00   bone 1538 #> 3077  11.7      3 1877 1877-03-02 20:00:00  blood 1539 #> 3078  11.7      3 1877 1877-03-02 20:00:00   bone 1539 #> 3079  15.8      4 1877 1877-04-02 06:00:00  blood 1540 #> 3080  15.8      4 1877 1877-04-02 06:00:00   bone 1540 #> 3081  21.2      5 1877 1877-05-02 16:00:00  blood 1541 #> 3082  21.2      5 1877 1877-05-02 16:00:00   bone 1541 #> 3083  13.4      6 1877 1877-06-02 02:00:00  blood 1542 #> 3084  13.4      6 1877 1877-06-02 02:00:00   bone 1542 #> 3085   5.9      7 1877 1877-07-02 12:00:00  blood 1543 #> 3086   5.9      7 1877 1877-07-02 12:00:00   bone 1543 #> 3087   6.3      8 1877 1877-08-01 22:00:00  blood 1544 #> 3088   6.3      8 1877 1877-08-01 22:00:00   bone 1544 #> 3089  16.4      9 1877 1877-09-01 08:00:00  blood 1545 #> 3090  16.4      9 1877 1877-09-01 08:00:00   bone 1545 #> 3091   6.7     10 1877 1877-10-01 18:00:00  blood 1546 #> 3092   6.7     10 1877 1877-10-01 18:00:00   bone 1546 #> 3093  14.5     11 1877 1877-11-01 04:00:00  blood 1547 #> 3094  14.5     11 1877 1877-11-01 04:00:00   bone 1547 #> 3095   2.3     12 1877 1877-12-01 14:00:00  blood 1548 #> 3096   2.3     12 1877 1877-12-01 14:00:00   bone 1548 #> 3097   3.3      1 1878 1878-01-01 00:00:00  blood 1549 #> 3098   3.3      1 1878 1878-01-01 00:00:00   bone 1549 #> 3099   6.0      2 1878 1878-01-31 10:00:00  blood 1550 #> 3100   6.0      2 1878 1878-01-31 10:00:00   bone 1550 #> 3101   7.8      3 1878 1878-03-02 20:00:00  blood 1551 #> 3102   7.8      3 1878 1878-03-02 20:00:00   bone 1551 #> 3103   0.1      4 1878 1878-04-02 06:00:00  blood 1552 #> 3104   0.1      4 1878 1878-04-02 06:00:00   bone 1552 #> 3105   5.8      5 1878 1878-05-02 16:00:00  blood 1553 #> 3106   5.8      5 1878 1878-05-02 16:00:00   bone 1553 #> 3107   6.4      6 1878 1878-06-02 02:00:00  blood 1554 #> 3108   6.4      6 1878 1878-06-02 02:00:00   bone 1554 #> 3109   0.1      7 1878 1878-07-02 12:00:00  blood 1555 #> 3110   0.1      7 1878 1878-07-02 12:00:00   bone 1555 #> 3111   0.0      8 1878 1878-08-01 22:00:00  blood 1556 #> 3112   0.0      8 1878 1878-08-01 22:00:00   bone 1556 #> 3113   5.3      9 1878 1878-09-01 08:00:00  blood 1557 #> 3114   5.3      9 1878 1878-09-01 08:00:00   bone 1557 #> 3115   1.1     10 1878 1878-10-01 18:00:00  blood 1558 #> 3116   1.1     10 1878 1878-10-01 18:00:00   bone 1558 #> 3117   4.1     11 1878 1878-11-01 04:00:00  blood 1559 #> 3118   4.1     11 1878 1878-11-01 04:00:00   bone 1559 #> 3119   0.5     12 1878 1878-12-01 14:00:00  blood 1560 #> 3120   0.5     12 1878 1878-12-01 14:00:00   bone 1560 #> 3121   0.8      1 1879 1879-01-01 00:00:00  blood 1561 #> 3122   0.8      1 1879 1879-01-01 00:00:00   bone 1561 #> 3123   0.6      2 1879 1879-01-31 10:00:00  blood 1562 #> 3124   0.6      2 1879 1879-01-31 10:00:00   bone 1562 #> 3125   0.0      3 1879 1879-03-02 20:00:00  blood 1563 #> 3126   0.0      3 1879 1879-03-02 20:00:00   bone 1563 #> 3127   6.2      4 1879 1879-04-02 06:00:00  blood 1564 #> 3128   6.2      4 1879 1879-04-02 06:00:00   bone 1564 #> 3129   2.4      5 1879 1879-05-02 16:00:00  blood 1565 #> 3130   2.4      5 1879 1879-05-02 16:00:00   bone 1565 #> 3131   4.8      6 1879 1879-06-02 02:00:00  blood 1566 #> 3132   4.8      6 1879 1879-06-02 02:00:00   bone 1566 #> 3133   7.5      7 1879 1879-07-02 12:00:00  blood 1567 #> 3134   7.5      7 1879 1879-07-02 12:00:00   bone 1567 #> 3135  10.7      8 1879 1879-08-01 22:00:00  blood 1568 #> 3136  10.7      8 1879 1879-08-01 22:00:00   bone 1568 #> 3137   6.1      9 1879 1879-09-01 08:00:00  blood 1569 #> 3138   6.1      9 1879 1879-09-01 08:00:00   bone 1569 #> 3139  12.3     10 1879 1879-10-01 18:00:00  blood 1570 #> 3140  12.3     10 1879 1879-10-01 18:00:00   bone 1570 #> 3141  12.9     11 1879 1879-11-01 04:00:00  blood 1571 #> 3142  12.9     11 1879 1879-11-01 04:00:00   bone 1571 #> 3143   7.2     12 1879 1879-12-01 14:00:00  blood 1572 #> 3144   7.2     12 1879 1879-12-01 14:00:00   bone 1572 #> 3145  24.0      1 1880 1880-01-01 00:00:00  blood 1573 #> 3146  24.0      1 1880 1880-01-01 00:00:00   bone 1573 #> 3147  27.5      2 1880 1880-01-31 12:00:00  blood 1574 #> 3148  27.5      2 1880 1880-01-31 12:00:00   bone 1574 #> 3149  19.5      3 1880 1880-03-02 00:00:00  blood 1575 #> 3150  19.5      3 1880 1880-03-02 00:00:00   bone 1575 #> 3151  19.3      4 1880 1880-04-01 12:00:00  blood 1576 #> 3152  19.3      4 1880 1880-04-01 12:00:00   bone 1576 #> 3153  23.5      5 1880 1880-05-02 00:00:00  blood 1577 #> 3154  23.5      5 1880 1880-05-02 00:00:00   bone 1577 #> 3155  34.1      6 1880 1880-06-01 12:00:00  blood 1578 #> 3156  34.1      6 1880 1880-06-01 12:00:00   bone 1578 #> 3157  21.9      7 1880 1880-07-02 00:00:00  blood 1579 #> 3158  21.9      7 1880 1880-07-02 00:00:00   bone 1579 #> 3159  48.1      8 1880 1880-08-01 12:00:00  blood 1580 #> 3160  48.1      8 1880 1880-08-01 12:00:00   bone 1580 #> 3161  66.0      9 1880 1880-09-01 00:00:00  blood 1581 #> 3162  66.0      9 1880 1880-09-01 00:00:00   bone 1581 #> 3163  43.0     10 1880 1880-10-01 12:00:00  blood 1582 #> 3164  43.0     10 1880 1880-10-01 12:00:00   bone 1582 #> 3165  30.7     11 1880 1880-11-01 00:00:00  blood 1583 #> 3166  30.7     11 1880 1880-11-01 00:00:00   bone 1583 #> 3167  29.6     12 1880 1880-12-01 12:00:00  blood 1584 #> 3168  29.6     12 1880 1880-12-01 12:00:00   bone 1584 #> 3169  36.4      1 1881 1881-01-01 00:00:00  blood 1585 #> 3170  36.4      1 1881 1881-01-01 00:00:00   bone 1585 #> 3171  53.2      2 1881 1881-01-31 10:00:00  blood 1586 #> 3172  53.2      2 1881 1881-01-31 10:00:00   bone 1586 #> 3173  51.5      3 1881 1881-03-02 20:00:00  blood 1587 #> 3174  51.5      3 1881 1881-03-02 20:00:00   bone 1587 #> 3175  51.7      4 1881 1881-04-02 06:00:00  blood 1588 #> 3176  51.7      4 1881 1881-04-02 06:00:00   bone 1588 #> 3177  43.5      5 1881 1881-05-02 16:00:00  blood 1589 #> 3178  43.5      5 1881 1881-05-02 16:00:00   bone 1589 #> 3179  60.5      6 1881 1881-06-02 02:00:00  blood 1590 #> 3180  60.5      6 1881 1881-06-02 02:00:00   bone 1590 #> 3181  76.9      7 1881 1881-07-02 12:00:00  blood 1591 #> 3182  76.9      7 1881 1881-07-02 12:00:00   bone 1591 #> 3183  58.0      8 1881 1881-08-01 22:00:00  blood 1592 #> 3184  58.0      8 1881 1881-08-01 22:00:00   bone 1592 #> 3185  53.2      9 1881 1881-09-01 08:00:00  blood 1593 #> 3186  53.2      9 1881 1881-09-01 08:00:00   bone 1593 #> 3187  64.0     10 1881 1881-10-01 18:00:00  blood 1594 #> 3188  64.0     10 1881 1881-10-01 18:00:00   bone 1594 #> 3189  54.8     11 1881 1881-11-01 04:00:00  blood 1595 #> 3190  54.8     11 1881 1881-11-01 04:00:00   bone 1595 #> 3191  47.3     12 1881 1881-12-01 14:00:00  blood 1596 #> 3192  47.3     12 1881 1881-12-01 14:00:00   bone 1596 #> 3193  45.0      1 1882 1882-01-01 00:00:00  blood 1597 #> 3194  45.0      1 1882 1882-01-01 00:00:00   bone 1597 #> 3195  69.3      2 1882 1882-01-31 10:00:00  blood 1598 #> 3196  69.3      2 1882 1882-01-31 10:00:00   bone 1598 #> 3197  67.5      3 1882 1882-03-02 20:00:00  blood 1599 #> 3198  67.5      3 1882 1882-03-02 20:00:00   bone 1599 #> 3199  95.8      4 1882 1882-04-02 06:00:00  blood 1600 #> 3200  95.8      4 1882 1882-04-02 06:00:00   bone 1600 #> 3201  64.1      5 1882 1882-05-02 16:00:00  blood 1601 #> 3202  64.1      5 1882 1882-05-02 16:00:00   bone 1601 #> 3203  45.2      6 1882 1882-06-02 02:00:00  blood 1602 #> 3204  45.2      6 1882 1882-06-02 02:00:00   bone 1602 #> 3205  45.4      7 1882 1882-07-02 12:00:00  blood 1603 #> 3206  45.4      7 1882 1882-07-02 12:00:00   bone 1603 #> 3207  40.4      8 1882 1882-08-01 22:00:00  blood 1604 #> 3208  40.4      8 1882 1882-08-01 22:00:00   bone 1604 #> 3209  57.7      9 1882 1882-09-01 08:00:00  blood 1605 #> 3210  57.7      9 1882 1882-09-01 08:00:00   bone 1605 #> 3211  59.2     10 1882 1882-10-01 18:00:00  blood 1606 #> 3212  59.2     10 1882 1882-10-01 18:00:00   bone 1606 #> 3213  84.4     11 1882 1882-11-01 04:00:00  blood 1607 #> 3214  84.4     11 1882 1882-11-01 04:00:00   bone 1607 #> 3215  41.8     12 1882 1882-12-01 14:00:00  blood 1608 #> 3216  41.8     12 1882 1882-12-01 14:00:00   bone 1608 #> 3217  60.6      1 1883 1883-01-01 00:00:00  blood 1609 #> 3218  60.6      1 1883 1883-01-01 00:00:00   bone 1609 #> 3219  46.9      2 1883 1883-01-31 10:00:00  blood 1610 #> 3220  46.9      2 1883 1883-01-31 10:00:00   bone 1610 #> 3221  42.8      3 1883 1883-03-02 20:00:00  blood 1611 #> 3222  42.8      3 1883 1883-03-02 20:00:00   bone 1611 #> 3223  82.1      4 1883 1883-04-02 06:00:00  blood 1612 #> 3224  82.1      4 1883 1883-04-02 06:00:00   bone 1612 #> 3225  32.1      5 1883 1883-05-02 16:00:00  blood 1613 #> 3226  32.1      5 1883 1883-05-02 16:00:00   bone 1613 #> 3227  76.5      6 1883 1883-06-02 02:00:00  blood 1614 #> 3228  76.5      6 1883 1883-06-02 02:00:00   bone 1614 #> 3229  80.6      7 1883 1883-07-02 12:00:00  blood 1615 #> 3230  80.6      7 1883 1883-07-02 12:00:00   bone 1615 #> 3231  46.0      8 1883 1883-08-01 22:00:00  blood 1616 #> 3232  46.0      8 1883 1883-08-01 22:00:00   bone 1616 #> 3233  52.6      9 1883 1883-09-01 08:00:00  blood 1617 #> 3234  52.6      9 1883 1883-09-01 08:00:00   bone 1617 #> 3235  83.8     10 1883 1883-10-01 18:00:00  blood 1618 #> 3236  83.8     10 1883 1883-10-01 18:00:00   bone 1618 #> 3237  84.5     11 1883 1883-11-01 04:00:00  blood 1619 #> 3238  84.5     11 1883 1883-11-01 04:00:00   bone 1619 #> 3239  75.9     12 1883 1883-12-01 14:00:00  blood 1620 #> 3240  75.9     12 1883 1883-12-01 14:00:00   bone 1620 #> 3241  91.5      1 1884 1884-01-01 00:00:00  blood 1621 #> 3242  91.5      1 1884 1884-01-01 00:00:00   bone 1621 #> 3243  86.9      2 1884 1884-01-31 12:00:00  blood 1622 #> 3244  86.9      2 1884 1884-01-31 12:00:00   bone 1622 #> 3245  86.8      3 1884 1884-03-02 00:00:00  blood 1623 #> 3246  86.8      3 1884 1884-03-02 00:00:00   bone 1623 #> 3247  76.1      4 1884 1884-04-01 12:00:00  blood 1624 #> 3248  76.1      4 1884 1884-04-01 12:00:00   bone 1624 #> 3249  66.5      5 1884 1884-05-02 00:00:00  blood 1625 #> 3250  66.5      5 1884 1884-05-02 00:00:00   bone 1625 #> 3251  51.2      6 1884 1884-06-01 12:00:00  blood 1626 #> 3252  51.2      6 1884 1884-06-01 12:00:00   bone 1626 #> 3253  53.1      7 1884 1884-07-02 00:00:00  blood 1627 #> 3254  53.1      7 1884 1884-07-02 00:00:00   bone 1627 #> 3255  55.8      8 1884 1884-08-01 12:00:00  blood 1628 #> 3256  55.8      8 1884 1884-08-01 12:00:00   bone 1628 #> 3257  61.9      9 1884 1884-09-01 00:00:00  blood 1629 #> 3258  61.9      9 1884 1884-09-01 00:00:00   bone 1629 #> 3259  47.8     10 1884 1884-10-01 12:00:00  blood 1630 #> 3260  47.8     10 1884 1884-10-01 12:00:00   bone 1630 #> 3261  36.6     11 1884 1884-11-01 00:00:00  blood 1631 #> 3262  36.6     11 1884 1884-11-01 00:00:00   bone 1631 #> 3263  47.2     12 1884 1884-12-01 12:00:00  blood 1632 #> 3264  47.2     12 1884 1884-12-01 12:00:00   bone 1632 #> 3265  42.8      1 1885 1885-01-01 00:00:00  blood 1633 #> 3266  42.8      1 1885 1885-01-01 00:00:00   bone 1633 #> 3267  71.8      2 1885 1885-01-31 10:00:00  blood 1634 #> 3268  71.8      2 1885 1885-01-31 10:00:00   bone 1634 #> 3269  49.8      3 1885 1885-03-02 20:00:00  blood 1635 #> 3270  49.8      3 1885 1885-03-02 20:00:00   bone 1635 #> 3271  55.0      4 1885 1885-04-02 06:00:00  blood 1636 #> 3272  55.0      4 1885 1885-04-02 06:00:00   bone 1636 #> 3273  73.0      5 1885 1885-05-02 16:00:00  blood 1637 #> 3274  73.0      5 1885 1885-05-02 16:00:00   bone 1637 #> 3275  83.7      6 1885 1885-06-02 02:00:00  blood 1638 #> 3276  83.7      6 1885 1885-06-02 02:00:00   bone 1638 #> 3277  66.5      7 1885 1885-07-02 12:00:00  blood 1639 #> 3278  66.5      7 1885 1885-07-02 12:00:00   bone 1639 #> 3279  50.0      8 1885 1885-08-01 22:00:00  blood 1640 #> 3280  50.0      8 1885 1885-08-01 22:00:00   bone 1640 #> 3281  39.6      9 1885 1885-09-01 08:00:00  blood 1641 #> 3282  39.6      9 1885 1885-09-01 08:00:00   bone 1641 #> 3283  38.7     10 1885 1885-10-01 18:00:00  blood 1642 #> 3284  38.7     10 1885 1885-10-01 18:00:00   bone 1642 #> 3285  33.3     11 1885 1885-11-01 04:00:00  blood 1643 #> 3286  33.3     11 1885 1885-11-01 04:00:00   bone 1643 #> 3287  21.7     12 1885 1885-12-01 14:00:00  blood 1644 #> 3288  21.7     12 1885 1885-12-01 14:00:00   bone 1644 #> 3289  29.9      1 1886 1886-01-01 00:00:00  blood 1645 #> 3290  29.9      1 1886 1886-01-01 00:00:00   bone 1645 #> 3291  25.9      2 1886 1886-01-31 10:00:00  blood 1646 #> 3292  25.9      2 1886 1886-01-31 10:00:00   bone 1646 #> 3293  57.3      3 1886 1886-03-02 20:00:00  blood 1647 #> 3294  57.3      3 1886 1886-03-02 20:00:00   bone 1647 #> 3295  43.7      4 1886 1886-04-02 06:00:00  blood 1648 #> 3296  43.7      4 1886 1886-04-02 06:00:00   bone 1648 #> 3297  30.7      5 1886 1886-05-02 16:00:00  blood 1649 #> 3298  30.7      5 1886 1886-05-02 16:00:00   bone 1649 #> 3299  27.1      6 1886 1886-06-02 02:00:00  blood 1650 #> 3300  27.1      6 1886 1886-06-02 02:00:00   bone 1650 #> 3301  30.3      7 1886 1886-07-02 12:00:00  blood 1651 #> 3302  30.3      7 1886 1886-07-02 12:00:00   bone 1651 #> 3303  16.9      8 1886 1886-08-01 22:00:00  blood 1652 #> 3304  16.9      8 1886 1886-08-01 22:00:00   bone 1652 #> 3305  21.4      9 1886 1886-09-01 08:00:00  blood 1653 #> 3306  21.4      9 1886 1886-09-01 08:00:00   bone 1653 #> 3307   8.6     10 1886 1886-10-01 18:00:00  blood 1654 #> 3308   8.6     10 1886 1886-10-01 18:00:00   bone 1654 #> 3309   0.3     11 1886 1886-11-01 04:00:00  blood 1655 #> 3310   0.3     11 1886 1886-11-01 04:00:00   bone 1655 #> 3311  12.4     12 1886 1886-12-01 14:00:00  blood 1656 #> 3312  12.4     12 1886 1886-12-01 14:00:00   bone 1656 #> 3313  10.3      1 1887 1887-01-01 00:00:00  blood 1657 #> 3314  10.3      1 1887 1887-01-01 00:00:00   bone 1657 #> 3315  13.2      2 1887 1887-01-31 10:00:00  blood 1658 #> 3316  13.2      2 1887 1887-01-31 10:00:00   bone 1658 #> 3317   4.2      3 1887 1887-03-02 20:00:00  blood 1659 #> 3318   4.2      3 1887 1887-03-02 20:00:00   bone 1659 #> 3319   6.9      4 1887 1887-04-02 06:00:00  blood 1660 #> 3320   6.9      4 1887 1887-04-02 06:00:00   bone 1660 #> 3321  20.0      5 1887 1887-05-02 16:00:00  blood 1661 #> 3322  20.0      5 1887 1887-05-02 16:00:00   bone 1661 #> 3323  15.7      6 1887 1887-06-02 02:00:00  blood 1662 #> 3324  15.7      6 1887 1887-06-02 02:00:00   bone 1662 #> 3325  23.3      7 1887 1887-07-02 12:00:00  blood 1663 #> 3326  23.3      7 1887 1887-07-02 12:00:00   bone 1663 #> 3327  21.4      8 1887 1887-08-01 22:00:00  blood 1664 #> 3328  21.4      8 1887 1887-08-01 22:00:00   bone 1664 #> 3329   7.4      9 1887 1887-09-01 08:00:00  blood 1665 #> 3330   7.4      9 1887 1887-09-01 08:00:00   bone 1665 #> 3331   6.6     10 1887 1887-10-01 18:00:00  blood 1666 #> 3332   6.6     10 1887 1887-10-01 18:00:00   bone 1666 #> 3333   6.9     11 1887 1887-11-01 04:00:00  blood 1667 #> 3334   6.9     11 1887 1887-11-01 04:00:00   bone 1667 #> 3335  20.7     12 1887 1887-12-01 14:00:00  blood 1668 #> 3336  20.7     12 1887 1887-12-01 14:00:00   bone 1668 #> 3337  12.7      1 1888 1888-01-01 00:00:00  blood 1669 #> 3338  12.7      1 1888 1888-01-01 00:00:00   bone 1669 #> 3339   7.1      2 1888 1888-01-31 12:00:00  blood 1670 #> 3340   7.1      2 1888 1888-01-31 12:00:00   bone 1670 #> 3341   7.8      3 1888 1888-03-02 00:00:00  blood 1671 #> 3342   7.8      3 1888 1888-03-02 00:00:00   bone 1671 #> 3343   5.1      4 1888 1888-04-01 12:00:00  blood 1672 #> 3344   5.1      4 1888 1888-04-01 12:00:00   bone 1672 #> 3345   7.0      5 1888 1888-05-02 00:00:00  blood 1673 #> 3346   7.0      5 1888 1888-05-02 00:00:00   bone 1673 #> 3347   7.1      6 1888 1888-06-01 12:00:00  blood 1674 #> 3348   7.1      6 1888 1888-06-01 12:00:00   bone 1674 #> 3349   3.1      7 1888 1888-07-02 00:00:00  blood 1675 #> 3350   3.1      7 1888 1888-07-02 00:00:00   bone 1675 #> 3351   2.8      8 1888 1888-08-01 12:00:00  blood 1676 #> 3352   2.8      8 1888 1888-08-01 12:00:00   bone 1676 #> 3353   8.8      9 1888 1888-09-01 00:00:00  blood 1677 #> 3354   8.8      9 1888 1888-09-01 00:00:00   bone 1677 #> 3355   2.1     10 1888 1888-10-01 12:00:00  blood 1678 #> 3356   2.1     10 1888 1888-10-01 12:00:00   bone 1678 #> 3357  10.7     11 1888 1888-11-01 00:00:00  blood 1679 #> 3358  10.7     11 1888 1888-11-01 00:00:00   bone 1679 #> 3359   6.7     12 1888 1888-12-01 12:00:00  blood 1680 #> 3360   6.7     12 1888 1888-12-01 12:00:00   bone 1680 #> 3361   0.8      1 1889 1889-01-01 00:00:00  blood 1681 #> 3362   0.8      1 1889 1889-01-01 00:00:00   bone 1681 #> 3363   8.5      2 1889 1889-01-31 10:00:00  blood 1682 #> 3364   8.5      2 1889 1889-01-31 10:00:00   bone 1682 #> 3365   7.0      3 1889 1889-03-02 20:00:00  blood 1683 #> 3366   7.0      3 1889 1889-03-02 20:00:00   bone 1683 #> 3367   4.3      4 1889 1889-04-02 06:00:00  blood 1684 #> 3368   4.3      4 1889 1889-04-02 06:00:00   bone 1684 #> 3369   2.4      5 1889 1889-05-02 16:00:00  blood 1685 #> 3370   2.4      5 1889 1889-05-02 16:00:00   bone 1685 #> 3371   6.4      6 1889 1889-06-02 02:00:00  blood 1686 #> 3372   6.4      6 1889 1889-06-02 02:00:00   bone 1686 #> 3373   9.7      7 1889 1889-07-02 12:00:00  blood 1687 #> 3374   9.7      7 1889 1889-07-02 12:00:00   bone 1687 #> 3375  20.6      8 1889 1889-08-01 22:00:00  blood 1688 #> 3376  20.6      8 1889 1889-08-01 22:00:00   bone 1688 #> 3377   6.5      9 1889 1889-09-01 08:00:00  blood 1689 #> 3378   6.5      9 1889 1889-09-01 08:00:00   bone 1689 #> 3379   2.1     10 1889 1889-10-01 18:00:00  blood 1690 #> 3380   2.1     10 1889 1889-10-01 18:00:00   bone 1690 #> 3381   0.2     11 1889 1889-11-01 04:00:00  blood 1691 #> 3382   0.2     11 1889 1889-11-01 04:00:00   bone 1691 #> 3383   6.7     12 1889 1889-12-01 14:00:00  blood 1692 #> 3384   6.7     12 1889 1889-12-01 14:00:00   bone 1692 #> 3385   5.3      1 1890 1890-01-01 00:00:00  blood 1693 #> 3386   5.3      1 1890 1890-01-01 00:00:00   bone 1693 #> 3387   0.6      2 1890 1890-01-31 10:00:00  blood 1694 #> 3388   0.6      2 1890 1890-01-31 10:00:00   bone 1694 #> 3389   5.1      3 1890 1890-03-02 20:00:00  blood 1695 #> 3390   5.1      3 1890 1890-03-02 20:00:00   bone 1695 #> 3391   1.6      4 1890 1890-04-02 06:00:00  blood 1696 #> 3392   1.6      4 1890 1890-04-02 06:00:00   bone 1696 #> 3393   4.8      5 1890 1890-05-02 16:00:00  blood 1697 #> 3394   4.8      5 1890 1890-05-02 16:00:00   bone 1697 #> 3395   1.3      6 1890 1890-06-02 02:00:00  blood 1698 #> 3396   1.3      6 1890 1890-06-02 02:00:00   bone 1698 #> 3397  11.6      7 1890 1890-07-02 12:00:00  blood 1699 #> 3398  11.6      7 1890 1890-07-02 12:00:00   bone 1699 #> 3399   8.5      8 1890 1890-08-01 22:00:00  blood 1700 #> 3400   8.5      8 1890 1890-08-01 22:00:00   bone 1700 #> 3401  17.2      9 1890 1890-09-01 08:00:00  blood 1701 #> 3402  17.2      9 1890 1890-09-01 08:00:00   bone 1701 #> 3403  11.2     10 1890 1890-10-01 18:00:00  blood 1702 #> 3404  11.2     10 1890 1890-10-01 18:00:00   bone 1702 #> 3405   9.6     11 1890 1890-11-01 04:00:00  blood 1703 #> 3406   9.6     11 1890 1890-11-01 04:00:00   bone 1703 #> 3407   7.8     12 1890 1890-12-01 14:00:00  blood 1704 #> 3408   7.8     12 1890 1890-12-01 14:00:00   bone 1704 #> 3409  13.5      1 1891 1891-01-01 00:00:00  blood 1705 #> 3410  13.5      1 1891 1891-01-01 00:00:00   bone 1705 #> 3411  22.2      2 1891 1891-01-31 10:00:00  blood 1706 #> 3412  22.2      2 1891 1891-01-31 10:00:00   bone 1706 #> 3413  10.4      3 1891 1891-03-02 20:00:00  blood 1707 #> 3414  10.4      3 1891 1891-03-02 20:00:00   bone 1707 #> 3415  20.5      4 1891 1891-04-02 06:00:00  blood 1708 #> 3416  20.5      4 1891 1891-04-02 06:00:00   bone 1708 #> 3417  41.1      5 1891 1891-05-02 16:00:00  blood 1709 #> 3418  41.1      5 1891 1891-05-02 16:00:00   bone 1709 #> 3419  48.3      6 1891 1891-06-02 02:00:00  blood 1710 #> 3420  48.3      6 1891 1891-06-02 02:00:00   bone 1710 #> 3421  58.8      7 1891 1891-07-02 12:00:00  blood 1711 #> 3422  58.8      7 1891 1891-07-02 12:00:00   bone 1711 #> 3423  33.2      8 1891 1891-08-01 22:00:00  blood 1712 #> 3424  33.2      8 1891 1891-08-01 22:00:00   bone 1712 #> 3425  53.8      9 1891 1891-09-01 08:00:00  blood 1713 #> 3426  53.8      9 1891 1891-09-01 08:00:00   bone 1713 #> 3427  51.5     10 1891 1891-10-01 18:00:00  blood 1714 #> 3428  51.5     10 1891 1891-10-01 18:00:00   bone 1714 #> 3429  41.9     11 1891 1891-11-01 04:00:00  blood 1715 #> 3430  41.9     11 1891 1891-11-01 04:00:00   bone 1715 #> 3431  32.3     12 1891 1891-12-01 14:00:00  blood 1716 #> 3432  32.3     12 1891 1891-12-01 14:00:00   bone 1716 #> 3433  69.1      1 1892 1892-01-01 00:00:00  blood 1717 #> 3434  69.1      1 1892 1892-01-01 00:00:00   bone 1717 #> 3435  75.6      2 1892 1892-01-31 12:00:00  blood 1718 #> 3436  75.6      2 1892 1892-01-31 12:00:00   bone 1718 #> 3437  49.9      3 1892 1892-03-02 00:00:00  blood 1719 #> 3438  49.9      3 1892 1892-03-02 00:00:00   bone 1719 #> 3439  69.6      4 1892 1892-04-01 12:00:00  blood 1720 #> 3440  69.6      4 1892 1892-04-01 12:00:00   bone 1720 #> 3441  79.6      5 1892 1892-05-02 00:00:00  blood 1721 #> 3442  79.6      5 1892 1892-05-02 00:00:00   bone 1721 #> 3443  76.3      6 1892 1892-06-01 12:00:00  blood 1722 #> 3444  76.3      6 1892 1892-06-01 12:00:00   bone 1722 #> 3445  76.8      7 1892 1892-07-02 00:00:00  blood 1723 #> 3446  76.8      7 1892 1892-07-02 00:00:00   bone 1723 #> 3447 101.4      8 1892 1892-08-01 12:00:00  blood 1724 #> 3448 101.4      8 1892 1892-08-01 12:00:00   bone 1724 #> 3449  62.8      9 1892 1892-09-01 00:00:00  blood 1725 #> 3450  62.8      9 1892 1892-09-01 00:00:00   bone 1725 #> 3451  70.5     10 1892 1892-10-01 12:00:00  blood 1726 #> 3452  70.5     10 1892 1892-10-01 12:00:00   bone 1726 #> 3453  65.4     11 1892 1892-11-01 00:00:00  blood 1727 #> 3454  65.4     11 1892 1892-11-01 00:00:00   bone 1727 #> 3455  78.6     12 1892 1892-12-01 12:00:00  blood 1728 #> 3456  78.6     12 1892 1892-12-01 12:00:00   bone 1728 #> 3457  75.0      1 1893 1893-01-01 00:00:00  blood 1729 #> 3458  75.0      1 1893 1893-01-01 00:00:00   bone 1729 #> 3459  73.0      2 1893 1893-01-31 10:00:00  blood 1730 #> 3460  73.0      2 1893 1893-01-31 10:00:00   bone 1730 #> 3461  65.7      3 1893 1893-03-02 20:00:00  blood 1731 #> 3462  65.7      3 1893 1893-03-02 20:00:00   bone 1731 #> 3463  88.1      4 1893 1893-04-02 06:00:00  blood 1732 #> 3464  88.1      4 1893 1893-04-02 06:00:00   bone 1732 #> 3465  84.7      5 1893 1893-05-02 16:00:00  blood 1733 #> 3466  84.7      5 1893 1893-05-02 16:00:00   bone 1733 #> 3467  88.2      6 1893 1893-06-02 02:00:00  blood 1734 #> 3468  88.2      6 1893 1893-06-02 02:00:00   bone 1734 #> 3469  88.8      7 1893 1893-07-02 12:00:00  blood 1735 #> 3470  88.8      7 1893 1893-07-02 12:00:00   bone 1735 #> 3471 129.2      8 1893 1893-08-01 22:00:00  blood 1736 #> 3472 129.2      8 1893 1893-08-01 22:00:00   bone 1736 #> 3473  77.9      9 1893 1893-09-01 08:00:00  blood 1737 #> 3474  77.9      9 1893 1893-09-01 08:00:00   bone 1737 #> 3475  79.7     10 1893 1893-10-01 18:00:00  blood 1738 #> 3476  79.7     10 1893 1893-10-01 18:00:00   bone 1738 #> 3477  75.1     11 1893 1893-11-01 04:00:00  blood 1739 #> 3478  75.1     11 1893 1893-11-01 04:00:00   bone 1739 #> 3479  93.8     12 1893 1893-12-01 14:00:00  blood 1740 #> 3480  93.8     12 1893 1893-12-01 14:00:00   bone 1740 #> 3481  83.2      1 1894 1894-01-01 00:00:00  blood 1741 #> 3482  83.2      1 1894 1894-01-01 00:00:00   bone 1741 #> 3483  84.6      2 1894 1894-01-31 10:00:00  blood 1742 #> 3484  84.6      2 1894 1894-01-31 10:00:00   bone 1742 #> 3485  52.3      3 1894 1894-03-02 20:00:00  blood 1743 #> 3486  52.3      3 1894 1894-03-02 20:00:00   bone 1743 #> 3487  81.6      4 1894 1894-04-02 06:00:00  blood 1744 #> 3488  81.6      4 1894 1894-04-02 06:00:00   bone 1744 #> 3489 101.2      5 1894 1894-05-02 16:00:00  blood 1745 #> 3490 101.2      5 1894 1894-05-02 16:00:00   bone 1745 #> 3491  98.9      6 1894 1894-06-02 02:00:00  blood 1746 #> 3492  98.9      6 1894 1894-06-02 02:00:00   bone 1746 #> 3493 106.0      7 1894 1894-07-02 12:00:00  blood 1747 #> 3494 106.0      7 1894 1894-07-02 12:00:00   bone 1747 #> 3495  70.3      8 1894 1894-08-01 22:00:00  blood 1748 #> 3496  70.3      8 1894 1894-08-01 22:00:00   bone 1748 #> 3497  65.9      9 1894 1894-09-01 08:00:00  blood 1749 #> 3498  65.9      9 1894 1894-09-01 08:00:00   bone 1749 #> 3499  75.5     10 1894 1894-10-01 18:00:00  blood 1750 #> 3500  75.5     10 1894 1894-10-01 18:00:00   bone 1750 #> 3501  56.6     11 1894 1894-11-01 04:00:00  blood 1751 #> 3502  56.6     11 1894 1894-11-01 04:00:00   bone 1751 #> 3503  60.0     12 1894 1894-12-01 14:00:00  blood 1752 #> 3504  60.0     12 1894 1894-12-01 14:00:00   bone 1752 #> 3505  63.3      1 1895 1895-01-01 00:00:00  blood 1753 #> 3506  63.3      1 1895 1895-01-01 00:00:00   bone 1753 #> 3507  67.2      2 1895 1895-01-31 10:00:00  blood 1754 #> 3508  67.2      2 1895 1895-01-31 10:00:00   bone 1754 #> 3509  61.0      3 1895 1895-03-02 20:00:00  blood 1755 #> 3510  61.0      3 1895 1895-03-02 20:00:00   bone 1755 #> 3511  76.9      4 1895 1895-04-02 06:00:00  blood 1756 #> 3512  76.9      4 1895 1895-04-02 06:00:00   bone 1756 #> 3513  67.5      5 1895 1895-05-02 16:00:00  blood 1757 #> 3514  67.5      5 1895 1895-05-02 16:00:00   bone 1757 #> 3515  71.5      6 1895 1895-06-02 02:00:00  blood 1758 #> 3516  71.5      6 1895 1895-06-02 02:00:00   bone 1758 #> 3517  47.8      7 1895 1895-07-02 12:00:00  blood 1759 #> 3518  47.8      7 1895 1895-07-02 12:00:00   bone 1759 #> 3519  68.9      8 1895 1895-08-01 22:00:00  blood 1760 #> 3520  68.9      8 1895 1895-08-01 22:00:00   bone 1760 #> 3521  57.7      9 1895 1895-09-01 08:00:00  blood 1761 #> 3522  57.7      9 1895 1895-09-01 08:00:00   bone 1761 #> 3523  67.9     10 1895 1895-10-01 18:00:00  blood 1762 #> 3524  67.9     10 1895 1895-10-01 18:00:00   bone 1762 #> 3525  47.2     11 1895 1895-11-01 04:00:00  blood 1763 #> 3526  47.2     11 1895 1895-11-01 04:00:00   bone 1763 #> 3527  70.7     12 1895 1895-12-01 14:00:00  blood 1764 #> 3528  70.7     12 1895 1895-12-01 14:00:00   bone 1764 #> 3529  29.0      1 1896 1896-01-01 00:00:00  blood 1765 #> 3530  29.0      1 1896 1896-01-01 00:00:00   bone 1765 #> 3531  57.4      2 1896 1896-01-31 12:00:00  blood 1766 #> 3532  57.4      2 1896 1896-01-31 12:00:00   bone 1766 #> 3533  52.0      3 1896 1896-03-02 00:00:00  blood 1767 #> 3534  52.0      3 1896 1896-03-02 00:00:00   bone 1767 #> 3535  43.8      4 1896 1896-04-01 12:00:00  blood 1768 #> 3536  43.8      4 1896 1896-04-01 12:00:00   bone 1768 #> 3537  27.7      5 1896 1896-05-02 00:00:00  blood 1769 #> 3538  27.7      5 1896 1896-05-02 00:00:00   bone 1769 #> 3539  49.0      6 1896 1896-06-01 12:00:00  blood 1770 #> 3540  49.0      6 1896 1896-06-01 12:00:00   bone 1770 #> 3541  45.0      7 1896 1896-07-02 00:00:00  blood 1771 #> 3542  45.0      7 1896 1896-07-02 00:00:00   bone 1771 #> 3543  27.2      8 1896 1896-08-01 12:00:00  blood 1772 #> 3544  27.2      8 1896 1896-08-01 12:00:00   bone 1772 #> 3545  61.3      9 1896 1896-09-01 00:00:00  blood 1773 #> 3546  61.3      9 1896 1896-09-01 00:00:00   bone 1773 #> 3547  28.4     10 1896 1896-10-01 12:00:00  blood 1774 #> 3548  28.4     10 1896 1896-10-01 12:00:00   bone 1774 #> 3549  38.0     11 1896 1896-11-01 00:00:00  blood 1775 #> 3550  38.0     11 1896 1896-11-01 00:00:00   bone 1775 #> 3551  42.6     12 1896 1896-12-01 12:00:00  blood 1776 #> 3552  42.6     12 1896 1896-12-01 12:00:00   bone 1776 #> 3553  40.6      1 1897 1897-01-01 00:00:00  blood 1777 #> 3554  40.6      1 1897 1897-01-01 00:00:00   bone 1777 #> 3555  29.4      2 1897 1897-01-31 10:00:00  blood 1778 #> 3556  29.4      2 1897 1897-01-31 10:00:00   bone 1778 #> 3557  29.1      3 1897 1897-03-02 20:00:00  blood 1779 #> 3558  29.1      3 1897 1897-03-02 20:00:00   bone 1779 #> 3559  31.0      4 1897 1897-04-02 06:00:00  blood 1780 #> 3560  31.0      4 1897 1897-04-02 06:00:00   bone 1780 #> 3561  20.0      5 1897 1897-05-02 16:00:00  blood 1781 #> 3562  20.0      5 1897 1897-05-02 16:00:00   bone 1781 #> 3563  11.3      6 1897 1897-06-02 02:00:00  blood 1782 #> 3564  11.3      6 1897 1897-06-02 02:00:00   bone 1782 #> 3565  27.6      7 1897 1897-07-02 12:00:00  blood 1783 #> 3566  27.6      7 1897 1897-07-02 12:00:00   bone 1783 #> 3567  21.8      8 1897 1897-08-01 22:00:00  blood 1784 #> 3568  21.8      8 1897 1897-08-01 22:00:00   bone 1784 #> 3569  48.1      9 1897 1897-09-01 08:00:00  blood 1785 #> 3570  48.1      9 1897 1897-09-01 08:00:00   bone 1785 #> 3571  14.3     10 1897 1897-10-01 18:00:00  blood 1786 #> 3572  14.3     10 1897 1897-10-01 18:00:00   bone 1786 #> 3573   8.4     11 1897 1897-11-01 04:00:00  blood 1787 #> 3574   8.4     11 1897 1897-11-01 04:00:00   bone 1787 #> 3575  33.3     12 1897 1897-12-01 14:00:00  blood 1788 #> 3576  33.3     12 1897 1897-12-01 14:00:00   bone 1788 #> 3577  30.2      1 1898 1898-01-01 00:00:00  blood 1789 #> 3578  30.2      1 1898 1898-01-01 00:00:00   bone 1789 #> 3579  36.4      2 1898 1898-01-31 10:00:00  blood 1790 #> 3580  36.4      2 1898 1898-01-31 10:00:00   bone 1790 #> 3581  38.3      3 1898 1898-03-02 20:00:00  blood 1791 #> 3582  38.3      3 1898 1898-03-02 20:00:00   bone 1791 #> 3583  14.5      4 1898 1898-04-02 06:00:00  blood 1792 #> 3584  14.5      4 1898 1898-04-02 06:00:00   bone 1792 #> 3585  25.8      5 1898 1898-05-02 16:00:00  blood 1793 #> 3586  25.8      5 1898 1898-05-02 16:00:00   bone 1793 #> 3587  22.3      6 1898 1898-06-02 02:00:00  blood 1794 #> 3588  22.3      6 1898 1898-06-02 02:00:00   bone 1794 #> 3589   9.0      7 1898 1898-07-02 12:00:00  blood 1795 #> 3590   9.0      7 1898 1898-07-02 12:00:00   bone 1795 #> 3591  31.4      8 1898 1898-08-01 22:00:00  blood 1796 #> 3592  31.4      8 1898 1898-08-01 22:00:00   bone 1796 #> 3593  34.8      9 1898 1898-09-01 08:00:00  blood 1797 #> 3594  34.8      9 1898 1898-09-01 08:00:00   bone 1797 #> 3595  34.4     10 1898 1898-10-01 18:00:00  blood 1798 #> 3596  34.4     10 1898 1898-10-01 18:00:00   bone 1798 #> 3597  30.9     11 1898 1898-11-01 04:00:00  blood 1799 #> 3598  30.9     11 1898 1898-11-01 04:00:00   bone 1799 #> 3599  12.6     12 1898 1898-12-01 14:00:00  blood 1800 #> 3600  12.6     12 1898 1898-12-01 14:00:00   bone 1800 #> 3601  19.5      1 1899 1899-01-01 00:00:00  blood 1801 #> 3602  19.5      1 1899 1899-01-01 00:00:00   bone 1801 #> 3603   9.2      2 1899 1899-01-31 10:00:00  blood 1802 #> 3604   9.2      2 1899 1899-01-31 10:00:00   bone 1802 #> 3605  18.1      3 1899 1899-03-02 20:00:00  blood 1803 #> 3606  18.1      3 1899 1899-03-02 20:00:00   bone 1803 #> 3607  14.2      4 1899 1899-04-02 06:00:00  blood 1804 #> 3608  14.2      4 1899 1899-04-02 06:00:00   bone 1804 #> 3609   7.7      5 1899 1899-05-02 16:00:00  blood 1805 #> 3610   7.7      5 1899 1899-05-02 16:00:00   bone 1805 #> 3611  20.5      6 1899 1899-06-02 02:00:00  blood 1806 #> 3612  20.5      6 1899 1899-06-02 02:00:00   bone 1806 #> 3613  13.5      7 1899 1899-07-02 12:00:00  blood 1807 #> 3614  13.5      7 1899 1899-07-02 12:00:00   bone 1807 #> 3615   2.9      8 1899 1899-08-01 22:00:00  blood 1808 #> 3616   2.9      8 1899 1899-08-01 22:00:00   bone 1808 #> 3617   8.4      9 1899 1899-09-01 08:00:00  blood 1809 #> 3618   8.4      9 1899 1899-09-01 08:00:00   bone 1809 #> 3619  13.0     10 1899 1899-10-01 18:00:00  blood 1810 #> 3620  13.0     10 1899 1899-10-01 18:00:00   bone 1810 #> 3621   7.8     11 1899 1899-11-01 04:00:00  blood 1811 #> 3622   7.8     11 1899 1899-11-01 04:00:00   bone 1811 #> 3623  10.5     12 1899 1899-12-01 14:00:00  blood 1812 #> 3624  10.5     12 1899 1899-12-01 14:00:00   bone 1812 #> 3625   9.4      1 1900 1900-01-01 00:00:00  blood 1813 #> 3626   9.4      1 1900 1900-01-01 00:00:00   bone 1813 #> 3627  13.6      2 1900 1900-01-31 10:00:00  blood 1814 #> 3628  13.6      2 1900 1900-01-31 10:00:00   bone 1814 #> 3629   8.6      3 1900 1900-03-02 20:00:00  blood 1815 #> 3630   8.6      3 1900 1900-03-02 20:00:00   bone 1815 #> 3631  16.0      4 1900 1900-04-02 06:00:00  blood 1816 #> 3632  16.0      4 1900 1900-04-02 06:00:00   bone 1816 #> 3633  15.2      5 1900 1900-05-02 16:00:00  blood 1817 #> 3634  15.2      5 1900 1900-05-02 16:00:00   bone 1817 #> 3635  12.1      6 1900 1900-06-02 02:00:00  blood 1818 #> 3636  12.1      6 1900 1900-06-02 02:00:00   bone 1818 #> 3637   8.3      7 1900 1900-07-02 12:00:00  blood 1819 #> 3638   8.3      7 1900 1900-07-02 12:00:00   bone 1819 #> 3639   4.3      8 1900 1900-08-01 22:00:00  blood 1820 #> 3640   4.3      8 1900 1900-08-01 22:00:00   bone 1820 #> 3641   8.3      9 1900 1900-09-01 08:00:00  blood 1821 #> 3642   8.3      9 1900 1900-09-01 08:00:00   bone 1821 #> 3643  12.9     10 1900 1900-10-01 18:00:00  blood 1822 #> 3644  12.9     10 1900 1900-10-01 18:00:00   bone 1822 #> 3645   4.5     11 1900 1900-11-01 04:00:00  blood 1823 #> 3646   4.5     11 1900 1900-11-01 04:00:00   bone 1823 #> 3647   0.3     12 1900 1900-12-01 14:00:00  blood 1824 #> 3648   0.3     12 1900 1900-12-01 14:00:00   bone 1824 #> 3649   0.2      1 1901 1901-01-01 00:00:00  blood 1825 #> 3650   0.2      1 1901 1901-01-01 00:00:00   bone 1825 #> 3651   2.4      2 1901 1901-01-31 10:00:00  blood 1826 #> 3652   2.4      2 1901 1901-01-31 10:00:00   bone 1826 #> 3653   4.5      3 1901 1901-03-02 20:00:00  blood 1827 #> 3654   4.5      3 1901 1901-03-02 20:00:00   bone 1827 #> 3655   0.0      4 1901 1901-04-02 06:00:00  blood 1828 #> 3656   0.0      4 1901 1901-04-02 06:00:00   bone 1828 #> 3657  10.2      5 1901 1901-05-02 16:00:00  blood 1829 #> 3658  10.2      5 1901 1901-05-02 16:00:00   bone 1829 #> 3659   5.8      6 1901 1901-06-02 02:00:00  blood 1830 #> 3660   5.8      6 1901 1901-06-02 02:00:00   bone 1830 #> 3661   0.7      7 1901 1901-07-02 12:00:00  blood 1831 #> 3662   0.7      7 1901 1901-07-02 12:00:00   bone 1831 #> 3663   1.0      8 1901 1901-08-01 22:00:00  blood 1832 #> 3664   1.0      8 1901 1901-08-01 22:00:00   bone 1832 #> 3665   0.6      9 1901 1901-09-01 08:00:00  blood 1833 #> 3666   0.6      9 1901 1901-09-01 08:00:00   bone 1833 #> 3667   3.7     10 1901 1901-10-01 18:00:00  blood 1834 #> 3668   3.7     10 1901 1901-10-01 18:00:00   bone 1834 #> 3669   3.8     11 1901 1901-11-01 04:00:00  blood 1835 #> 3670   3.8     11 1901 1901-11-01 04:00:00   bone 1835 #> 3671   0.0     12 1901 1901-12-01 14:00:00  blood 1836 #> 3672   0.0     12 1901 1901-12-01 14:00:00   bone 1836 #> 3673   5.2      1 1902 1902-01-01 00:00:00  blood 1837 #> 3674   5.2      1 1902 1902-01-01 00:00:00   bone 1837 #> 3675   0.0      2 1902 1902-01-31 10:00:00  blood 1838 #> 3676   0.0      2 1902 1902-01-31 10:00:00   bone 1838 #> 3677  12.4      3 1902 1902-03-02 20:00:00  blood 1839 #> 3678  12.4      3 1902 1902-03-02 20:00:00   bone 1839 #> 3679   0.0      4 1902 1902-04-02 06:00:00  blood 1840 #> 3680   0.0      4 1902 1902-04-02 06:00:00   bone 1840 #> 3681   2.8      5 1902 1902-05-02 16:00:00  blood 1841 #> 3682   2.8      5 1902 1902-05-02 16:00:00   bone 1841 #> 3683   1.4      6 1902 1902-06-02 02:00:00  blood 1842 #> 3684   1.4      6 1902 1902-06-02 02:00:00   bone 1842 #> 3685   0.9      7 1902 1902-07-02 12:00:00  blood 1843 #> 3686   0.9      7 1902 1902-07-02 12:00:00   bone 1843 #> 3687   2.3      8 1902 1902-08-01 22:00:00  blood 1844 #> 3688   2.3      8 1902 1902-08-01 22:00:00   bone 1844 #> 3689   7.6      9 1902 1902-09-01 08:00:00  blood 1845 #> 3690   7.6      9 1902 1902-09-01 08:00:00   bone 1845 #> 3691  16.3     10 1902 1902-10-01 18:00:00  blood 1846 #> 3692  16.3     10 1902 1902-10-01 18:00:00   bone 1846 #> 3693  10.3     11 1902 1902-11-01 04:00:00  blood 1847 #> 3694  10.3     11 1902 1902-11-01 04:00:00   bone 1847 #> 3695   1.1     12 1902 1902-12-01 14:00:00  blood 1848 #> 3696   1.1     12 1902 1902-12-01 14:00:00   bone 1848 #> 3697   8.3      1 1903 1903-01-01 00:00:00  blood 1849 #> 3698   8.3      1 1903 1903-01-01 00:00:00   bone 1849 #> 3699  17.0      2 1903 1903-01-31 10:00:00  blood 1850 #> 3700  17.0      2 1903 1903-01-31 10:00:00   bone 1850 #> 3701  13.5      3 1903 1903-03-02 20:00:00  blood 1851 #> 3702  13.5      3 1903 1903-03-02 20:00:00   bone 1851 #> 3703  26.1      4 1903 1903-04-02 06:00:00  blood 1852 #> 3704  26.1      4 1903 1903-04-02 06:00:00   bone 1852 #> 3705  14.6      5 1903 1903-05-02 16:00:00  blood 1853 #> 3706  14.6      5 1903 1903-05-02 16:00:00   bone 1853 #> 3707  16.3      6 1903 1903-06-02 02:00:00  blood 1854 #> 3708  16.3      6 1903 1903-06-02 02:00:00   bone 1854 #> 3709  27.9      7 1903 1903-07-02 12:00:00  blood 1855 #> 3710  27.9      7 1903 1903-07-02 12:00:00   bone 1855 #> 3711  28.8      8 1903 1903-08-01 22:00:00  blood 1856 #> 3712  28.8      8 1903 1903-08-01 22:00:00   bone 1856 #> 3713  11.1      9 1903 1903-09-01 08:00:00  blood 1857 #> 3714  11.1      9 1903 1903-09-01 08:00:00   bone 1857 #> 3715  38.9     10 1903 1903-10-01 18:00:00  blood 1858 #> 3716  38.9     10 1903 1903-10-01 18:00:00   bone 1858 #> 3717  44.5     11 1903 1903-11-01 04:00:00  blood 1859 #> 3718  44.5     11 1903 1903-11-01 04:00:00   bone 1859 #> 3719  45.6     12 1903 1903-12-01 14:00:00  blood 1860 #> 3720  45.6     12 1903 1903-12-01 14:00:00   bone 1860 #> 3721  31.6      1 1904 1904-01-01 00:00:00  blood 1861 #> 3722  31.6      1 1904 1904-01-01 00:00:00   bone 1861 #> 3723  24.5      2 1904 1904-01-31 12:00:00  blood 1862 #> 3724  24.5      2 1904 1904-01-31 12:00:00   bone 1862 #> 3725  37.2      3 1904 1904-03-02 00:00:00  blood 1863 #> 3726  37.2      3 1904 1904-03-02 00:00:00   bone 1863 #> 3727  43.0      4 1904 1904-04-01 12:00:00  blood 1864 #> 3728  43.0      4 1904 1904-04-01 12:00:00   bone 1864 #> 3729  39.5      5 1904 1904-05-02 00:00:00  blood 1865 #> 3730  39.5      5 1904 1904-05-02 00:00:00   bone 1865 #> 3731  41.9      6 1904 1904-06-01 12:00:00  blood 1866 #> 3732  41.9      6 1904 1904-06-01 12:00:00   bone 1866 #> 3733  50.6      7 1904 1904-07-02 00:00:00  blood 1867 #> 3734  50.6      7 1904 1904-07-02 00:00:00   bone 1867 #> 3735  58.2      8 1904 1904-08-01 12:00:00  blood 1868 #> 3736  58.2      8 1904 1904-08-01 12:00:00   bone 1868 #> 3737  30.1      9 1904 1904-09-01 00:00:00  blood 1869 #> 3738  30.1      9 1904 1904-09-01 00:00:00   bone 1869 #> 3739  54.2     10 1904 1904-10-01 12:00:00  blood 1870 #> 3740  54.2     10 1904 1904-10-01 12:00:00   bone 1870 #> 3741  38.0     11 1904 1904-11-01 00:00:00  blood 1871 #> 3742  38.0     11 1904 1904-11-01 00:00:00   bone 1871 #> 3743  54.6     12 1904 1904-12-01 12:00:00  blood 1872 #> 3744  54.6     12 1904 1904-12-01 12:00:00   bone 1872 #> 3745  54.8      1 1905 1905-01-01 00:00:00  blood 1873 #> 3746  54.8      1 1905 1905-01-01 00:00:00   bone 1873 #> 3747  85.8      2 1905 1905-01-31 10:00:00  blood 1874 #> 3748  85.8      2 1905 1905-01-31 10:00:00   bone 1874 #> 3749  56.5      3 1905 1905-03-02 20:00:00  blood 1875 #> 3750  56.5      3 1905 1905-03-02 20:00:00   bone 1875 #> 3751  39.3      4 1905 1905-04-02 06:00:00  blood 1876 #> 3752  39.3      4 1905 1905-04-02 06:00:00   bone 1876 #> 3753  48.0      5 1905 1905-05-02 16:00:00  blood 1877 #> 3754  48.0      5 1905 1905-05-02 16:00:00   bone 1877 #> 3755  49.0      6 1905 1905-06-02 02:00:00  blood 1878 #> 3756  49.0      6 1905 1905-06-02 02:00:00   bone 1878 #> 3757  73.0      7 1905 1905-07-02 12:00:00  blood 1879 #> 3758  73.0      7 1905 1905-07-02 12:00:00   bone 1879 #> 3759  58.8      8 1905 1905-08-01 22:00:00  blood 1880 #> 3760  58.8      8 1905 1905-08-01 22:00:00   bone 1880 #> 3761  55.0      9 1905 1905-09-01 08:00:00  blood 1881 #> 3762  55.0      9 1905 1905-09-01 08:00:00   bone 1881 #> 3763  78.7     10 1905 1905-10-01 18:00:00  blood 1882 #> 3764  78.7     10 1905 1905-10-01 18:00:00   bone 1882 #> 3765 107.2     11 1905 1905-11-01 04:00:00  blood 1883 #> 3766 107.2     11 1905 1905-11-01 04:00:00   bone 1883 #> 3767  55.5     12 1905 1905-12-01 14:00:00  blood 1884 #> 3768  55.5     12 1905 1905-12-01 14:00:00   bone 1884 #> 3769  45.5      1 1906 1906-01-01 00:00:00  blood 1885 #> 3770  45.5      1 1906 1906-01-01 00:00:00   bone 1885 #> 3771  31.3      2 1906 1906-01-31 10:00:00  blood 1886 #> 3772  31.3      2 1906 1906-01-31 10:00:00   bone 1886 #> 3773  64.5      3 1906 1906-03-02 20:00:00  blood 1887 #> 3774  64.5      3 1906 1906-03-02 20:00:00   bone 1887 #> 3775  55.3      4 1906 1906-04-02 06:00:00  blood 1888 #> 3776  55.3      4 1906 1906-04-02 06:00:00   bone 1888 #> 3777  57.7      5 1906 1906-05-02 16:00:00  blood 1889 #> 3778  57.7      5 1906 1906-05-02 16:00:00   bone 1889 #> 3779  63.2      6 1906 1906-06-02 02:00:00  blood 1890 #> 3780  63.2      6 1906 1906-06-02 02:00:00   bone 1890 #> 3781 103.6      7 1906 1906-07-02 12:00:00  blood 1891 #> 3782 103.6      7 1906 1906-07-02 12:00:00   bone 1891 #> 3783  47.7      8 1906 1906-08-01 22:00:00  blood 1892 #> 3784  47.7      8 1906 1906-08-01 22:00:00   bone 1892 #> 3785  56.1      9 1906 1906-09-01 08:00:00  blood 1893 #> 3786  56.1      9 1906 1906-09-01 08:00:00   bone 1893 #> 3787  17.8     10 1906 1906-10-01 18:00:00  blood 1894 #> 3788  17.8     10 1906 1906-10-01 18:00:00   bone 1894 #> 3789  38.9     11 1906 1906-11-01 04:00:00  blood 1895 #> 3790  38.9     11 1906 1906-11-01 04:00:00   bone 1895 #> 3791  64.7     12 1906 1906-12-01 14:00:00  blood 1896 #> 3792  64.7     12 1906 1906-12-01 14:00:00   bone 1896 #> 3793  76.4      1 1907 1907-01-01 00:00:00  blood 1897 #> 3794  76.4      1 1907 1907-01-01 00:00:00   bone 1897 #> 3795 108.2      2 1907 1907-01-31 10:00:00  blood 1898 #> 3796 108.2      2 1907 1907-01-31 10:00:00   bone 1898 #> 3797  60.7      3 1907 1907-03-02 20:00:00  blood 1899 #> 3798  60.7      3 1907 1907-03-02 20:00:00   bone 1899 #> 3799  52.6      4 1907 1907-04-02 06:00:00  blood 1900 #> 3800  52.6      4 1907 1907-04-02 06:00:00   bone 1900 #> 3801  42.9      5 1907 1907-05-02 16:00:00  blood 1901 #> 3802  42.9      5 1907 1907-05-02 16:00:00   bone 1901 #> 3803  40.4      6 1907 1907-06-02 02:00:00  blood 1902 #> 3804  40.4      6 1907 1907-06-02 02:00:00   bone 1902 #> 3805  49.7      7 1907 1907-07-02 12:00:00  blood 1903 #> 3806  49.7      7 1907 1907-07-02 12:00:00   bone 1903 #> 3807  54.3      8 1907 1907-08-01 22:00:00  blood 1904 #> 3808  54.3      8 1907 1907-08-01 22:00:00   bone 1904 #> 3809  85.0      9 1907 1907-09-01 08:00:00  blood 1905 #> 3810  85.0      9 1907 1907-09-01 08:00:00   bone 1905 #> 3811  65.4     10 1907 1907-10-01 18:00:00  blood 1906 #> 3812  65.4     10 1907 1907-10-01 18:00:00   bone 1906 #> 3813  61.5     11 1907 1907-11-01 04:00:00  blood 1907 #> 3814  61.5     11 1907 1907-11-01 04:00:00   bone 1907 #> 3815  47.3     12 1907 1907-12-01 14:00:00  blood 1908 #> 3816  47.3     12 1907 1907-12-01 14:00:00   bone 1908 #> 3817  39.2      1 1908 1908-01-01 00:00:00  blood 1909 #> 3818  39.2      1 1908 1908-01-01 00:00:00   bone 1909 #> 3819  33.9      2 1908 1908-01-31 12:00:00  blood 1910 #> 3820  33.9      2 1908 1908-01-31 12:00:00   bone 1910 #> 3821  28.7      3 1908 1908-03-02 00:00:00  blood 1911 #> 3822  28.7      3 1908 1908-03-02 00:00:00   bone 1911 #> 3823  57.6      4 1908 1908-04-01 12:00:00  blood 1912 #> 3824  57.6      4 1908 1908-04-01 12:00:00   bone 1912 #> 3825  40.8      5 1908 1908-05-02 00:00:00  blood 1913 #> 3826  40.8      5 1908 1908-05-02 00:00:00   bone 1913 #> 3827  48.1      6 1908 1908-06-01 12:00:00  blood 1914 #> 3828  48.1      6 1908 1908-06-01 12:00:00   bone 1914 #> 3829  39.5      7 1908 1908-07-02 00:00:00  blood 1915 #> 3830  39.5      7 1908 1908-07-02 00:00:00   bone 1915 #> 3831  90.5      8 1908 1908-08-01 12:00:00  blood 1916 #> 3832  90.5      8 1908 1908-08-01 12:00:00   bone 1916 #> 3833  86.9      9 1908 1908-09-01 00:00:00  blood 1917 #> 3834  86.9      9 1908 1908-09-01 00:00:00   bone 1917 #> 3835  32.3     10 1908 1908-10-01 12:00:00  blood 1918 #> 3836  32.3     10 1908 1908-10-01 12:00:00   bone 1918 #> 3837  45.5     11 1908 1908-11-01 00:00:00  blood 1919 #> 3838  45.5     11 1908 1908-11-01 00:00:00   bone 1919 #> 3839  39.5     12 1908 1908-12-01 12:00:00  blood 1920 #> 3840  39.5     12 1908 1908-12-01 12:00:00   bone 1920 #> 3841  56.7      1 1909 1909-01-01 00:00:00  blood 1921 #> 3842  56.7      1 1909 1909-01-01 00:00:00   bone 1921 #> 3843  46.6      2 1909 1909-01-31 10:00:00  blood 1922 #> 3844  46.6      2 1909 1909-01-31 10:00:00   bone 1922 #> 3845  66.3      3 1909 1909-03-02 20:00:00  blood 1923 #> 3846  66.3      3 1909 1909-03-02 20:00:00   bone 1923 #> 3847  32.3      4 1909 1909-04-02 06:00:00  blood 1924 #> 3848  32.3      4 1909 1909-04-02 06:00:00   bone 1924 #> 3849  36.0      5 1909 1909-05-02 16:00:00  blood 1925 #> 3850  36.0      5 1909 1909-05-02 16:00:00   bone 1925 #> 3851  22.6      6 1909 1909-06-02 02:00:00  blood 1926 #> 3852  22.6      6 1909 1909-06-02 02:00:00   bone 1926 #> 3853  35.8      7 1909 1909-07-02 12:00:00  blood 1927 #> 3854  35.8      7 1909 1909-07-02 12:00:00   bone 1927 #> 3855  23.1      8 1909 1909-08-01 22:00:00  blood 1928 #> 3856  23.1      8 1909 1909-08-01 22:00:00   bone 1928 #> 3857  38.8      9 1909 1909-09-01 08:00:00  blood 1929 #> 3858  38.8      9 1909 1909-09-01 08:00:00   bone 1929 #> 3859  58.4     10 1909 1909-10-01 18:00:00  blood 1930 #> 3860  58.4     10 1909 1909-10-01 18:00:00   bone 1930 #> 3861  55.8     11 1909 1909-11-01 04:00:00  blood 1931 #> 3862  55.8     11 1909 1909-11-01 04:00:00   bone 1931 #> 3863  54.2     12 1909 1909-12-01 14:00:00  blood 1932 #> 3864  54.2     12 1909 1909-12-01 14:00:00   bone 1932 #> 3865  26.4      1 1910 1910-01-01 00:00:00  blood 1933 #> 3866  26.4      1 1910 1910-01-01 00:00:00   bone 1933 #> 3867  31.5      2 1910 1910-01-31 10:00:00  blood 1934 #> 3868  31.5      2 1910 1910-01-31 10:00:00   bone 1934 #> 3869  21.4      3 1910 1910-03-02 20:00:00  blood 1935 #> 3870  21.4      3 1910 1910-03-02 20:00:00   bone 1935 #> 3871   8.4      4 1910 1910-04-02 06:00:00  blood 1936 #> 3872   8.4      4 1910 1910-04-02 06:00:00   bone 1936 #> 3873  22.2      5 1910 1910-05-02 16:00:00  blood 1937 #> 3874  22.2      5 1910 1910-05-02 16:00:00   bone 1937 #> 3875  12.3      6 1910 1910-06-02 02:00:00  blood 1938 #> 3876  12.3      6 1910 1910-06-02 02:00:00   bone 1938 #> 3877  14.1      7 1910 1910-07-02 12:00:00  blood 1939 #> 3878  14.1      7 1910 1910-07-02 12:00:00   bone 1939 #> 3879  11.5      8 1910 1910-08-01 22:00:00  blood 1940 #> 3880  11.5      8 1910 1910-08-01 22:00:00   bone 1940 #> 3881  26.2      9 1910 1910-09-01 08:00:00  blood 1941 #> 3882  26.2      9 1910 1910-09-01 08:00:00   bone 1941 #> 3883  38.3     10 1910 1910-10-01 18:00:00  blood 1942 #> 3884  38.3     10 1910 1910-10-01 18:00:00   bone 1942 #> 3885   4.9     11 1910 1910-11-01 04:00:00  blood 1943 #> 3886   4.9     11 1910 1910-11-01 04:00:00   bone 1943 #> 3887   5.8     12 1910 1910-12-01 14:00:00  blood 1944 #> 3888   5.8     12 1910 1910-12-01 14:00:00   bone 1944 #> 3889   3.4      1 1911 1911-01-01 00:00:00  blood 1945 #> 3890   3.4      1 1911 1911-01-01 00:00:00   bone 1945 #> 3891   9.0      2 1911 1911-01-31 10:00:00  blood 1946 #> 3892   9.0      2 1911 1911-01-31 10:00:00   bone 1946 #> 3893   7.8      3 1911 1911-03-02 20:00:00  blood 1947 #> 3894   7.8      3 1911 1911-03-02 20:00:00   bone 1947 #> 3895  16.5      4 1911 1911-04-02 06:00:00  blood 1948 #> 3896  16.5      4 1911 1911-04-02 06:00:00   bone 1948 #> 3897   9.0      5 1911 1911-05-02 16:00:00  blood 1949 #> 3898   9.0      5 1911 1911-05-02 16:00:00   bone 1949 #> 3899   2.2      6 1911 1911-06-02 02:00:00  blood 1950 #> 3900   2.2      6 1911 1911-06-02 02:00:00   bone 1950 #> 3901   3.5      7 1911 1911-07-02 12:00:00  blood 1951 #> 3902   3.5      7 1911 1911-07-02 12:00:00   bone 1951 #> 3903   4.0      8 1911 1911-08-01 22:00:00  blood 1952 #> 3904   4.0      8 1911 1911-08-01 22:00:00   bone 1952 #> 3905   4.0      9 1911 1911-09-01 08:00:00  blood 1953 #> 3906   4.0      9 1911 1911-09-01 08:00:00   bone 1953 #> 3907   2.6     10 1911 1911-10-01 18:00:00  blood 1954 #> 3908   2.6     10 1911 1911-10-01 18:00:00   bone 1954 #> 3909   4.2     11 1911 1911-11-01 04:00:00  blood 1955 #> 3910   4.2     11 1911 1911-11-01 04:00:00   bone 1955 #> 3911   2.2     12 1911 1911-12-01 14:00:00  blood 1956 #> 3912   2.2     12 1911 1911-12-01 14:00:00   bone 1956 #> 3913   0.3      1 1912 1912-01-01 00:00:00  blood 1957 #> 3914   0.3      1 1912 1912-01-01 00:00:00   bone 1957 #> 3915   0.0      2 1912 1912-01-31 12:00:00  blood 1958 #> 3916   0.0      2 1912 1912-01-31 12:00:00   bone 1958 #> 3917   4.9      3 1912 1912-03-02 00:00:00  blood 1959 #> 3918   4.9      3 1912 1912-03-02 00:00:00   bone 1959 #> 3919   4.5      4 1912 1912-04-01 12:00:00  blood 1960 #> 3920   4.5      4 1912 1912-04-01 12:00:00   bone 1960 #> 3921   4.4      5 1912 1912-05-02 00:00:00  blood 1961 #> 3922   4.4      5 1912 1912-05-02 00:00:00   bone 1961 #> 3923   4.1      6 1912 1912-06-01 12:00:00  blood 1962 #> 3924   4.1      6 1912 1912-06-01 12:00:00   bone 1962 #> 3925   3.0      7 1912 1912-07-02 00:00:00  blood 1963 #> 3926   3.0      7 1912 1912-07-02 00:00:00   bone 1963 #> 3927   0.3      8 1912 1912-08-01 12:00:00  blood 1964 #> 3928   0.3      8 1912 1912-08-01 12:00:00   bone 1964 #> 3929   9.5      9 1912 1912-09-01 00:00:00  blood 1965 #> 3930   9.5      9 1912 1912-09-01 00:00:00   bone 1965 #> 3931   4.6     10 1912 1912-10-01 12:00:00  blood 1966 #> 3932   4.6     10 1912 1912-10-01 12:00:00   bone 1966 #> 3933   1.1     11 1912 1912-11-01 00:00:00  blood 1967 #> 3934   1.1     11 1912 1912-11-01 00:00:00   bone 1967 #> 3935   6.4     12 1912 1912-12-01 12:00:00  blood 1968 #> 3936   6.4     12 1912 1912-12-01 12:00:00   bone 1968 #> 3937   2.3      1 1913 1913-01-01 00:00:00  blood 1969 #> 3938   2.3      1 1913 1913-01-01 00:00:00   bone 1969 #> 3939   2.9      2 1913 1913-01-31 10:00:00  blood 1970 #> 3940   2.9      2 1913 1913-01-31 10:00:00   bone 1970 #> 3941   0.5      3 1913 1913-03-02 20:00:00  blood 1971 #> 3942   0.5      3 1913 1913-03-02 20:00:00   bone 1971 #> 3943   0.9      4 1913 1913-04-02 06:00:00  blood 1972 #> 3944   0.9      4 1913 1913-04-02 06:00:00   bone 1972 #> 3945   0.0      5 1913 1913-05-02 16:00:00  blood 1973 #> 3946   0.0      5 1913 1913-05-02 16:00:00   bone 1973 #> 3947   0.0      6 1913 1913-06-02 02:00:00  blood 1974 #> 3948   0.0      6 1913 1913-06-02 02:00:00   bone 1974 #> 3949   1.7      7 1913 1913-07-02 12:00:00  blood 1975 #> 3950   1.7      7 1913 1913-07-02 12:00:00   bone 1975 #> 3951   0.2      8 1913 1913-08-01 22:00:00  blood 1976 #> 3952   0.2      8 1913 1913-08-01 22:00:00   bone 1976 #> 3953   1.2      9 1913 1913-09-01 08:00:00  blood 1977 #> 3954   1.2      9 1913 1913-09-01 08:00:00   bone 1977 #> 3955   3.1     10 1913 1913-10-01 18:00:00  blood 1978 #> 3956   3.1     10 1913 1913-10-01 18:00:00   bone 1978 #> 3957   0.7     11 1913 1913-11-01 04:00:00  blood 1979 #> 3958   0.7     11 1913 1913-11-01 04:00:00   bone 1979 #> 3959   3.8     12 1913 1913-12-01 14:00:00  blood 1980 #> 3960   3.8     12 1913 1913-12-01 14:00:00   bone 1980 #> 3961   2.8      1 1914 1914-01-01 00:00:00  blood 1981 #> 3962   2.8      1 1914 1914-01-01 00:00:00   bone 1981 #> 3963   2.6      2 1914 1914-01-31 10:00:00  blood 1982 #> 3964   2.6      2 1914 1914-01-31 10:00:00   bone 1982 #> 3965   3.1      3 1914 1914-03-02 20:00:00  blood 1983 #> 3966   3.1      3 1914 1914-03-02 20:00:00   bone 1983 #> 3967  17.3      4 1914 1914-04-02 06:00:00  blood 1984 #> 3968  17.3      4 1914 1914-04-02 06:00:00   bone 1984 #> 3969   5.2      5 1914 1914-05-02 16:00:00  blood 1985 #> 3970   5.2      5 1914 1914-05-02 16:00:00   bone 1985 #> 3971  11.4      6 1914 1914-06-02 02:00:00  blood 1986 #> 3972  11.4      6 1914 1914-06-02 02:00:00   bone 1986 #> 3973   5.4      7 1914 1914-07-02 12:00:00  blood 1987 #> 3974   5.4      7 1914 1914-07-02 12:00:00   bone 1987 #> 3975   7.7      8 1914 1914-08-01 22:00:00  blood 1988 #> 3976   7.7      8 1914 1914-08-01 22:00:00   bone 1988 #> 3977  12.7      9 1914 1914-09-01 08:00:00  blood 1989 #> 3978  12.7      9 1914 1914-09-01 08:00:00   bone 1989 #> 3979   8.2     10 1914 1914-10-01 18:00:00  blood 1990 #> 3980   8.2     10 1914 1914-10-01 18:00:00   bone 1990 #> 3981  16.4     11 1914 1914-11-01 04:00:00  blood 1991 #> 3982  16.4     11 1914 1914-11-01 04:00:00   bone 1991 #> 3983  22.3     12 1914 1914-12-01 14:00:00  blood 1992 #> 3984  22.3     12 1914 1914-12-01 14:00:00   bone 1992 #> 3985  23.0      1 1915 1915-01-01 00:00:00  blood 1993 #> 3986  23.0      1 1915 1915-01-01 00:00:00   bone 1993 #> 3987  42.3      2 1915 1915-01-31 10:00:00  blood 1994 #> 3988  42.3      2 1915 1915-01-31 10:00:00   bone 1994 #> 3989  38.8      3 1915 1915-03-02 20:00:00  blood 1995 #> 3990  38.8      3 1915 1915-03-02 20:00:00   bone 1995 #> 3991  41.3      4 1915 1915-04-02 06:00:00  blood 1996 #> 3992  41.3      4 1915 1915-04-02 06:00:00   bone 1996 #> 3993  33.0      5 1915 1915-05-02 16:00:00  blood 1997 #> 3994  33.0      5 1915 1915-05-02 16:00:00   bone 1997 #> 3995  68.8      6 1915 1915-06-02 02:00:00  blood 1998 #> 3996  68.8      6 1915 1915-06-02 02:00:00   bone 1998 #> 3997  71.6      7 1915 1915-07-02 12:00:00  blood 1999 #> 3998  71.6      7 1915 1915-07-02 12:00:00   bone 1999 #> 3999  69.6      8 1915 1915-08-01 22:00:00  blood 2000 #> 4000  69.6      8 1915 1915-08-01 22:00:00   bone 2000 #> 4001  49.5      9 1915 1915-09-01 08:00:00  blood 2001 #> 4002  49.5      9 1915 1915-09-01 08:00:00   bone 2001 #> 4003  53.5     10 1915 1915-10-01 18:00:00  blood 2002 #> 4004  53.5     10 1915 1915-10-01 18:00:00   bone 2002 #> 4005  42.5     11 1915 1915-11-01 04:00:00  blood 2003 #> 4006  42.5     11 1915 1915-11-01 04:00:00   bone 2003 #> 4007  34.5     12 1915 1915-12-01 14:00:00  blood 2004 #> 4008  34.5     12 1915 1915-12-01 14:00:00   bone 2004 #> 4009  45.3      1 1916 1916-01-01 00:00:00  blood 2005 #> 4010  45.3      1 1916 1916-01-01 00:00:00   bone 2005 #> 4011  55.4      2 1916 1916-01-31 12:00:00  blood 2006 #> 4012  55.4      2 1916 1916-01-31 12:00:00   bone 2006 #> 4013  67.0      3 1916 1916-03-02 00:00:00  blood 2007 #> 4014  67.0      3 1916 1916-03-02 00:00:00   bone 2007 #> 4015  71.8      4 1916 1916-04-01 12:00:00  blood 2008 #> 4016  71.8      4 1916 1916-04-01 12:00:00   bone 2008 #> 4017  74.5      5 1916 1916-05-02 00:00:00  blood 2009 #> 4018  74.5      5 1916 1916-05-02 00:00:00   bone 2009 #> 4019  67.7      6 1916 1916-06-01 12:00:00  blood 2010 #> 4020  67.7      6 1916 1916-06-01 12:00:00   bone 2010 #> 4021  53.5      7 1916 1916-07-02 00:00:00  blood 2011 #> 4022  53.5      7 1916 1916-07-02 00:00:00   bone 2011 #> 4023  35.2      8 1916 1916-08-01 12:00:00  blood 2012 #> 4024  35.2      8 1916 1916-08-01 12:00:00   bone 2012 #> 4025  45.1      9 1916 1916-09-01 00:00:00  blood 2013 #> 4026  45.1      9 1916 1916-09-01 00:00:00   bone 2013 #> 4027  50.7     10 1916 1916-10-01 12:00:00  blood 2014 #> 4028  50.7     10 1916 1916-10-01 12:00:00   bone 2014 #> 4029  65.6     11 1916 1916-11-01 00:00:00  blood 2015 #> 4030  65.6     11 1916 1916-11-01 00:00:00   bone 2015 #> 4031  53.0     12 1916 1916-12-01 12:00:00  blood 2016 #> 4032  53.0     12 1916 1916-12-01 12:00:00   bone 2016 #> 4033  74.7      1 1917 1917-01-01 00:00:00  blood 2017 #> 4034  74.7      1 1917 1917-01-01 00:00:00   bone 2017 #> 4035  71.9      2 1917 1917-01-31 10:00:00  blood 2018 #> 4036  71.9      2 1917 1917-01-31 10:00:00   bone 2018 #> 4037  94.8      3 1917 1917-03-02 20:00:00  blood 2019 #> 4038  94.8      3 1917 1917-03-02 20:00:00   bone 2019 #> 4039  74.7      4 1917 1917-04-02 06:00:00  blood 2020 #> 4040  74.7      4 1917 1917-04-02 06:00:00   bone 2020 #> 4041 114.1      5 1917 1917-05-02 16:00:00  blood 2021 #> 4042 114.1      5 1917 1917-05-02 16:00:00   bone 2021 #> 4043 114.9      6 1917 1917-06-02 02:00:00  blood 2022 #> 4044 114.9      6 1917 1917-06-02 02:00:00   bone 2022 #> 4045 119.8      7 1917 1917-07-02 12:00:00  blood 2023 #> 4046 119.8      7 1917 1917-07-02 12:00:00   bone 2023 #> 4047 154.5      8 1917 1917-08-01 22:00:00  blood 2024 #> 4048 154.5      8 1917 1917-08-01 22:00:00   bone 2024 #> 4049 129.4      9 1917 1917-09-01 08:00:00  blood 2025 #> 4050 129.4      9 1917 1917-09-01 08:00:00   bone 2025 #> 4051  72.2     10 1917 1917-10-01 18:00:00  blood 2026 #> 4052  72.2     10 1917 1917-10-01 18:00:00   bone 2026 #> 4053  96.4     11 1917 1917-11-01 04:00:00  blood 2027 #> 4054  96.4     11 1917 1917-11-01 04:00:00   bone 2027 #> 4055 129.3     12 1917 1917-12-01 14:00:00  blood 2028 #> 4056 129.3     12 1917 1917-12-01 14:00:00   bone 2028 #> 4057  96.0      1 1918 1918-01-01 00:00:00  blood 2029 #> 4058  96.0      1 1918 1918-01-01 00:00:00   bone 2029 #> 4059  65.3      2 1918 1918-01-31 10:00:00  blood 2030 #> 4060  65.3      2 1918 1918-01-31 10:00:00   bone 2030 #> 4061  72.2      3 1918 1918-03-02 20:00:00  blood 2031 #> 4062  72.2      3 1918 1918-03-02 20:00:00   bone 2031 #> 4063  80.5      4 1918 1918-04-02 06:00:00  blood 2032 #> 4064  80.5      4 1918 1918-04-02 06:00:00   bone 2032 #> 4065  76.7      5 1918 1918-05-02 16:00:00  blood 2033 #> 4066  76.7      5 1918 1918-05-02 16:00:00   bone 2033 #> 4067  59.4      6 1918 1918-06-02 02:00:00  blood 2034 #> 4068  59.4      6 1918 1918-06-02 02:00:00   bone 2034 #> 4069 107.6      7 1918 1918-07-02 12:00:00  blood 2035 #> 4070 107.6      7 1918 1918-07-02 12:00:00   bone 2035 #> 4071 101.7      8 1918 1918-08-01 22:00:00  blood 2036 #> 4072 101.7      8 1918 1918-08-01 22:00:00   bone 2036 #> 4073  79.9      9 1918 1918-09-01 08:00:00  blood 2037 #> 4074  79.9      9 1918 1918-09-01 08:00:00   bone 2037 #> 4075  85.0     10 1918 1918-10-01 18:00:00  blood 2038 #> 4076  85.0     10 1918 1918-10-01 18:00:00   bone 2038 #> 4077  83.4     11 1918 1918-11-01 04:00:00  blood 2039 #> 4078  83.4     11 1918 1918-11-01 04:00:00   bone 2039 #> 4079  59.2     12 1918 1918-12-01 14:00:00  blood 2040 #> 4080  59.2     12 1918 1918-12-01 14:00:00   bone 2040 #> 4081  48.1      1 1919 1919-01-01 00:00:00  blood 2041 #> 4082  48.1      1 1919 1919-01-01 00:00:00   bone 2041 #> 4083  79.5      2 1919 1919-01-31 10:00:00  blood 2042 #> 4084  79.5      2 1919 1919-01-31 10:00:00   bone 2042 #> 4085  66.5      3 1919 1919-03-02 20:00:00  blood 2043 #> 4086  66.5      3 1919 1919-03-02 20:00:00   bone 2043 #> 4087  51.8      4 1919 1919-04-02 06:00:00  blood 2044 #> 4088  51.8      4 1919 1919-04-02 06:00:00   bone 2044 #> 4089  88.1      5 1919 1919-05-02 16:00:00  blood 2045 #> 4090  88.1      5 1919 1919-05-02 16:00:00   bone 2045 #> 4091 111.2      6 1919 1919-06-02 02:00:00  blood 2046 #> 4092 111.2      6 1919 1919-06-02 02:00:00   bone 2046 #> 4093  64.7      7 1919 1919-07-02 12:00:00  blood 2047 #> 4094  64.7      7 1919 1919-07-02 12:00:00   bone 2047 #> 4095  69.0      8 1919 1919-08-01 22:00:00  blood 2048 #> 4096  69.0      8 1919 1919-08-01 22:00:00   bone 2048 #> 4097  54.7      9 1919 1919-09-01 08:00:00  blood 2049 #> 4098  54.7      9 1919 1919-09-01 08:00:00   bone 2049 #> 4099  52.8     10 1919 1919-10-01 18:00:00  blood 2050 #> 4100  52.8     10 1919 1919-10-01 18:00:00   bone 2050 #> 4101  42.0     11 1919 1919-11-01 04:00:00  blood 2051 #> 4102  42.0     11 1919 1919-11-01 04:00:00   bone 2051 #> 4103  34.9     12 1919 1919-12-01 14:00:00  blood 2052 #> 4104  34.9     12 1919 1919-12-01 14:00:00   bone 2052 #> 4105  51.1      1 1920 1920-01-01 00:00:00  blood 2053 #> 4106  51.1      1 1920 1920-01-01 00:00:00   bone 2053 #> 4107  53.9      2 1920 1920-01-31 12:00:00  blood 2054 #> 4108  53.9      2 1920 1920-01-31 12:00:00   bone 2054 #> 4109  70.2      3 1920 1920-03-02 00:00:00  blood 2055 #> 4110  70.2      3 1920 1920-03-02 00:00:00   bone 2055 #> 4111  14.8      4 1920 1920-04-01 12:00:00  blood 2056 #> 4112  14.8      4 1920 1920-04-01 12:00:00   bone 2056 #> 4113  33.3      5 1920 1920-05-02 00:00:00  blood 2057 #> 4114  33.3      5 1920 1920-05-02 00:00:00   bone 2057 #> 4115  38.7      6 1920 1920-06-01 12:00:00  blood 2058 #> 4116  38.7      6 1920 1920-06-01 12:00:00   bone 2058 #> 4117  27.5      7 1920 1920-07-02 00:00:00  blood 2059 #> 4118  27.5      7 1920 1920-07-02 00:00:00   bone 2059 #> 4119  19.2      8 1920 1920-08-01 12:00:00  blood 2060 #> 4120  19.2      8 1920 1920-08-01 12:00:00   bone 2060 #> 4121  36.3      9 1920 1920-09-01 00:00:00  blood 2061 #> 4122  36.3      9 1920 1920-09-01 00:00:00   bone 2061 #> 4123  49.6     10 1920 1920-10-01 12:00:00  blood 2062 #> 4124  49.6     10 1920 1920-10-01 12:00:00   bone 2062 #> 4125  27.2     11 1920 1920-11-01 00:00:00  blood 2063 #> 4126  27.2     11 1920 1920-11-01 00:00:00   bone 2063 #> 4127  29.9     12 1920 1920-12-01 12:00:00  blood 2064 #> 4128  29.9     12 1920 1920-12-01 12:00:00   bone 2064 #> 4129  31.5      1 1921 1921-01-01 00:00:00  blood 2065 #> 4130  31.5      1 1921 1921-01-01 00:00:00   bone 2065 #> 4131  28.3      2 1921 1921-01-31 10:00:00  blood 2066 #> 4132  28.3      2 1921 1921-01-31 10:00:00   bone 2066 #> 4133  26.7      3 1921 1921-03-02 20:00:00  blood 2067 #> 4134  26.7      3 1921 1921-03-02 20:00:00   bone 2067 #> 4135  32.4      4 1921 1921-04-02 06:00:00  blood 2068 #> 4136  32.4      4 1921 1921-04-02 06:00:00   bone 2068 #> 4137  22.2      5 1921 1921-05-02 16:00:00  blood 2069 #> 4138  22.2      5 1921 1921-05-02 16:00:00   bone 2069 #> 4139  33.7      6 1921 1921-06-02 02:00:00  blood 2070 #> 4140  33.7      6 1921 1921-06-02 02:00:00   bone 2070 #> 4141  41.9      7 1921 1921-07-02 12:00:00  blood 2071 #> 4142  41.9      7 1921 1921-07-02 12:00:00   bone 2071 #> 4143  22.8      8 1921 1921-08-01 22:00:00  blood 2072 #> 4144  22.8      8 1921 1921-08-01 22:00:00   bone 2072 #> 4145  17.8      9 1921 1921-09-01 08:00:00  blood 2073 #> 4146  17.8      9 1921 1921-09-01 08:00:00   bone 2073 #> 4147  18.2     10 1921 1921-10-01 18:00:00  blood 2074 #> 4148  18.2     10 1921 1921-10-01 18:00:00   bone 2074 #> 4149  17.8     11 1921 1921-11-01 04:00:00  blood 2075 #> 4150  17.8     11 1921 1921-11-01 04:00:00   bone 2075 #> 4151  20.3     12 1921 1921-12-01 14:00:00  blood 2076 #> 4152  20.3     12 1921 1921-12-01 14:00:00   bone 2076 #> 4153  11.8      1 1922 1922-01-01 00:00:00  blood 2077 #> 4154  11.8      1 1922 1922-01-01 00:00:00   bone 2077 #> 4155  26.4      2 1922 1922-01-31 10:00:00  blood 2078 #> 4156  26.4      2 1922 1922-01-31 10:00:00   bone 2078 #> 4157  54.7      3 1922 1922-03-02 20:00:00  blood 2079 #> 4158  54.7      3 1922 1922-03-02 20:00:00   bone 2079 #> 4159  11.0      4 1922 1922-04-02 06:00:00  blood 2080 #> 4160  11.0      4 1922 1922-04-02 06:00:00   bone 2080 #> 4161   8.0      5 1922 1922-05-02 16:00:00  blood 2081 #> 4162   8.0      5 1922 1922-05-02 16:00:00   bone 2081 #> 4163   5.8      6 1922 1922-06-02 02:00:00  blood 2082 #> 4164   5.8      6 1922 1922-06-02 02:00:00   bone 2082 #> 4165  10.9      7 1922 1922-07-02 12:00:00  blood 2083 #> 4166  10.9      7 1922 1922-07-02 12:00:00   bone 2083 #> 4167   6.5      8 1922 1922-08-01 22:00:00  blood 2084 #> 4168   6.5      8 1922 1922-08-01 22:00:00   bone 2084 #> 4169   4.7      9 1922 1922-09-01 08:00:00  blood 2085 #> 4170   4.7      9 1922 1922-09-01 08:00:00   bone 2085 #> 4171   6.2     10 1922 1922-10-01 18:00:00  blood 2086 #> 4172   6.2     10 1922 1922-10-01 18:00:00   bone 2086 #> 4173   7.4     11 1922 1922-11-01 04:00:00  blood 2087 #> 4174   7.4     11 1922 1922-11-01 04:00:00   bone 2087 #> 4175  17.5     12 1922 1922-12-01 14:00:00  blood 2088 #> 4176  17.5     12 1922 1922-12-01 14:00:00   bone 2088 #> 4177   4.5      1 1923 1923-01-01 00:00:00  blood 2089 #> 4178   4.5      1 1923 1923-01-01 00:00:00   bone 2089 #> 4179   1.5      2 1923 1923-01-31 10:00:00  blood 2090 #> 4180   1.5      2 1923 1923-01-31 10:00:00   bone 2090 #> 4181   3.3      3 1923 1923-03-02 20:00:00  blood 2091 #> 4182   3.3      3 1923 1923-03-02 20:00:00   bone 2091 #> 4183   6.1      4 1923 1923-04-02 06:00:00  blood 2092 #> 4184   6.1      4 1923 1923-04-02 06:00:00   bone 2092 #> 4185   3.2      5 1923 1923-05-02 16:00:00  blood 2093 #> 4186   3.2      5 1923 1923-05-02 16:00:00   bone 2093 #> 4187   9.1      6 1923 1923-06-02 02:00:00  blood 2094 #> 4188   9.1      6 1923 1923-06-02 02:00:00   bone 2094 #> 4189   3.5      7 1923 1923-07-02 12:00:00  blood 2095 #> 4190   3.5      7 1923 1923-07-02 12:00:00   bone 2095 #> 4191   0.5      8 1923 1923-08-01 22:00:00  blood 2096 #> 4192   0.5      8 1923 1923-08-01 22:00:00   bone 2096 #> 4193  13.2      9 1923 1923-09-01 08:00:00  blood 2097 #> 4194  13.2      9 1923 1923-09-01 08:00:00   bone 2097 #> 4195  11.6     10 1923 1923-10-01 18:00:00  blood 2098 #> 4196  11.6     10 1923 1923-10-01 18:00:00   bone 2098 #> 4197  10.0     11 1923 1923-11-01 04:00:00  blood 2099 #> 4198  10.0     11 1923 1923-11-01 04:00:00   bone 2099 #> 4199   2.8     12 1923 1923-12-01 14:00:00  blood 2100 #> 4200   2.8     12 1923 1923-12-01 14:00:00   bone 2100 #> 4201   0.5      1 1924 1924-01-01 00:00:00  blood 2101 #> 4202   0.5      1 1924 1924-01-01 00:00:00   bone 2101 #> 4203   5.1      2 1924 1924-01-31 12:00:00  blood 2102 #> 4204   5.1      2 1924 1924-01-31 12:00:00   bone 2102 #> 4205   1.8      3 1924 1924-03-02 00:00:00  blood 2103 #> 4206   1.8      3 1924 1924-03-02 00:00:00   bone 2103 #> 4207  11.3      4 1924 1924-04-01 12:00:00  blood 2104 #> 4208  11.3      4 1924 1924-04-01 12:00:00   bone 2104 #> 4209  20.8      5 1924 1924-05-02 00:00:00  blood 2105 #> 4210  20.8      5 1924 1924-05-02 00:00:00   bone 2105 #> 4211  24.0      6 1924 1924-06-01 12:00:00  blood 2106 #> 4212  24.0      6 1924 1924-06-01 12:00:00   bone 2106 #> 4213  28.1      7 1924 1924-07-02 00:00:00  blood 2107 #> 4214  28.1      7 1924 1924-07-02 00:00:00   bone 2107 #> 4215  19.3      8 1924 1924-08-01 12:00:00  blood 2108 #> 4216  19.3      8 1924 1924-08-01 12:00:00   bone 2108 #> 4217  25.1      9 1924 1924-09-01 00:00:00  blood 2109 #> 4218  25.1      9 1924 1924-09-01 00:00:00   bone 2109 #> 4219  25.6     10 1924 1924-10-01 12:00:00  blood 2110 #> 4220  25.6     10 1924 1924-10-01 12:00:00   bone 2110 #> 4221  22.5     11 1924 1924-11-01 00:00:00  blood 2111 #> 4222  22.5     11 1924 1924-11-01 00:00:00   bone 2111 #> 4223  16.5     12 1924 1924-12-01 12:00:00  blood 2112 #> 4224  16.5     12 1924 1924-12-01 12:00:00   bone 2112 #> 4225   5.5      1 1925 1925-01-01 00:00:00  blood 2113 #> 4226   5.5      1 1925 1925-01-01 00:00:00   bone 2113 #> 4227  23.2      2 1925 1925-01-31 10:00:00  blood 2114 #> 4228  23.2      2 1925 1925-01-31 10:00:00   bone 2114 #> 4229  18.0      3 1925 1925-03-02 20:00:00  blood 2115 #> 4230  18.0      3 1925 1925-03-02 20:00:00   bone 2115 #> 4231  31.7      4 1925 1925-04-02 06:00:00  blood 2116 #> 4232  31.7      4 1925 1925-04-02 06:00:00   bone 2116 #> 4233  42.8      5 1925 1925-05-02 16:00:00  blood 2117 #> 4234  42.8      5 1925 1925-05-02 16:00:00   bone 2117 #> 4235  47.5      6 1925 1925-06-02 02:00:00  blood 2118 #> 4236  47.5      6 1925 1925-06-02 02:00:00   bone 2118 #> 4237  38.5      7 1925 1925-07-02 12:00:00  blood 2119 #> 4238  38.5      7 1925 1925-07-02 12:00:00   bone 2119 #> 4239  37.9      8 1925 1925-08-01 22:00:00  blood 2120 #> 4240  37.9      8 1925 1925-08-01 22:00:00   bone 2120 #> 4241  60.2      9 1925 1925-09-01 08:00:00  blood 2121 #> 4242  60.2      9 1925 1925-09-01 08:00:00   bone 2121 #> 4243  69.2     10 1925 1925-10-01 18:00:00  blood 2122 #> 4244  69.2     10 1925 1925-10-01 18:00:00   bone 2122 #> 4245  58.6     11 1925 1925-11-01 04:00:00  blood 2123 #> 4246  58.6     11 1925 1925-11-01 04:00:00   bone 2123 #> 4247  98.6     12 1925 1925-12-01 14:00:00  blood 2124 #> 4248  98.6     12 1925 1925-12-01 14:00:00   bone 2124 #> 4249  71.8      1 1926 1926-01-01 00:00:00  blood 2125 #> 4250  71.8      1 1926 1926-01-01 00:00:00   bone 2125 #> 4251  70.0      2 1926 1926-01-31 10:00:00  blood 2126 #> 4252  70.0      2 1926 1926-01-31 10:00:00   bone 2126 #> 4253  62.5      3 1926 1926-03-02 20:00:00  blood 2127 #> 4254  62.5      3 1926 1926-03-02 20:00:00   bone 2127 #> 4255  38.5      4 1926 1926-04-02 06:00:00  blood 2128 #> 4256  38.5      4 1926 1926-04-02 06:00:00   bone 2128 #> 4257  64.3      5 1926 1926-05-02 16:00:00  blood 2129 #> 4258  64.3      5 1926 1926-05-02 16:00:00   bone 2129 #> 4259  73.5      6 1926 1926-06-02 02:00:00  blood 2130 #> 4260  73.5      6 1926 1926-06-02 02:00:00   bone 2130 #> 4261  52.3      7 1926 1926-07-02 12:00:00  blood 2131 #> 4262  52.3      7 1926 1926-07-02 12:00:00   bone 2131 #> 4263  61.6      8 1926 1926-08-01 22:00:00  blood 2132 #> 4264  61.6      8 1926 1926-08-01 22:00:00   bone 2132 #> 4265  60.8      9 1926 1926-09-01 08:00:00  blood 2133 #> 4266  60.8      9 1926 1926-09-01 08:00:00   bone 2133 #> 4267  71.5     10 1926 1926-10-01 18:00:00  blood 2134 #> 4268  71.5     10 1926 1926-10-01 18:00:00   bone 2134 #> 4269  60.5     11 1926 1926-11-01 04:00:00  blood 2135 #> 4270  60.5     11 1926 1926-11-01 04:00:00   bone 2135 #> 4271  79.4     12 1926 1926-12-01 14:00:00  blood 2136 #> 4272  79.4     12 1926 1926-12-01 14:00:00   bone 2136 #> 4273  81.6      1 1927 1927-01-01 00:00:00  blood 2137 #> 4274  81.6      1 1927 1927-01-01 00:00:00   bone 2137 #> 4275  93.0      2 1927 1927-01-31 10:00:00  blood 2138 #> 4276  93.0      2 1927 1927-01-31 10:00:00   bone 2138 #> 4277  69.6      3 1927 1927-03-02 20:00:00  blood 2139 #> 4278  69.6      3 1927 1927-03-02 20:00:00   bone 2139 #> 4279  93.5      4 1927 1927-04-02 06:00:00  blood 2140 #> 4280  93.5      4 1927 1927-04-02 06:00:00   bone 2140 #> 4281  79.1      5 1927 1927-05-02 16:00:00  blood 2141 #> 4282  79.1      5 1927 1927-05-02 16:00:00   bone 2141 #> 4283  59.1      6 1927 1927-06-02 02:00:00  blood 2142 #> 4284  59.1      6 1927 1927-06-02 02:00:00   bone 2142 #> 4285  54.9      7 1927 1927-07-02 12:00:00  blood 2143 #> 4286  54.9      7 1927 1927-07-02 12:00:00   bone 2143 #> 4287  53.8      8 1927 1927-08-01 22:00:00  blood 2144 #> 4288  53.8      8 1927 1927-08-01 22:00:00   bone 2144 #> 4289  68.4      9 1927 1927-09-01 08:00:00  blood 2145 #> 4290  68.4      9 1927 1927-09-01 08:00:00   bone 2145 #> 4291  63.1     10 1927 1927-10-01 18:00:00  blood 2146 #> 4292  63.1     10 1927 1927-10-01 18:00:00   bone 2146 #> 4293  67.2     11 1927 1927-11-01 04:00:00  blood 2147 #> 4294  67.2     11 1927 1927-11-01 04:00:00   bone 2147 #> 4295  45.2     12 1927 1927-12-01 14:00:00  blood 2148 #> 4296  45.2     12 1927 1927-12-01 14:00:00   bone 2148 #> 4297  83.5      1 1928 1928-01-01 00:00:00  blood 2149 #> 4298  83.5      1 1928 1928-01-01 00:00:00   bone 2149 #> 4299  73.5      2 1928 1928-01-31 12:00:00  blood 2150 #> 4300  73.5      2 1928 1928-01-31 12:00:00   bone 2150 #> 4301  85.4      3 1928 1928-03-02 00:00:00  blood 2151 #> 4302  85.4      3 1928 1928-03-02 00:00:00   bone 2151 #> 4303  80.6      4 1928 1928-04-01 12:00:00  blood 2152 #> 4304  80.6      4 1928 1928-04-01 12:00:00   bone 2152 #> 4305  76.9      5 1928 1928-05-02 00:00:00  blood 2153 #> 4306  76.9      5 1928 1928-05-02 00:00:00   bone 2153 #> 4307  91.4      6 1928 1928-06-01 12:00:00  blood 2154 #> 4308  91.4      6 1928 1928-06-01 12:00:00   bone 2154 #> 4309  98.0      7 1928 1928-07-02 00:00:00  blood 2155 #> 4310  98.0      7 1928 1928-07-02 00:00:00   bone 2155 #> 4311  83.8      8 1928 1928-08-01 12:00:00  blood 2156 #> 4312  83.8      8 1928 1928-08-01 12:00:00   bone 2156 #> 4313  89.7      9 1928 1928-09-01 00:00:00  blood 2157 #> 4314  89.7      9 1928 1928-09-01 00:00:00   bone 2157 #> 4315  61.4     10 1928 1928-10-01 12:00:00  blood 2158 #> 4316  61.4     10 1928 1928-10-01 12:00:00   bone 2158 #> 4317  50.3     11 1928 1928-11-01 00:00:00  blood 2159 #> 4318  50.3     11 1928 1928-11-01 00:00:00   bone 2159 #> 4319  59.0     12 1928 1928-12-01 12:00:00  blood 2160 #> 4320  59.0     12 1928 1928-12-01 12:00:00   bone 2160 #> 4321  68.9      1 1929 1929-01-01 00:00:00  blood 2161 #> 4322  68.9      1 1929 1929-01-01 00:00:00   bone 2161 #> 4323  64.1      2 1929 1929-01-31 10:00:00  blood 2162 #> 4324  64.1      2 1929 1929-01-31 10:00:00   bone 2162 #> 4325  50.2      3 1929 1929-03-02 20:00:00  blood 2163 #> 4326  50.2      3 1929 1929-03-02 20:00:00   bone 2163 #> 4327  52.8      4 1929 1929-04-02 06:00:00  blood 2164 #> 4328  52.8      4 1929 1929-04-02 06:00:00   bone 2164 #> 4329  58.2      5 1929 1929-05-02 16:00:00  blood 2165 #> 4330  58.2      5 1929 1929-05-02 16:00:00   bone 2165 #> 4331  71.9      6 1929 1929-06-02 02:00:00  blood 2166 #> 4332  71.9      6 1929 1929-06-02 02:00:00   bone 2166 #> 4333  70.2      7 1929 1929-07-02 12:00:00  blood 2167 #> 4334  70.2      7 1929 1929-07-02 12:00:00   bone 2167 #> 4335  65.8      8 1929 1929-08-01 22:00:00  blood 2168 #> 4336  65.8      8 1929 1929-08-01 22:00:00   bone 2168 #> 4337  34.4      9 1929 1929-09-01 08:00:00  blood 2169 #> 4338  34.4      9 1929 1929-09-01 08:00:00   bone 2169 #> 4339  54.0     10 1929 1929-10-01 18:00:00  blood 2170 #> 4340  54.0     10 1929 1929-10-01 18:00:00   bone 2170 #> 4341  81.1     11 1929 1929-11-01 04:00:00  blood 2171 #> 4342  81.1     11 1929 1929-11-01 04:00:00   bone 2171 #> 4343 108.0     12 1929 1929-12-01 14:00:00  blood 2172 #> 4344 108.0     12 1929 1929-12-01 14:00:00   bone 2172 #> 4345  65.3      1 1930 1930-01-01 00:00:00  blood 2173 #> 4346  65.3      1 1930 1930-01-01 00:00:00   bone 2173 #> 4347  49.2      2 1930 1930-01-31 10:00:00  blood 2174 #> 4348  49.2      2 1930 1930-01-31 10:00:00   bone 2174 #> 4349  35.0      3 1930 1930-03-02 20:00:00  blood 2175 #> 4350  35.0      3 1930 1930-03-02 20:00:00   bone 2175 #> 4351  38.2      4 1930 1930-04-02 06:00:00  blood 2176 #> 4352  38.2      4 1930 1930-04-02 06:00:00   bone 2176 #> 4353  36.8      5 1930 1930-05-02 16:00:00  blood 2177 #> 4354  36.8      5 1930 1930-05-02 16:00:00   bone 2177 #> 4355  28.8      6 1930 1930-06-02 02:00:00  blood 2178 #> 4356  28.8      6 1930 1930-06-02 02:00:00   bone 2178 #> 4357  21.9      7 1930 1930-07-02 12:00:00  blood 2179 #> 4358  21.9      7 1930 1930-07-02 12:00:00   bone 2179 #> 4359  24.9      8 1930 1930-08-01 22:00:00  blood 2180 #> 4360  24.9      8 1930 1930-08-01 22:00:00   bone 2180 #> 4361  32.1      9 1930 1930-09-01 08:00:00  blood 2181 #> 4362  32.1      9 1930 1930-09-01 08:00:00   bone 2181 #> 4363  34.4     10 1930 1930-10-01 18:00:00  blood 2182 #> 4364  34.4     10 1930 1930-10-01 18:00:00   bone 2182 #> 4365  35.6     11 1930 1930-11-01 04:00:00  blood 2183 #> 4366  35.6     11 1930 1930-11-01 04:00:00   bone 2183 #> 4367  25.8     12 1930 1930-12-01 14:00:00  blood 2184 #> 4368  25.8     12 1930 1930-12-01 14:00:00   bone 2184 #> 4369  14.6      1 1931 1931-01-01 00:00:00  blood 2185 #> 4370  14.6      1 1931 1931-01-01 00:00:00   bone 2185 #> 4371  43.1      2 1931 1931-01-31 10:00:00  blood 2186 #> 4372  43.1      2 1931 1931-01-31 10:00:00   bone 2186 #> 4373  30.0      3 1931 1931-03-02 20:00:00  blood 2187 #> 4374  30.0      3 1931 1931-03-02 20:00:00   bone 2187 #> 4375  31.2      4 1931 1931-04-02 06:00:00  blood 2188 #> 4376  31.2      4 1931 1931-04-02 06:00:00   bone 2188 #> 4377  24.6      5 1931 1931-05-02 16:00:00  blood 2189 #> 4378  24.6      5 1931 1931-05-02 16:00:00   bone 2189 #> 4379  15.3      6 1931 1931-06-02 02:00:00  blood 2190 #> 4380  15.3      6 1931 1931-06-02 02:00:00   bone 2190 #> 4381  17.4      7 1931 1931-07-02 12:00:00  blood 2191 #> 4382  17.4      7 1931 1931-07-02 12:00:00   bone 2191 #> 4383  13.0      8 1931 1931-08-01 22:00:00  blood 2192 #> 4384  13.0      8 1931 1931-08-01 22:00:00   bone 2192 #> 4385  19.0      9 1931 1931-09-01 08:00:00  blood 2193 #> 4386  19.0      9 1931 1931-09-01 08:00:00   bone 2193 #> 4387  10.0     10 1931 1931-10-01 18:00:00  blood 2194 #> 4388  10.0     10 1931 1931-10-01 18:00:00   bone 2194 #> 4389  18.7     11 1931 1931-11-01 04:00:00  blood 2195 #> 4390  18.7     11 1931 1931-11-01 04:00:00   bone 2195 #> 4391  17.8     12 1931 1931-12-01 14:00:00  blood 2196 #> 4392  17.8     12 1931 1931-12-01 14:00:00   bone 2196 #> 4393  12.1      1 1932 1932-01-01 00:00:00  blood 2197 #> 4394  12.1      1 1932 1932-01-01 00:00:00   bone 2197 #> 4395  10.6      2 1932 1932-01-31 12:00:00  blood 2198 #> 4396  10.6      2 1932 1932-01-31 12:00:00   bone 2198 #> 4397  11.2      3 1932 1932-03-02 00:00:00  blood 2199 #> 4398  11.2      3 1932 1932-03-02 00:00:00   bone 2199 #> 4399  11.2      4 1932 1932-04-01 12:00:00  blood 2200 #> 4400  11.2      4 1932 1932-04-01 12:00:00   bone 2200 #> 4401  17.9      5 1932 1932-05-02 00:00:00  blood 2201 #> 4402  17.9      5 1932 1932-05-02 00:00:00   bone 2201 #> 4403  22.2      6 1932 1932-06-01 12:00:00  blood 2202 #> 4404  22.2      6 1932 1932-06-01 12:00:00   bone 2202 #> 4405   9.6      7 1932 1932-07-02 00:00:00  blood 2203 #> 4406   9.6      7 1932 1932-07-02 00:00:00   bone 2203 #> 4407   6.8      8 1932 1932-08-01 12:00:00  blood 2204 #> 4408   6.8      8 1932 1932-08-01 12:00:00   bone 2204 #> 4409   4.0      9 1932 1932-09-01 00:00:00  blood 2205 #> 4410   4.0      9 1932 1932-09-01 00:00:00   bone 2205 #> 4411   8.9     10 1932 1932-10-01 12:00:00  blood 2206 #> 4412   8.9     10 1932 1932-10-01 12:00:00   bone 2206 #> 4413   8.2     11 1932 1932-11-01 00:00:00  blood 2207 #> 4414   8.2     11 1932 1932-11-01 00:00:00   bone 2207 #> 4415  11.0     12 1932 1932-12-01 12:00:00  blood 2208 #> 4416  11.0     12 1932 1932-12-01 12:00:00   bone 2208 #> 4417  12.3      1 1933 1933-01-01 00:00:00  blood 2209 #> 4418  12.3      1 1933 1933-01-01 00:00:00   bone 2209 #> 4419  22.2      2 1933 1933-01-31 10:00:00  blood 2210 #> 4420  22.2      2 1933 1933-01-31 10:00:00   bone 2210 #> 4421  10.1      3 1933 1933-03-02 20:00:00  blood 2211 #> 4422  10.1      3 1933 1933-03-02 20:00:00   bone 2211 #> 4423   2.9      4 1933 1933-04-02 06:00:00  blood 2212 #> 4424   2.9      4 1933 1933-04-02 06:00:00   bone 2212 #> 4425   3.2      5 1933 1933-05-02 16:00:00  blood 2213 #> 4426   3.2      5 1933 1933-05-02 16:00:00   bone 2213 #> 4427   5.2      6 1933 1933-06-02 02:00:00  blood 2214 #> 4428   5.2      6 1933 1933-06-02 02:00:00   bone 2214 #> 4429   2.8      7 1933 1933-07-02 12:00:00  blood 2215 #> 4430   2.8      7 1933 1933-07-02 12:00:00   bone 2215 #> 4431   0.2      8 1933 1933-08-01 22:00:00  blood 2216 #> 4432   0.2      8 1933 1933-08-01 22:00:00   bone 2216 #> 4433   5.1      9 1933 1933-09-01 08:00:00  blood 2217 #> 4434   5.1      9 1933 1933-09-01 08:00:00   bone 2217 #> 4435   3.0     10 1933 1933-10-01 18:00:00  blood 2218 #> 4436   3.0     10 1933 1933-10-01 18:00:00   bone 2218 #> 4437   0.6     11 1933 1933-11-01 04:00:00  blood 2219 #> 4438   0.6     11 1933 1933-11-01 04:00:00   bone 2219 #> 4439   0.3     12 1933 1933-12-01 14:00:00  blood 2220 #> 4440   0.3     12 1933 1933-12-01 14:00:00   bone 2220 #> 4441   3.4      1 1934 1934-01-01 00:00:00  blood 2221 #> 4442   3.4      1 1934 1934-01-01 00:00:00   bone 2221 #> 4443   7.8      2 1934 1934-01-31 10:00:00  blood 2222 #> 4444   7.8      2 1934 1934-01-31 10:00:00   bone 2222 #> 4445   4.3      3 1934 1934-03-02 20:00:00  blood 2223 #> 4446   4.3      3 1934 1934-03-02 20:00:00   bone 2223 #> 4447  11.3      4 1934 1934-04-02 06:00:00  blood 2224 #> 4448  11.3      4 1934 1934-04-02 06:00:00   bone 2224 #> 4449  19.7      5 1934 1934-05-02 16:00:00  blood 2225 #> 4450  19.7      5 1934 1934-05-02 16:00:00   bone 2225 #> 4451   6.7      6 1934 1934-06-02 02:00:00  blood 2226 #> 4452   6.7      6 1934 1934-06-02 02:00:00   bone 2226 #> 4453   9.3      7 1934 1934-07-02 12:00:00  blood 2227 #> 4454   9.3      7 1934 1934-07-02 12:00:00   bone 2227 #> 4455   8.3      8 1934 1934-08-01 22:00:00  blood 2228 #> 4456   8.3      8 1934 1934-08-01 22:00:00   bone 2228 #> 4457   4.0      9 1934 1934-09-01 08:00:00  blood 2229 #> 4458   4.0      9 1934 1934-09-01 08:00:00   bone 2229 #> 4459   5.7     10 1934 1934-10-01 18:00:00  blood 2230 #> 4460   5.7     10 1934 1934-10-01 18:00:00   bone 2230 #> 4461   8.7     11 1934 1934-11-01 04:00:00  blood 2231 #> 4462   8.7     11 1934 1934-11-01 04:00:00   bone 2231 #> 4463  15.4     12 1934 1934-12-01 14:00:00  blood 2232 #> 4464  15.4     12 1934 1934-12-01 14:00:00   bone 2232 #> 4465  18.9      1 1935 1935-01-01 00:00:00  blood 2233 #> 4466  18.9      1 1935 1935-01-01 00:00:00   bone 2233 #> 4467  20.5      2 1935 1935-01-31 10:00:00  blood 2234 #> 4468  20.5      2 1935 1935-01-31 10:00:00   bone 2234 #> 4469  23.1      3 1935 1935-03-02 20:00:00  blood 2235 #> 4470  23.1      3 1935 1935-03-02 20:00:00   bone 2235 #> 4471  12.2      4 1935 1935-04-02 06:00:00  blood 2236 #> 4472  12.2      4 1935 1935-04-02 06:00:00   bone 2236 #> 4473  27.3      5 1935 1935-05-02 16:00:00  blood 2237 #> 4474  27.3      5 1935 1935-05-02 16:00:00   bone 2237 #> 4475  45.7      6 1935 1935-06-02 02:00:00  blood 2238 #> 4476  45.7      6 1935 1935-06-02 02:00:00   bone 2238 #> 4477  33.9      7 1935 1935-07-02 12:00:00  blood 2239 #> 4478  33.9      7 1935 1935-07-02 12:00:00   bone 2239 #> 4479  30.1      8 1935 1935-08-01 22:00:00  blood 2240 #> 4480  30.1      8 1935 1935-08-01 22:00:00   bone 2240 #> 4481  42.1      9 1935 1935-09-01 08:00:00  blood 2241 #> 4482  42.1      9 1935 1935-09-01 08:00:00   bone 2241 #> 4483  53.2     10 1935 1935-10-01 18:00:00  blood 2242 #> 4484  53.2     10 1935 1935-10-01 18:00:00   bone 2242 #> 4485  64.2     11 1935 1935-11-01 04:00:00  blood 2243 #> 4486  64.2     11 1935 1935-11-01 04:00:00   bone 2243 #> 4487  61.5     12 1935 1935-12-01 14:00:00  blood 2244 #> 4488  61.5     12 1935 1935-12-01 14:00:00   bone 2244 #> 4489  62.8      1 1936 1936-01-01 00:00:00  blood 2245 #> 4490  62.8      1 1936 1936-01-01 00:00:00   bone 2245 #> 4491  74.3      2 1936 1936-01-31 12:00:00  blood 2246 #> 4492  74.3      2 1936 1936-01-31 12:00:00   bone 2246 #> 4493  77.1      3 1936 1936-03-02 00:00:00  blood 2247 #> 4494  77.1      3 1936 1936-03-02 00:00:00   bone 2247 #> 4495  74.9      4 1936 1936-04-01 12:00:00  blood 2248 #> 4496  74.9      4 1936 1936-04-01 12:00:00   bone 2248 #> 4497  54.6      5 1936 1936-05-02 00:00:00  blood 2249 #> 4498  54.6      5 1936 1936-05-02 00:00:00   bone 2249 #> 4499  70.0      6 1936 1936-06-01 12:00:00  blood 2250 #> 4500  70.0      6 1936 1936-06-01 12:00:00   bone 2250 #> 4501  52.3      7 1936 1936-07-02 00:00:00  blood 2251 #> 4502  52.3      7 1936 1936-07-02 00:00:00   bone 2251 #> 4503  87.0      8 1936 1936-08-01 12:00:00  blood 2252 #> 4504  87.0      8 1936 1936-08-01 12:00:00   bone 2252 #> 4505  76.0      9 1936 1936-09-01 00:00:00  blood 2253 #> 4506  76.0      9 1936 1936-09-01 00:00:00   bone 2253 #> 4507  89.0     10 1936 1936-10-01 12:00:00  blood 2254 #> 4508  89.0     10 1936 1936-10-01 12:00:00   bone 2254 #> 4509 115.4     11 1936 1936-11-01 00:00:00  blood 2255 #> 4510 115.4     11 1936 1936-11-01 00:00:00   bone 2255 #> 4511 123.4     12 1936 1936-12-01 12:00:00  blood 2256 #> 4512 123.4     12 1936 1936-12-01 12:00:00   bone 2256 #> 4513 132.5      1 1937 1937-01-01 00:00:00  blood 2257 #> 4514 132.5      1 1937 1937-01-01 00:00:00   bone 2257 #> 4515 128.5      2 1937 1937-01-31 10:00:00  blood 2258 #> 4516 128.5      2 1937 1937-01-31 10:00:00   bone 2258 #> 4517  83.9      3 1937 1937-03-02 20:00:00  blood 2259 #> 4518  83.9      3 1937 1937-03-02 20:00:00   bone 2259 #> 4519 109.3      4 1937 1937-04-02 06:00:00  blood 2260 #> 4520 109.3      4 1937 1937-04-02 06:00:00   bone 2260 #> 4521 116.7      5 1937 1937-05-02 16:00:00  blood 2261 #> 4522 116.7      5 1937 1937-05-02 16:00:00   bone 2261 #> 4523 130.3      6 1937 1937-06-02 02:00:00  blood 2262 #> 4524 130.3      6 1937 1937-06-02 02:00:00   bone 2262 #> 4525 145.1      7 1937 1937-07-02 12:00:00  blood 2263 #> 4526 145.1      7 1937 1937-07-02 12:00:00   bone 2263 #> 4527 137.7      8 1937 1937-08-01 22:00:00  blood 2264 #> 4528 137.7      8 1937 1937-08-01 22:00:00   bone 2264 #> 4529 100.7      9 1937 1937-09-01 08:00:00  blood 2265 #> 4530 100.7      9 1937 1937-09-01 08:00:00   bone 2265 #> 4531 124.9     10 1937 1937-10-01 18:00:00  blood 2266 #> 4532 124.9     10 1937 1937-10-01 18:00:00   bone 2266 #> 4533  74.4     11 1937 1937-11-01 04:00:00  blood 2267 #> 4534  74.4     11 1937 1937-11-01 04:00:00   bone 2267 #> 4535  88.8     12 1937 1937-12-01 14:00:00  blood 2268 #> 4536  88.8     12 1937 1937-12-01 14:00:00   bone 2268 #> 4537  98.4      1 1938 1938-01-01 00:00:00  blood 2269 #> 4538  98.4      1 1938 1938-01-01 00:00:00   bone 2269 #> 4539 119.2      2 1938 1938-01-31 10:00:00  blood 2270 #> 4540 119.2      2 1938 1938-01-31 10:00:00   bone 2270 #> 4541  86.5      3 1938 1938-03-02 20:00:00  blood 2271 #> 4542  86.5      3 1938 1938-03-02 20:00:00   bone 2271 #> 4543 101.0      4 1938 1938-04-02 06:00:00  blood 2272 #> 4544 101.0      4 1938 1938-04-02 06:00:00   bone 2272 #> 4545 127.4      5 1938 1938-05-02 16:00:00  blood 2273 #> 4546 127.4      5 1938 1938-05-02 16:00:00   bone 2273 #> 4547  97.5      6 1938 1938-06-02 02:00:00  blood 2274 #> 4548  97.5      6 1938 1938-06-02 02:00:00   bone 2274 #> 4549 165.3      7 1938 1938-07-02 12:00:00  blood 2275 #> 4550 165.3      7 1938 1938-07-02 12:00:00   bone 2275 #> 4551 115.7      8 1938 1938-08-01 22:00:00  blood 2276 #> 4552 115.7      8 1938 1938-08-01 22:00:00   bone 2276 #> 4553  89.6      9 1938 1938-09-01 08:00:00  blood 2277 #> 4554  89.6      9 1938 1938-09-01 08:00:00   bone 2277 #> 4555  99.1     10 1938 1938-10-01 18:00:00  blood 2278 #> 4556  99.1     10 1938 1938-10-01 18:00:00   bone 2278 #> 4557 122.2     11 1938 1938-11-01 04:00:00  blood 2279 #> 4558 122.2     11 1938 1938-11-01 04:00:00   bone 2279 #> 4559  92.7     12 1938 1938-12-01 14:00:00  blood 2280 #> 4560  92.7     12 1938 1938-12-01 14:00:00   bone 2280 #> 4561  80.3      1 1939 1939-01-01 00:00:00  blood 2281 #> 4562  80.3      1 1939 1939-01-01 00:00:00   bone 2281 #> 4563  77.4      2 1939 1939-01-31 10:00:00  blood 2282 #> 4564  77.4      2 1939 1939-01-31 10:00:00   bone 2282 #> 4565  64.6      3 1939 1939-03-02 20:00:00  blood 2283 #> 4566  64.6      3 1939 1939-03-02 20:00:00   bone 2283 #> 4567 109.1      4 1939 1939-04-02 06:00:00  blood 2284 #> 4568 109.1      4 1939 1939-04-02 06:00:00   bone 2284 #> 4569 118.3      5 1939 1939-05-02 16:00:00  blood 2285 #> 4570 118.3      5 1939 1939-05-02 16:00:00   bone 2285 #> 4571 101.0      6 1939 1939-06-02 02:00:00  blood 2286 #> 4572 101.0      6 1939 1939-06-02 02:00:00   bone 2286 #> 4573  97.6      7 1939 1939-07-02 12:00:00  blood 2287 #> 4574  97.6      7 1939 1939-07-02 12:00:00   bone 2287 #> 4575 105.8      8 1939 1939-08-01 22:00:00  blood 2288 #> 4576 105.8      8 1939 1939-08-01 22:00:00   bone 2288 #> 4577 112.6      9 1939 1939-09-01 08:00:00  blood 2289 #> 4578 112.6      9 1939 1939-09-01 08:00:00   bone 2289 #> 4579  88.1     10 1939 1939-10-01 18:00:00  blood 2290 #> 4580  88.1     10 1939 1939-10-01 18:00:00   bone 2290 #> 4581  68.1     11 1939 1939-11-01 04:00:00  blood 2291 #> 4582  68.1     11 1939 1939-11-01 04:00:00   bone 2291 #> 4583  42.1     12 1939 1939-12-01 14:00:00  blood 2292 #> 4584  42.1     12 1939 1939-12-01 14:00:00   bone 2292 #> 4585  50.5      1 1940 1940-01-01 00:00:00  blood 2293 #> 4586  50.5      1 1940 1940-01-01 00:00:00   bone 2293 #> 4587  59.4      2 1940 1940-01-31 12:00:00  blood 2294 #> 4588  59.4      2 1940 1940-01-31 12:00:00   bone 2294 #> 4589  83.3      3 1940 1940-03-02 00:00:00  blood 2295 #> 4590  83.3      3 1940 1940-03-02 00:00:00   bone 2295 #> 4591  60.7      4 1940 1940-04-01 12:00:00  blood 2296 #> 4592  60.7      4 1940 1940-04-01 12:00:00   bone 2296 #> 4593  54.4      5 1940 1940-05-02 00:00:00  blood 2297 #> 4594  54.4      5 1940 1940-05-02 00:00:00   bone 2297 #> 4595  83.9      6 1940 1940-06-01 12:00:00  blood 2298 #> 4596  83.9      6 1940 1940-06-01 12:00:00   bone 2298 #> 4597  67.5      7 1940 1940-07-02 00:00:00  blood 2299 #> 4598  67.5      7 1940 1940-07-02 00:00:00   bone 2299 #> 4599 105.5      8 1940 1940-08-01 12:00:00  blood 2300 #> 4600 105.5      8 1940 1940-08-01 12:00:00   bone 2300 #> 4601  66.5      9 1940 1940-09-01 00:00:00  blood 2301 #> 4602  66.5      9 1940 1940-09-01 00:00:00   bone 2301 #> 4603  55.0     10 1940 1940-10-01 12:00:00  blood 2302 #> 4604  55.0     10 1940 1940-10-01 12:00:00   bone 2302 #> 4605  58.4     11 1940 1940-11-01 00:00:00  blood 2303 #> 4606  58.4     11 1940 1940-11-01 00:00:00   bone 2303 #> 4607  68.3     12 1940 1940-12-01 12:00:00  blood 2304 #> 4608  68.3     12 1940 1940-12-01 12:00:00   bone 2304 #> 4609  45.6      1 1941 1941-01-01 00:00:00  blood 2305 #> 4610  45.6      1 1941 1941-01-01 00:00:00   bone 2305 #> 4611  44.5      2 1941 1941-01-31 10:00:00  blood 2306 #> 4612  44.5      2 1941 1941-01-31 10:00:00   bone 2306 #> 4613  46.4      3 1941 1941-03-02 20:00:00  blood 2307 #> 4614  46.4      3 1941 1941-03-02 20:00:00   bone 2307 #> 4615  32.8      4 1941 1941-04-02 06:00:00  blood 2308 #> 4616  32.8      4 1941 1941-04-02 06:00:00   bone 2308 #> 4617  29.5      5 1941 1941-05-02 16:00:00  blood 2309 #> 4618  29.5      5 1941 1941-05-02 16:00:00   bone 2309 #> 4619  59.8      6 1941 1941-06-02 02:00:00  blood 2310 #> 4620  59.8      6 1941 1941-06-02 02:00:00   bone 2310 #> 4621  66.9      7 1941 1941-07-02 12:00:00  blood 2311 #> 4622  66.9      7 1941 1941-07-02 12:00:00   bone 2311 #> 4623  60.0      8 1941 1941-08-01 22:00:00  blood 2312 #> 4624  60.0      8 1941 1941-08-01 22:00:00   bone 2312 #> 4625  65.9      9 1941 1941-09-01 08:00:00  blood 2313 #> 4626  65.9      9 1941 1941-09-01 08:00:00   bone 2313 #> 4627  46.3     10 1941 1941-10-01 18:00:00  blood 2314 #> 4628  46.3     10 1941 1941-10-01 18:00:00   bone 2314 #> 4629  38.3     11 1941 1941-11-01 04:00:00  blood 2315 #> 4630  38.3     11 1941 1941-11-01 04:00:00   bone 2315 #> 4631  33.7     12 1941 1941-12-01 14:00:00  blood 2316 #> 4632  33.7     12 1941 1941-12-01 14:00:00   bone 2316 #> 4633  35.6      1 1942 1942-01-01 00:00:00  blood 2317 #> 4634  35.6      1 1942 1942-01-01 00:00:00   bone 2317 #> 4635  52.8      2 1942 1942-01-31 10:00:00  blood 2318 #> 4636  52.8      2 1942 1942-01-31 10:00:00   bone 2318 #> 4637  54.2      3 1942 1942-03-02 20:00:00  blood 2319 #> 4638  54.2      3 1942 1942-03-02 20:00:00   bone 2319 #> 4639  60.7      4 1942 1942-04-02 06:00:00  blood 2320 #> 4640  60.7      4 1942 1942-04-02 06:00:00   bone 2320 #> 4641  25.0      5 1942 1942-05-02 16:00:00  blood 2321 #> 4642  25.0      5 1942 1942-05-02 16:00:00   bone 2321 #> 4643  11.4      6 1942 1942-06-02 02:00:00  blood 2322 #> 4644  11.4      6 1942 1942-06-02 02:00:00   bone 2322 #> 4645  17.7      7 1942 1942-07-02 12:00:00  blood 2323 #> 4646  17.7      7 1942 1942-07-02 12:00:00   bone 2323 #> 4647  20.2      8 1942 1942-08-01 22:00:00  blood 2324 #> 4648  20.2      8 1942 1942-08-01 22:00:00   bone 2324 #> 4649  17.2      9 1942 1942-09-01 08:00:00  blood 2325 #> 4650  17.2      9 1942 1942-09-01 08:00:00   bone 2325 #> 4651  19.2     10 1942 1942-10-01 18:00:00  blood 2326 #> 4652  19.2     10 1942 1942-10-01 18:00:00   bone 2326 #> 4653  30.7     11 1942 1942-11-01 04:00:00  blood 2327 #> 4654  30.7     11 1942 1942-11-01 04:00:00   bone 2327 #> 4655  22.5     12 1942 1942-12-01 14:00:00  blood 2328 #> 4656  22.5     12 1942 1942-12-01 14:00:00   bone 2328 #> 4657  12.4      1 1943 1943-01-01 00:00:00  blood 2329 #> 4658  12.4      1 1943 1943-01-01 00:00:00   bone 2329 #> 4659  28.9      2 1943 1943-01-31 10:00:00  blood 2330 #> 4660  28.9      2 1943 1943-01-31 10:00:00   bone 2330 #> 4661  27.4      3 1943 1943-03-02 20:00:00  blood 2331 #> 4662  27.4      3 1943 1943-03-02 20:00:00   bone 2331 #> 4663  26.1      4 1943 1943-04-02 06:00:00  blood 2332 #> 4664  26.1      4 1943 1943-04-02 06:00:00   bone 2332 #> 4665  14.1      5 1943 1943-05-02 16:00:00  blood 2333 #> 4666  14.1      5 1943 1943-05-02 16:00:00   bone 2333 #> 4667   7.6      6 1943 1943-06-02 02:00:00  blood 2334 #> 4668   7.6      6 1943 1943-06-02 02:00:00   bone 2334 #> 4669  13.2      7 1943 1943-07-02 12:00:00  blood 2335 #> 4670  13.2      7 1943 1943-07-02 12:00:00   bone 2335 #> 4671  19.4      8 1943 1943-08-01 22:00:00  blood 2336 #> 4672  19.4      8 1943 1943-08-01 22:00:00   bone 2336 #> 4673  10.0      9 1943 1943-09-01 08:00:00  blood 2337 #> 4674  10.0      9 1943 1943-09-01 08:00:00   bone 2337 #> 4675   7.8     10 1943 1943-10-01 18:00:00  blood 2338 #> 4676   7.8     10 1943 1943-10-01 18:00:00   bone 2338 #> 4677  10.2     11 1943 1943-11-01 04:00:00  blood 2339 #> 4678  10.2     11 1943 1943-11-01 04:00:00   bone 2339 #> 4679  18.8     12 1943 1943-12-01 14:00:00  blood 2340 #> 4680  18.8     12 1943 1943-12-01 14:00:00   bone 2340 #> 4681   3.7      1 1944 1944-01-01 00:00:00  blood 2341 #> 4682   3.7      1 1944 1944-01-01 00:00:00   bone 2341 #> 4683   0.5      2 1944 1944-01-31 12:00:00  blood 2342 #> 4684   0.5      2 1944 1944-01-31 12:00:00   bone 2342 #> 4685  11.0      3 1944 1944-03-02 00:00:00  blood 2343 #> 4686  11.0      3 1944 1944-03-02 00:00:00   bone 2343 #> 4687   0.3      4 1944 1944-04-01 12:00:00  blood 2344 #> 4688   0.3      4 1944 1944-04-01 12:00:00   bone 2344 #> 4689   2.5      5 1944 1944-05-02 00:00:00  blood 2345 #> 4690   2.5      5 1944 1944-05-02 00:00:00   bone 2345 #> 4691   5.0      6 1944 1944-06-01 12:00:00  blood 2346 #> 4692   5.0      6 1944 1944-06-01 12:00:00   bone 2346 #> 4693   5.0      7 1944 1944-07-02 00:00:00  blood 2347 #> 4694   5.0      7 1944 1944-07-02 00:00:00   bone 2347 #> 4695  16.7      8 1944 1944-08-01 12:00:00  blood 2348 #> 4696  16.7      8 1944 1944-08-01 12:00:00   bone 2348 #> 4697  14.3      9 1944 1944-09-01 00:00:00  blood 2349 #> 4698  14.3      9 1944 1944-09-01 00:00:00   bone 2349 #> 4699  16.9     10 1944 1944-10-01 12:00:00  blood 2350 #> 4700  16.9     10 1944 1944-10-01 12:00:00   bone 2350 #> 4701  10.8     11 1944 1944-11-01 00:00:00  blood 2351 #> 4702  10.8     11 1944 1944-11-01 00:00:00   bone 2351 #> 4703  28.4     12 1944 1944-12-01 12:00:00  blood 2352 #> 4704  28.4     12 1944 1944-12-01 12:00:00   bone 2352 #> 4705  18.5      1 1945 1945-01-01 00:00:00  blood 2353 #> 4706  18.5      1 1945 1945-01-01 00:00:00   bone 2353 #> 4707  12.7      2 1945 1945-01-31 10:00:00  blood 2354 #> 4708  12.7      2 1945 1945-01-31 10:00:00   bone 2354 #> 4709  21.5      3 1945 1945-03-02 20:00:00  blood 2355 #> 4710  21.5      3 1945 1945-03-02 20:00:00   bone 2355 #> 4711  32.0      4 1945 1945-04-02 06:00:00  blood 2356 #> 4712  32.0      4 1945 1945-04-02 06:00:00   bone 2356 #> 4713  30.6      5 1945 1945-05-02 16:00:00  blood 2357 #> 4714  30.6      5 1945 1945-05-02 16:00:00   bone 2357 #> 4715  36.2      6 1945 1945-06-02 02:00:00  blood 2358 #> 4716  36.2      6 1945 1945-06-02 02:00:00   bone 2358 #> 4717  42.6      7 1945 1945-07-02 12:00:00  blood 2359 #> 4718  42.6      7 1945 1945-07-02 12:00:00   bone 2359 #> 4719  25.9      8 1945 1945-08-01 22:00:00  blood 2360 #> 4720  25.9      8 1945 1945-08-01 22:00:00   bone 2360 #> 4721  34.9      9 1945 1945-09-01 08:00:00  blood 2361 #> 4722  34.9      9 1945 1945-09-01 08:00:00   bone 2361 #> 4723  68.8     10 1945 1945-10-01 18:00:00  blood 2362 #> 4724  68.8     10 1945 1945-10-01 18:00:00   bone 2362 #> 4725  46.0     11 1945 1945-11-01 04:00:00  blood 2363 #> 4726  46.0     11 1945 1945-11-01 04:00:00   bone 2363 #> 4727  27.4     12 1945 1945-12-01 14:00:00  blood 2364 #> 4728  27.4     12 1945 1945-12-01 14:00:00   bone 2364 #> 4729  47.6      1 1946 1946-01-01 00:00:00  blood 2365 #> 4730  47.6      1 1946 1946-01-01 00:00:00   bone 2365 #> 4731  86.2      2 1946 1946-01-31 10:00:00  blood 2366 #> 4732  86.2      2 1946 1946-01-31 10:00:00   bone 2366 #> 4733  76.6      3 1946 1946-03-02 20:00:00  blood 2367 #> 4734  76.6      3 1946 1946-03-02 20:00:00   bone 2367 #> 4735  75.7      4 1946 1946-04-02 06:00:00  blood 2368 #> 4736  75.7      4 1946 1946-04-02 06:00:00   bone 2368 #> 4737  84.9      5 1946 1946-05-02 16:00:00  blood 2369 #> 4738  84.9      5 1946 1946-05-02 16:00:00   bone 2369 #> 4739  73.5      6 1946 1946-06-02 02:00:00  blood 2370 #> 4740  73.5      6 1946 1946-06-02 02:00:00   bone 2370 #> 4741 116.2      7 1946 1946-07-02 12:00:00  blood 2371 #> 4742 116.2      7 1946 1946-07-02 12:00:00   bone 2371 #> 4743 107.2      8 1946 1946-08-01 22:00:00  blood 2372 #> 4744 107.2      8 1946 1946-08-01 22:00:00   bone 2372 #> 4745  94.4      9 1946 1946-09-01 08:00:00  blood 2373 #> 4746  94.4      9 1946 1946-09-01 08:00:00   bone 2373 #> 4747 102.3     10 1946 1946-10-01 18:00:00  blood 2374 #> 4748 102.3     10 1946 1946-10-01 18:00:00   bone 2374 #> 4749 123.8     11 1946 1946-11-01 04:00:00  blood 2375 #> 4750 123.8     11 1946 1946-11-01 04:00:00   bone 2375 #> 4751 121.7     12 1946 1946-12-01 14:00:00  blood 2376 #> 4752 121.7     12 1946 1946-12-01 14:00:00   bone 2376 #> 4753 115.7      1 1947 1947-01-01 00:00:00  blood 2377 #> 4754 115.7      1 1947 1947-01-01 00:00:00   bone 2377 #> 4755 113.4      2 1947 1947-01-31 10:00:00  blood 2378 #> 4756 113.4      2 1947 1947-01-31 10:00:00   bone 2378 #> 4757 129.8      3 1947 1947-03-02 20:00:00  blood 2379 #> 4758 129.8      3 1947 1947-03-02 20:00:00   bone 2379 #> 4759 149.8      4 1947 1947-04-02 06:00:00  blood 2380 #> 4760 149.8      4 1947 1947-04-02 06:00:00   bone 2380 #> 4761 201.3      5 1947 1947-05-02 16:00:00  blood 2381 #> 4762 201.3      5 1947 1947-05-02 16:00:00   bone 2381 #> 4763 163.9      6 1947 1947-06-02 02:00:00  blood 2382 #> 4764 163.9      6 1947 1947-06-02 02:00:00   bone 2382 #> 4765 157.9      7 1947 1947-07-02 12:00:00  blood 2383 #> 4766 157.9      7 1947 1947-07-02 12:00:00   bone 2383 #> 4767 188.8      8 1947 1947-08-01 22:00:00  blood 2384 #> 4768 188.8      8 1947 1947-08-01 22:00:00   bone 2384 #> 4769 169.4      9 1947 1947-09-01 08:00:00  blood 2385 #> 4770 169.4      9 1947 1947-09-01 08:00:00   bone 2385 #> 4771 163.6     10 1947 1947-10-01 18:00:00  blood 2386 #> 4772 163.6     10 1947 1947-10-01 18:00:00   bone 2386 #> 4773 128.0     11 1947 1947-11-01 04:00:00  blood 2387 #> 4774 128.0     11 1947 1947-11-01 04:00:00   bone 2387 #> 4775 116.5     12 1947 1947-12-01 14:00:00  blood 2388 #> 4776 116.5     12 1947 1947-12-01 14:00:00   bone 2388 #> 4777 108.5      1 1948 1948-01-01 00:00:00  blood 2389 #> 4778 108.5      1 1948 1948-01-01 00:00:00   bone 2389 #> 4779  86.1      2 1948 1948-01-31 12:00:00  blood 2390 #> 4780  86.1      2 1948 1948-01-31 12:00:00   bone 2390 #> 4781  94.8      3 1948 1948-03-02 00:00:00  blood 2391 #> 4782  94.8      3 1948 1948-03-02 00:00:00   bone 2391 #> 4783 189.7      4 1948 1948-04-01 12:00:00  blood 2392 #> 4784 189.7      4 1948 1948-04-01 12:00:00   bone 2392 #> 4785 174.0      5 1948 1948-05-02 00:00:00  blood 2393 #> 4786 174.0      5 1948 1948-05-02 00:00:00   bone 2393 #> 4787 167.8      6 1948 1948-06-01 12:00:00  blood 2394 #> 4788 167.8      6 1948 1948-06-01 12:00:00   bone 2394 #> 4789 142.2      7 1948 1948-07-02 00:00:00  blood 2395 #> 4790 142.2      7 1948 1948-07-02 00:00:00   bone 2395 #> 4791 157.9      8 1948 1948-08-01 12:00:00  blood 2396 #> 4792 157.9      8 1948 1948-08-01 12:00:00   bone 2396 #> 4793 143.3      9 1948 1948-09-01 00:00:00  blood 2397 #> 4794 143.3      9 1948 1948-09-01 00:00:00   bone 2397 #>  #> $data_test #>         y season year                date series time #> 1   136.3     10 1948 1948-10-01 12:00:00  blood 2398 #> 2   136.3     10 1948 1948-10-01 12:00:00   bone 2398 #> 3    95.8     11 1948 1948-11-01 00:00:00  blood 2399 #> 4    95.8     11 1948 1948-11-01 00:00:00   bone 2399 #> 5   138.0     12 1948 1948-12-01 12:00:00  blood 2400 #> 6   138.0     12 1948 1948-12-01 12:00:00   bone 2400 #> 7   119.1      1 1949 1949-01-01 00:00:00  blood 2401 #> 8   119.1      1 1949 1949-01-01 00:00:00   bone 2401 #> 9   182.3      2 1949 1949-01-31 10:00:00  blood 2402 #> 10  182.3      2 1949 1949-01-31 10:00:00   bone 2402 #> 11  157.5      3 1949 1949-03-02 20:00:00  blood 2403 #> 12  157.5      3 1949 1949-03-02 20:00:00   bone 2403 #> 13  147.0      4 1949 1949-04-02 06:00:00  blood 2404 #> 14  147.0      4 1949 1949-04-02 06:00:00   bone 2404 #> 15  106.2      5 1949 1949-05-02 16:00:00  blood 2405 #> 16  106.2      5 1949 1949-05-02 16:00:00   bone 2405 #> 17  121.7      6 1949 1949-06-02 02:00:00  blood 2406 #> 18  121.7      6 1949 1949-06-02 02:00:00   bone 2406 #> 19  125.8      7 1949 1949-07-02 12:00:00  blood 2407 #> 20  125.8      7 1949 1949-07-02 12:00:00   bone 2407 #> 21  123.8      8 1949 1949-08-01 22:00:00  blood 2408 #> 22  123.8      8 1949 1949-08-01 22:00:00   bone 2408 #> 23  145.3      9 1949 1949-09-01 08:00:00  blood 2409 #> 24  145.3      9 1949 1949-09-01 08:00:00   bone 2409 #> 25  131.6     10 1949 1949-10-01 18:00:00  blood 2410 #> 26  131.6     10 1949 1949-10-01 18:00:00   bone 2410 #> 27  143.5     11 1949 1949-11-01 04:00:00  blood 2411 #> 28  143.5     11 1949 1949-11-01 04:00:00   bone 2411 #> 29  117.6     12 1949 1949-12-01 14:00:00  blood 2412 #> 30  117.6     12 1949 1949-12-01 14:00:00   bone 2412 #> 31  101.6      1 1950 1950-01-01 00:00:00  blood 2413 #> 32  101.6      1 1950 1950-01-01 00:00:00   bone 2413 #> 33   94.8      2 1950 1950-01-31 10:00:00  blood 2414 #> 34   94.8      2 1950 1950-01-31 10:00:00   bone 2414 #> 35  109.7      3 1950 1950-03-02 20:00:00  blood 2415 #> 36  109.7      3 1950 1950-03-02 20:00:00   bone 2415 #> 37  113.4      4 1950 1950-04-02 06:00:00  blood 2416 #> 38  113.4      4 1950 1950-04-02 06:00:00   bone 2416 #> 39  106.2      5 1950 1950-05-02 16:00:00  blood 2417 #> 40  106.2      5 1950 1950-05-02 16:00:00   bone 2417 #> 41   83.6      6 1950 1950-06-02 02:00:00  blood 2418 #> 42   83.6      6 1950 1950-06-02 02:00:00   bone 2418 #> 43   91.0      7 1950 1950-07-02 12:00:00  blood 2419 #> 44   91.0      7 1950 1950-07-02 12:00:00   bone 2419 #> 45   85.2      8 1950 1950-08-01 22:00:00  blood 2420 #> 46   85.2      8 1950 1950-08-01 22:00:00   bone 2420 #> 47   51.3      9 1950 1950-09-01 08:00:00  blood 2421 #> 48   51.3      9 1950 1950-09-01 08:00:00   bone 2421 #> 49   61.4     10 1950 1950-10-01 18:00:00  blood 2422 #> 50   61.4     10 1950 1950-10-01 18:00:00   bone 2422 #> 51   54.8     11 1950 1950-11-01 04:00:00  blood 2423 #> 52   54.8     11 1950 1950-11-01 04:00:00   bone 2423 #> 53   54.1     12 1950 1950-12-01 14:00:00  blood 2424 #> 54   54.1     12 1950 1950-12-01 14:00:00   bone 2424 #> 55   59.9      1 1951 1951-01-01 00:00:00  blood 2425 #> 56   59.9      1 1951 1951-01-01 00:00:00   bone 2425 #> 57   59.9      2 1951 1951-01-31 10:00:00  blood 2426 #> 58   59.9      2 1951 1951-01-31 10:00:00   bone 2426 #> 59   59.9      3 1951 1951-03-02 20:00:00  blood 2427 #> 60   59.9      3 1951 1951-03-02 20:00:00   bone 2427 #> 61   92.9      4 1951 1951-04-02 06:00:00  blood 2428 #> 62   92.9      4 1951 1951-04-02 06:00:00   bone 2428 #> 63  108.5      5 1951 1951-05-02 16:00:00  blood 2429 #> 64  108.5      5 1951 1951-05-02 16:00:00   bone 2429 #> 65  100.6      6 1951 1951-06-02 02:00:00  blood 2430 #> 66  100.6      6 1951 1951-06-02 02:00:00   bone 2430 #> 67   61.5      7 1951 1951-07-02 12:00:00  blood 2431 #> 68   61.5      7 1951 1951-07-02 12:00:00   bone 2431 #> 69   61.0      8 1951 1951-08-01 22:00:00  blood 2432 #> 70   61.0      8 1951 1951-08-01 22:00:00   bone 2432 #> 71   83.1      9 1951 1951-09-01 08:00:00  blood 2433 #> 72   83.1      9 1951 1951-09-01 08:00:00   bone 2433 #> 73   51.6     10 1951 1951-10-01 18:00:00  blood 2434 #> 74   51.6     10 1951 1951-10-01 18:00:00   bone 2434 #> 75   52.4     11 1951 1951-11-01 04:00:00  blood 2435 #> 76   52.4     11 1951 1951-11-01 04:00:00   bone 2435 #> 77   45.8     12 1951 1951-12-01 14:00:00  blood 2436 #> 78   45.8     12 1951 1951-12-01 14:00:00   bone 2436 #> 79   40.7      1 1952 1952-01-01 00:00:00  blood 2437 #> 80   40.7      1 1952 1952-01-01 00:00:00   bone 2437 #> 81   22.7      2 1952 1952-01-31 12:00:00  blood 2438 #> 82   22.7      2 1952 1952-01-31 12:00:00   bone 2438 #> 83   22.0      3 1952 1952-03-02 00:00:00  blood 2439 #> 84   22.0      3 1952 1952-03-02 00:00:00   bone 2439 #> 85   29.1      4 1952 1952-04-01 12:00:00  blood 2440 #> 86   29.1      4 1952 1952-04-01 12:00:00   bone 2440 #> 87   23.4      5 1952 1952-05-02 00:00:00  blood 2441 #> 88   23.4      5 1952 1952-05-02 00:00:00   bone 2441 #> 89   36.4      6 1952 1952-06-01 12:00:00  blood 2442 #> 90   36.4      6 1952 1952-06-01 12:00:00   bone 2442 #> 91   39.3      7 1952 1952-07-02 00:00:00  blood 2443 #> 92   39.3      7 1952 1952-07-02 00:00:00   bone 2443 #> 93   54.9      8 1952 1952-08-01 12:00:00  blood 2444 #> 94   54.9      8 1952 1952-08-01 12:00:00   bone 2444 #> 95   28.2      9 1952 1952-09-01 00:00:00  blood 2445 #> 96   28.2      9 1952 1952-09-01 00:00:00   bone 2445 #> 97   23.8     10 1952 1952-10-01 12:00:00  blood 2446 #> 98   23.8     10 1952 1952-10-01 12:00:00   bone 2446 #> 99   22.1     11 1952 1952-11-01 00:00:00  blood 2447 #> 100  22.1     11 1952 1952-11-01 00:00:00   bone 2447 #> 101  34.3     12 1952 1952-12-01 12:00:00  blood 2448 #> 102  34.3     12 1952 1952-12-01 12:00:00   bone 2448 #> 103  26.5      1 1953 1953-01-01 00:00:00  blood 2449 #> 104  26.5      1 1953 1953-01-01 00:00:00   bone 2449 #> 105   3.9      2 1953 1953-01-31 10:00:00  blood 2450 #> 106   3.9      2 1953 1953-01-31 10:00:00   bone 2450 #> 107  10.0      3 1953 1953-03-02 20:00:00  blood 2451 #> 108  10.0      3 1953 1953-03-02 20:00:00   bone 2451 #> 109  27.8      4 1953 1953-04-02 06:00:00  blood 2452 #> 110  27.8      4 1953 1953-04-02 06:00:00   bone 2452 #> 111  12.5      5 1953 1953-05-02 16:00:00  blood 2453 #> 112  12.5      5 1953 1953-05-02 16:00:00   bone 2453 #> 113  21.8      6 1953 1953-06-02 02:00:00  blood 2454 #> 114  21.8      6 1953 1953-06-02 02:00:00   bone 2454 #> 115   8.6      7 1953 1953-07-02 12:00:00  blood 2455 #> 116   8.6      7 1953 1953-07-02 12:00:00   bone 2455 #> 117  23.5      8 1953 1953-08-01 22:00:00  blood 2456 #> 118  23.5      8 1953 1953-08-01 22:00:00   bone 2456 #> 119  19.3      9 1953 1953-09-01 08:00:00  blood 2457 #> 120  19.3      9 1953 1953-09-01 08:00:00   bone 2457 #> 121   8.2     10 1953 1953-10-01 18:00:00  blood 2458 #> 122   8.2     10 1953 1953-10-01 18:00:00   bone 2458 #> 123   1.6     11 1953 1953-11-01 04:00:00  blood 2459 #> 124   1.6     11 1953 1953-11-01 04:00:00   bone 2459 #> 125   2.5     12 1953 1953-12-01 14:00:00  blood 2460 #> 126   2.5     12 1953 1953-12-01 14:00:00   bone 2460 #> 127   0.2      1 1954 1954-01-01 00:00:00  blood 2461 #> 128   0.2      1 1954 1954-01-01 00:00:00   bone 2461 #> 129   0.5      2 1954 1954-01-31 10:00:00  blood 2462 #> 130   0.5      2 1954 1954-01-31 10:00:00   bone 2462 #> 131  10.9      3 1954 1954-03-02 20:00:00  blood 2463 #> 132  10.9      3 1954 1954-03-02 20:00:00   bone 2463 #> 133   1.8      4 1954 1954-04-02 06:00:00  blood 2464 #> 134   1.8      4 1954 1954-04-02 06:00:00   bone 2464 #> 135   0.8      5 1954 1954-05-02 16:00:00  blood 2465 #> 136   0.8      5 1954 1954-05-02 16:00:00   bone 2465 #> 137   0.2      6 1954 1954-06-02 02:00:00  blood 2466 #> 138   0.2      6 1954 1954-06-02 02:00:00   bone 2466 #> 139   4.8      7 1954 1954-07-02 12:00:00  blood 2467 #> 140   4.8      7 1954 1954-07-02 12:00:00   bone 2467 #> 141   8.4      8 1954 1954-08-01 22:00:00  blood 2468 #> 142   8.4      8 1954 1954-08-01 22:00:00   bone 2468 #> 143   1.5      9 1954 1954-09-01 08:00:00  blood 2469 #> 144   1.5      9 1954 1954-09-01 08:00:00   bone 2469 #> 145   7.0     10 1954 1954-10-01 18:00:00  blood 2470 #> 146   7.0     10 1954 1954-10-01 18:00:00   bone 2470 #> 147   9.2     11 1954 1954-11-01 04:00:00  blood 2471 #> 148   9.2     11 1954 1954-11-01 04:00:00   bone 2471 #> 149   7.6     12 1954 1954-12-01 14:00:00  blood 2472 #> 150   7.6     12 1954 1954-12-01 14:00:00   bone 2472 #> 151  23.1      1 1955 1955-01-01 00:00:00  blood 2473 #> 152  23.1      1 1955 1955-01-01 00:00:00   bone 2473 #> 153  20.8      2 1955 1955-01-31 10:00:00  blood 2474 #> 154  20.8      2 1955 1955-01-31 10:00:00   bone 2474 #> 155   4.9      3 1955 1955-03-02 20:00:00  blood 2475 #> 156   4.9      3 1955 1955-03-02 20:00:00   bone 2475 #> 157  11.3      4 1955 1955-04-02 06:00:00  blood 2476 #> 158  11.3      4 1955 1955-04-02 06:00:00   bone 2476 #> 159  28.9      5 1955 1955-05-02 16:00:00  blood 2477 #> 160  28.9      5 1955 1955-05-02 16:00:00   bone 2477 #> 161  31.7      6 1955 1955-06-02 02:00:00  blood 2478 #> 162  31.7      6 1955 1955-06-02 02:00:00   bone 2478 #> 163  26.7      7 1955 1955-07-02 12:00:00  blood 2479 #> 164  26.7      7 1955 1955-07-02 12:00:00   bone 2479 #> 165  40.7      8 1955 1955-08-01 22:00:00  blood 2480 #> 166  40.7      8 1955 1955-08-01 22:00:00   bone 2480 #> 167  42.7      9 1955 1955-09-01 08:00:00  blood 2481 #> 168  42.7      9 1955 1955-09-01 08:00:00   bone 2481 #> 169  58.5     10 1955 1955-10-01 18:00:00  blood 2482 #> 170  58.5     10 1955 1955-10-01 18:00:00   bone 2482 #> 171  89.2     11 1955 1955-11-01 04:00:00  blood 2483 #> 172  89.2     11 1955 1955-11-01 04:00:00   bone 2483 #> 173  76.9     12 1955 1955-12-01 14:00:00  blood 2484 #> 174  76.9     12 1955 1955-12-01 14:00:00   bone 2484 #> 175  73.6      1 1956 1956-01-01 00:00:00  blood 2485 #> 176  73.6      1 1956 1956-01-01 00:00:00   bone 2485 #> 177 124.0      2 1956 1956-01-31 12:00:00  blood 2486 #> 178 124.0      2 1956 1956-01-31 12:00:00   bone 2486 #> 179 118.4      3 1956 1956-03-02 00:00:00  blood 2487 #> 180 118.4      3 1956 1956-03-02 00:00:00   bone 2487 #> 181 110.7      4 1956 1956-04-01 12:00:00  blood 2488 #> 182 110.7      4 1956 1956-04-01 12:00:00   bone 2488 #> 183 136.6      5 1956 1956-05-02 00:00:00  blood 2489 #> 184 136.6      5 1956 1956-05-02 00:00:00   bone 2489 #> 185 116.6      6 1956 1956-06-01 12:00:00  blood 2490 #> 186 116.6      6 1956 1956-06-01 12:00:00   bone 2490 #> 187 129.1      7 1956 1956-07-02 00:00:00  blood 2491 #> 188 129.1      7 1956 1956-07-02 00:00:00   bone 2491 #> 189 169.6      8 1956 1956-08-01 12:00:00  blood 2492 #> 190 169.6      8 1956 1956-08-01 12:00:00   bone 2492 #> 191 173.2      9 1956 1956-09-01 00:00:00  blood 2493 #> 192 173.2      9 1956 1956-09-01 00:00:00   bone 2493 #> 193 155.3     10 1956 1956-10-01 12:00:00  blood 2494 #> 194 155.3     10 1956 1956-10-01 12:00:00   bone 2494 #> 195 201.3     11 1956 1956-11-01 00:00:00  blood 2495 #> 196 201.3     11 1956 1956-11-01 00:00:00   bone 2495 #> 197 192.1     12 1956 1956-12-01 12:00:00  blood 2496 #> 198 192.1     12 1956 1956-12-01 12:00:00   bone 2496 #> 199 165.0      1 1957 1957-01-01 00:00:00  blood 2497 #> 200 165.0      1 1957 1957-01-01 00:00:00   bone 2497 #> 201 130.2      2 1957 1957-01-31 10:00:00  blood 2498 #> 202 130.2      2 1957 1957-01-31 10:00:00   bone 2498 #> 203 157.4      3 1957 1957-03-02 20:00:00  blood 2499 #> 204 157.4      3 1957 1957-03-02 20:00:00   bone 2499 #> 205 175.2      4 1957 1957-04-02 06:00:00  blood 2500 #> 206 175.2      4 1957 1957-04-02 06:00:00   bone 2500 #> 207 164.6      5 1957 1957-05-02 16:00:00  blood 2501 #> 208 164.6      5 1957 1957-05-02 16:00:00   bone 2501 #> 209 200.7      6 1957 1957-06-02 02:00:00  blood 2502 #> 210 200.7      6 1957 1957-06-02 02:00:00   bone 2502 #> 211 187.2      7 1957 1957-07-02 12:00:00  blood 2503 #> 212 187.2      7 1957 1957-07-02 12:00:00   bone 2503 #> 213 158.0      8 1957 1957-08-01 22:00:00  blood 2504 #> 214 158.0      8 1957 1957-08-01 22:00:00   bone 2504 #> 215 235.8      9 1957 1957-09-01 08:00:00  blood 2505 #> 216 235.8      9 1957 1957-09-01 08:00:00   bone 2505 #> 217 253.8     10 1957 1957-10-01 18:00:00  blood 2506 #> 218 253.8     10 1957 1957-10-01 18:00:00   bone 2506 #> 219 210.9     11 1957 1957-11-01 04:00:00  blood 2507 #> 220 210.9     11 1957 1957-11-01 04:00:00   bone 2507 #> 221 239.4     12 1957 1957-12-01 14:00:00  blood 2508 #> 222 239.4     12 1957 1957-12-01 14:00:00   bone 2508 #> 223 202.5      1 1958 1958-01-01 00:00:00  blood 2509 #> 224 202.5      1 1958 1958-01-01 00:00:00   bone 2509 #> 225 164.9      2 1958 1958-01-31 10:00:00  blood 2510 #> 226 164.9      2 1958 1958-01-31 10:00:00   bone 2510 #> 227 190.7      3 1958 1958-03-02 20:00:00  blood 2511 #> 228 190.7      3 1958 1958-03-02 20:00:00   bone 2511 #> 229 196.0      4 1958 1958-04-02 06:00:00  blood 2512 #> 230 196.0      4 1958 1958-04-02 06:00:00   bone 2512 #> 231 175.3      5 1958 1958-05-02 16:00:00  blood 2513 #> 232 175.3      5 1958 1958-05-02 16:00:00   bone 2513 #> 233 171.5      6 1958 1958-06-02 02:00:00  blood 2514 #> 234 171.5      6 1958 1958-06-02 02:00:00   bone 2514 #> 235 191.4      7 1958 1958-07-02 12:00:00  blood 2515 #> 236 191.4      7 1958 1958-07-02 12:00:00   bone 2515 #> 237 200.2      8 1958 1958-08-01 22:00:00  blood 2516 #> 238 200.2      8 1958 1958-08-01 22:00:00   bone 2516 #> 239 201.2      9 1958 1958-09-01 08:00:00  blood 2517 #> 240 201.2      9 1958 1958-09-01 08:00:00   bone 2517 #> 241 181.5     10 1958 1958-10-01 18:00:00  blood 2518 #> 242 181.5     10 1958 1958-10-01 18:00:00   bone 2518 #> 243 152.3     11 1958 1958-11-01 04:00:00  blood 2519 #> 244 152.3     11 1958 1958-11-01 04:00:00   bone 2519 #> 245 187.6     12 1958 1958-12-01 14:00:00  blood 2520 #> 246 187.6     12 1958 1958-12-01 14:00:00   bone 2520 #> 247 217.4      1 1959 1959-01-01 00:00:00  blood 2521 #> 248 217.4      1 1959 1959-01-01 00:00:00   bone 2521 #> 249 143.1      2 1959 1959-01-31 10:00:00  blood 2522 #> 250 143.1      2 1959 1959-01-31 10:00:00   bone 2522 #> 251 185.7      3 1959 1959-03-02 20:00:00  blood 2523 #> 252 185.7      3 1959 1959-03-02 20:00:00   bone 2523 #> 253 163.3      4 1959 1959-04-02 06:00:00  blood 2524 #> 254 163.3      4 1959 1959-04-02 06:00:00   bone 2524 #> 255 172.0      5 1959 1959-05-02 16:00:00  blood 2525 #> 256 172.0      5 1959 1959-05-02 16:00:00   bone 2525 #> 257 168.7      6 1959 1959-06-02 02:00:00  blood 2526 #> 258 168.7      6 1959 1959-06-02 02:00:00   bone 2526 #> 259 149.6      7 1959 1959-07-02 12:00:00  blood 2527 #> 260 149.6      7 1959 1959-07-02 12:00:00   bone 2527 #> 261 199.6      8 1959 1959-08-01 22:00:00  blood 2528 #> 262 199.6      8 1959 1959-08-01 22:00:00   bone 2528 #> 263 145.2      9 1959 1959-09-01 08:00:00  blood 2529 #> 264 145.2      9 1959 1959-09-01 08:00:00   bone 2529 #> 265 111.4     10 1959 1959-10-01 18:00:00  blood 2530 #> 266 111.4     10 1959 1959-10-01 18:00:00   bone 2530 #> 267 124.0     11 1959 1959-11-01 04:00:00  blood 2531 #> 268 124.0     11 1959 1959-11-01 04:00:00   bone 2531 #> 269 125.0     12 1959 1959-12-01 14:00:00  blood 2532 #> 270 125.0     12 1959 1959-12-01 14:00:00   bone 2532 #> 271 146.3      1 1960 1960-01-01 00:00:00  blood 2533 #> 272 146.3      1 1960 1960-01-01 00:00:00   bone 2533 #> 273 106.0      2 1960 1960-01-31 12:00:00  blood 2534 #> 274 106.0      2 1960 1960-01-31 12:00:00   bone 2534 #> 275 102.2      3 1960 1960-03-02 00:00:00  blood 2535 #> 276 102.2      3 1960 1960-03-02 00:00:00   bone 2535 #> 277 122.0      4 1960 1960-04-01 12:00:00  blood 2536 #> 278 122.0      4 1960 1960-04-01 12:00:00   bone 2536 #> 279 119.6      5 1960 1960-05-02 00:00:00  blood 2537 #> 280 119.6      5 1960 1960-05-02 00:00:00   bone 2537 #> 281 110.2      6 1960 1960-06-01 12:00:00  blood 2538 #> 282 110.2      6 1960 1960-06-01 12:00:00   bone 2538 #> 283 121.7      7 1960 1960-07-02 00:00:00  blood 2539 #> 284 121.7      7 1960 1960-07-02 00:00:00   bone 2539 #> 285 134.1      8 1960 1960-08-01 12:00:00  blood 2540 #> 286 134.1      8 1960 1960-08-01 12:00:00   bone 2540 #> 287 127.2      9 1960 1960-09-01 00:00:00  blood 2541 #> 288 127.2      9 1960 1960-09-01 00:00:00   bone 2541 #> 289  82.8     10 1960 1960-10-01 12:00:00  blood 2542 #> 290  82.8     10 1960 1960-10-01 12:00:00   bone 2542 #> 291  89.6     11 1960 1960-11-01 00:00:00  blood 2543 #> 292  89.6     11 1960 1960-11-01 00:00:00   bone 2543 #> 293  85.6     12 1960 1960-12-01 12:00:00  blood 2544 #> 294  85.6     12 1960 1960-12-01 12:00:00   bone 2544 #> 295  57.9      1 1961 1961-01-01 00:00:00  blood 2545 #> 296  57.9      1 1961 1961-01-01 00:00:00   bone 2545 #> 297  46.1      2 1961 1961-01-31 10:00:00  blood 2546 #> 298  46.1      2 1961 1961-01-31 10:00:00   bone 2546 #> 299  53.0      3 1961 1961-03-02 20:00:00  blood 2547 #> 300  53.0      3 1961 1961-03-02 20:00:00   bone 2547 #> 301  61.4      4 1961 1961-04-02 06:00:00  blood 2548 #> 302  61.4      4 1961 1961-04-02 06:00:00   bone 2548 #> 303  51.0      5 1961 1961-05-02 16:00:00  blood 2549 #> 304  51.0      5 1961 1961-05-02 16:00:00   bone 2549 #> 305  77.4      6 1961 1961-06-02 02:00:00  blood 2550 #> 306  77.4      6 1961 1961-06-02 02:00:00   bone 2550 #> 307  70.2      7 1961 1961-07-02 12:00:00  blood 2551 #> 308  70.2      7 1961 1961-07-02 12:00:00   bone 2551 #> 309  55.9      8 1961 1961-08-01 22:00:00  blood 2552 #> 310  55.9      8 1961 1961-08-01 22:00:00   bone 2552 #> 311  63.6      9 1961 1961-09-01 08:00:00  blood 2553 #> 312  63.6      9 1961 1961-09-01 08:00:00   bone 2553 #> 313  37.7     10 1961 1961-10-01 18:00:00  blood 2554 #> 314  37.7     10 1961 1961-10-01 18:00:00   bone 2554 #> 315  32.6     11 1961 1961-11-01 04:00:00  blood 2555 #> 316  32.6     11 1961 1961-11-01 04:00:00   bone 2555 #> 317  40.0     12 1961 1961-12-01 14:00:00  blood 2556 #> 318  40.0     12 1961 1961-12-01 14:00:00   bone 2556 #> 319  38.7      1 1962 1962-01-01 00:00:00  blood 2557 #> 320  38.7      1 1962 1962-01-01 00:00:00   bone 2557 #> 321  50.3      2 1962 1962-01-31 10:00:00  blood 2558 #> 322  50.3      2 1962 1962-01-31 10:00:00   bone 2558 #> 323  45.6      3 1962 1962-03-02 20:00:00  blood 2559 #> 324  45.6      3 1962 1962-03-02 20:00:00   bone 2559 #> 325  46.4      4 1962 1962-04-02 06:00:00  blood 2560 #> 326  46.4      4 1962 1962-04-02 06:00:00   bone 2560 #> 327  43.7      5 1962 1962-05-02 16:00:00  blood 2561 #> 328  43.7      5 1962 1962-05-02 16:00:00   bone 2561 #> 329  42.0      6 1962 1962-06-02 02:00:00  blood 2562 #> 330  42.0      6 1962 1962-06-02 02:00:00   bone 2562 #> 331  21.8      7 1962 1962-07-02 12:00:00  blood 2563 #> 332  21.8      7 1962 1962-07-02 12:00:00   bone 2563 #> 333  21.8      8 1962 1962-08-01 22:00:00  blood 2564 #> 334  21.8      8 1962 1962-08-01 22:00:00   bone 2564 #> 335  51.3      9 1962 1962-09-01 08:00:00  blood 2565 #> 336  51.3      9 1962 1962-09-01 08:00:00   bone 2565 #> 337  39.5     10 1962 1962-10-01 18:00:00  blood 2566 #> 338  39.5     10 1962 1962-10-01 18:00:00   bone 2566 #> 339  26.9     11 1962 1962-11-01 04:00:00  blood 2567 #> 340  26.9     11 1962 1962-11-01 04:00:00   bone 2567 #> 341  23.2     12 1962 1962-12-01 14:00:00  blood 2568 #> 342  23.2     12 1962 1962-12-01 14:00:00   bone 2568 #> 343  19.8      1 1963 1963-01-01 00:00:00  blood 2569 #> 344  19.8      1 1963 1963-01-01 00:00:00   bone 2569 #> 345  24.4      2 1963 1963-01-31 10:00:00  blood 2570 #> 346  24.4      2 1963 1963-01-31 10:00:00   bone 2570 #> 347  17.1      3 1963 1963-03-02 20:00:00  blood 2571 #> 348  17.1      3 1963 1963-03-02 20:00:00   bone 2571 #> 349  29.3      4 1963 1963-04-02 06:00:00  blood 2572 #> 350  29.3      4 1963 1963-04-02 06:00:00   bone 2572 #> 351  43.0      5 1963 1963-05-02 16:00:00  blood 2573 #> 352  43.0      5 1963 1963-05-02 16:00:00   bone 2573 #> 353  35.9      6 1963 1963-06-02 02:00:00  blood 2574 #> 354  35.9      6 1963 1963-06-02 02:00:00   bone 2574 #> 355  19.6      7 1963 1963-07-02 12:00:00  blood 2575 #> 356  19.6      7 1963 1963-07-02 12:00:00   bone 2575 #> 357  33.2      8 1963 1963-08-01 22:00:00  blood 2576 #> 358  33.2      8 1963 1963-08-01 22:00:00   bone 2576 #> 359  38.8      9 1963 1963-09-01 08:00:00  blood 2577 #> 360  38.8      9 1963 1963-09-01 08:00:00   bone 2577 #> 361  35.3     10 1963 1963-10-01 18:00:00  blood 2578 #> 362  35.3     10 1963 1963-10-01 18:00:00   bone 2578 #> 363  23.4     11 1963 1963-11-01 04:00:00  blood 2579 #> 364  23.4     11 1963 1963-11-01 04:00:00   bone 2579 #> 365  14.9     12 1963 1963-12-01 14:00:00  blood 2580 #> 366  14.9     12 1963 1963-12-01 14:00:00   bone 2580 #> 367  15.3      1 1964 1964-01-01 00:00:00  blood 2581 #> 368  15.3      1 1964 1964-01-01 00:00:00   bone 2581 #> 369  17.7      2 1964 1964-01-31 12:00:00  blood 2582 #> 370  17.7      2 1964 1964-01-31 12:00:00   bone 2582 #> 371  16.5      3 1964 1964-03-02 00:00:00  blood 2583 #> 372  16.5      3 1964 1964-03-02 00:00:00   bone 2583 #> 373   8.6      4 1964 1964-04-01 12:00:00  blood 2584 #> 374   8.6      4 1964 1964-04-01 12:00:00   bone 2584 #> 375   9.5      5 1964 1964-05-02 00:00:00  blood 2585 #> 376   9.5      5 1964 1964-05-02 00:00:00   bone 2585 #> 377   9.1      6 1964 1964-06-01 12:00:00  blood 2586 #> 378   9.1      6 1964 1964-06-01 12:00:00   bone 2586 #> 379   3.1      7 1964 1964-07-02 00:00:00  blood 2587 #> 380   3.1      7 1964 1964-07-02 00:00:00   bone 2587 #> 381   9.3      8 1964 1964-08-01 12:00:00  blood 2588 #> 382   9.3      8 1964 1964-08-01 12:00:00   bone 2588 #> 383   4.7      9 1964 1964-09-01 00:00:00  blood 2589 #> 384   4.7      9 1964 1964-09-01 00:00:00   bone 2589 #> 385   6.1     10 1964 1964-10-01 12:00:00  blood 2590 #> 386   6.1     10 1964 1964-10-01 12:00:00   bone 2590 #> 387   7.4     11 1964 1964-11-01 00:00:00  blood 2591 #> 388   7.4     11 1964 1964-11-01 00:00:00   bone 2591 #> 389  15.1     12 1964 1964-12-01 12:00:00  blood 2592 #> 390  15.1     12 1964 1964-12-01 12:00:00   bone 2592 #> 391  17.5      1 1965 1965-01-01 00:00:00  blood 2593 #> 392  17.5      1 1965 1965-01-01 00:00:00   bone 2593 #> 393  14.2      2 1965 1965-01-31 10:00:00  blood 2594 #> 394  14.2      2 1965 1965-01-31 10:00:00   bone 2594 #> 395  11.7      3 1965 1965-03-02 20:00:00  blood 2595 #> 396  11.7      3 1965 1965-03-02 20:00:00   bone 2595 #> 397   6.8      4 1965 1965-04-02 06:00:00  blood 2596 #> 398   6.8      4 1965 1965-04-02 06:00:00   bone 2596 #> 399  24.1      5 1965 1965-05-02 16:00:00  blood 2597 #> 400  24.1      5 1965 1965-05-02 16:00:00   bone 2597 #> 401  15.9      6 1965 1965-06-02 02:00:00  blood 2598 #> 402  15.9      6 1965 1965-06-02 02:00:00   bone 2598 #> 403  11.9      7 1965 1965-07-02 12:00:00  blood 2599 #> 404  11.9      7 1965 1965-07-02 12:00:00   bone 2599 #> 405   8.9      8 1965 1965-08-01 22:00:00  blood 2600 #> 406   8.9      8 1965 1965-08-01 22:00:00   bone 2600 #> 407  16.8      9 1965 1965-09-01 08:00:00  blood 2601 #> 408  16.8      9 1965 1965-09-01 08:00:00   bone 2601 #> 409  20.1     10 1965 1965-10-01 18:00:00  blood 2602 #> 410  20.1     10 1965 1965-10-01 18:00:00   bone 2602 #> 411  15.8     11 1965 1965-11-01 04:00:00  blood 2603 #> 412  15.8     11 1965 1965-11-01 04:00:00   bone 2603 #> 413  17.0     12 1965 1965-12-01 14:00:00  blood 2604 #> 414  17.0     12 1965 1965-12-01 14:00:00   bone 2604 #> 415  28.2      1 1966 1966-01-01 00:00:00  blood 2605 #> 416  28.2      1 1966 1966-01-01 00:00:00   bone 2605 #> 417  24.4      2 1966 1966-01-31 10:00:00  blood 2606 #> 418  24.4      2 1966 1966-01-31 10:00:00   bone 2606 #> 419  25.3      3 1966 1966-03-02 20:00:00  blood 2607 #> 420  25.3      3 1966 1966-03-02 20:00:00   bone 2607 #> 421  48.7      4 1966 1966-04-02 06:00:00  blood 2608 #> 422  48.7      4 1966 1966-04-02 06:00:00   bone 2608 #> 423  45.3      5 1966 1966-05-02 16:00:00  blood 2609 #> 424  45.3      5 1966 1966-05-02 16:00:00   bone 2609 #> 425  47.7      6 1966 1966-06-02 02:00:00  blood 2610 #> 426  47.7      6 1966 1966-06-02 02:00:00   bone 2610 #> 427  56.7      7 1966 1966-07-02 12:00:00  blood 2611 #> 428  56.7      7 1966 1966-07-02 12:00:00   bone 2611 #> 429  51.2      8 1966 1966-08-01 22:00:00  blood 2612 #> 430  51.2      8 1966 1966-08-01 22:00:00   bone 2612 #> 431  50.2      9 1966 1966-09-01 08:00:00  blood 2613 #> 432  50.2      9 1966 1966-09-01 08:00:00   bone 2613 #> 433  57.2     10 1966 1966-10-01 18:00:00  blood 2614 #> 434  57.2     10 1966 1966-10-01 18:00:00   bone 2614 #> 435  57.2     11 1966 1966-11-01 04:00:00  blood 2615 #> 436  57.2     11 1966 1966-11-01 04:00:00   bone 2615 #> 437  70.4     12 1966 1966-12-01 14:00:00  blood 2616 #> 438  70.4     12 1966 1966-12-01 14:00:00   bone 2616 #> 439 110.9      1 1967 1967-01-01 00:00:00  blood 2617 #> 440 110.9      1 1967 1967-01-01 00:00:00   bone 2617 #> 441  93.6      2 1967 1967-01-31 10:00:00  blood 2618 #> 442  93.6      2 1967 1967-01-31 10:00:00   bone 2618 #> 443 111.8      3 1967 1967-03-02 20:00:00  blood 2619 #> 444 111.8      3 1967 1967-03-02 20:00:00   bone 2619 #> 445  69.5      4 1967 1967-04-02 06:00:00  blood 2620 #> 446  69.5      4 1967 1967-04-02 06:00:00   bone 2620 #> 447  86.5      5 1967 1967-05-02 16:00:00  blood 2621 #> 448  86.5      5 1967 1967-05-02 16:00:00   bone 2621 #> 449  67.3      6 1967 1967-06-02 02:00:00  blood 2622 #> 450  67.3      6 1967 1967-06-02 02:00:00   bone 2622 #> 451  91.5      7 1967 1967-07-02 12:00:00  blood 2623 #> 452  91.5      7 1967 1967-07-02 12:00:00   bone 2623 #> 453 107.2      8 1967 1967-08-01 22:00:00  blood 2624 #> 454 107.2      8 1967 1967-08-01 22:00:00   bone 2624 #> 455  76.8      9 1967 1967-09-01 08:00:00  blood 2625 #> 456  76.8      9 1967 1967-09-01 08:00:00   bone 2625 #> 457  88.2     10 1967 1967-10-01 18:00:00  blood 2626 #> 458  88.2     10 1967 1967-10-01 18:00:00   bone 2626 #> 459  94.3     11 1967 1967-11-01 04:00:00  blood 2627 #> 460  94.3     11 1967 1967-11-01 04:00:00   bone 2627 #> 461 126.4     12 1967 1967-12-01 14:00:00  blood 2628 #> 462 126.4     12 1967 1967-12-01 14:00:00   bone 2628 #> 463 121.8      1 1968 1968-01-01 00:00:00  blood 2629 #> 464 121.8      1 1968 1968-01-01 00:00:00   bone 2629 #> 465 111.9      2 1968 1968-01-31 12:00:00  blood 2630 #> 466 111.9      2 1968 1968-01-31 12:00:00   bone 2630 #> 467  92.2      3 1968 1968-03-02 00:00:00  blood 2631 #> 468  92.2      3 1968 1968-03-02 00:00:00   bone 2631 #> 469  81.2      4 1968 1968-04-01 12:00:00  blood 2632 #> 470  81.2      4 1968 1968-04-01 12:00:00   bone 2632 #> 471 127.2      5 1968 1968-05-02 00:00:00  blood 2633 #> 472 127.2      5 1968 1968-05-02 00:00:00   bone 2633 #> 473 110.3      6 1968 1968-06-01 12:00:00  blood 2634 #> 474 110.3      6 1968 1968-06-01 12:00:00   bone 2634 #> 475  96.1      7 1968 1968-07-02 00:00:00  blood 2635 #> 476  96.1      7 1968 1968-07-02 00:00:00   bone 2635 #> 477 109.3      8 1968 1968-08-01 12:00:00  blood 2636 #> 478 109.3      8 1968 1968-08-01 12:00:00   bone 2636 #> 479 117.2      9 1968 1968-09-01 00:00:00  blood 2637 #> 480 117.2      9 1968 1968-09-01 00:00:00   bone 2637 #> 481 107.7     10 1968 1968-10-01 12:00:00  blood 2638 #> 482 107.7     10 1968 1968-10-01 12:00:00   bone 2638 #> 483  86.0     11 1968 1968-11-01 00:00:00  blood 2639 #> 484  86.0     11 1968 1968-11-01 00:00:00   bone 2639 #> 485 109.8     12 1968 1968-12-01 12:00:00  blood 2640 #> 486 109.8     12 1968 1968-12-01 12:00:00   bone 2640 #> 487 104.4      1 1969 1969-01-01 00:00:00  blood 2641 #> 488 104.4      1 1969 1969-01-01 00:00:00   bone 2641 #> 489 120.5      2 1969 1969-01-31 10:00:00  blood 2642 #> 490 120.5      2 1969 1969-01-31 10:00:00   bone 2642 #> 491 135.8      3 1969 1969-03-02 20:00:00  blood 2643 #> 492 135.8      3 1969 1969-03-02 20:00:00   bone 2643 #> 493 106.8      4 1969 1969-04-02 06:00:00  blood 2644 #> 494 106.8      4 1969 1969-04-02 06:00:00   bone 2644 #> 495 120.0      5 1969 1969-05-02 16:00:00  blood 2645 #> 496 120.0      5 1969 1969-05-02 16:00:00   bone 2645 #> 497 106.0      6 1969 1969-06-02 02:00:00  blood 2646 #> 498 106.0      6 1969 1969-06-02 02:00:00   bone 2646 #> 499  96.8      7 1969 1969-07-02 12:00:00  blood 2647 #> 500  96.8      7 1969 1969-07-02 12:00:00   bone 2647 #> 501  98.0      8 1969 1969-08-01 22:00:00  blood 2648 #> 502  98.0      8 1969 1969-08-01 22:00:00   bone 2648 #> 503  91.3      9 1969 1969-09-01 08:00:00  blood 2649 #> 504  91.3      9 1969 1969-09-01 08:00:00   bone 2649 #> 505  95.7     10 1969 1969-10-01 18:00:00  blood 2650 #> 506  95.7     10 1969 1969-10-01 18:00:00   bone 2650 #> 507  93.5     11 1969 1969-11-01 04:00:00  blood 2651 #> 508  93.5     11 1969 1969-11-01 04:00:00   bone 2651 #> 509  97.9     12 1969 1969-12-01 14:00:00  blood 2652 #> 510  97.9     12 1969 1969-12-01 14:00:00   bone 2652 #> 511 111.5      1 1970 1970-01-01 00:00:00  blood 2653 #> 512 111.5      1 1970 1970-01-01 00:00:00   bone 2653 #> 513 127.8      2 1970 1970-01-31 10:00:00  blood 2654 #> 514 127.8      2 1970 1970-01-31 10:00:00   bone 2654 #> 515 102.9      3 1970 1970-03-02 20:00:00  blood 2655 #> 516 102.9      3 1970 1970-03-02 20:00:00   bone 2655 #> 517 109.5      4 1970 1970-04-02 06:00:00  blood 2656 #> 518 109.5      4 1970 1970-04-02 06:00:00   bone 2656 #> 519 127.5      5 1970 1970-05-02 16:00:00  blood 2657 #> 520 127.5      5 1970 1970-05-02 16:00:00   bone 2657 #> 521 106.8      6 1970 1970-06-02 02:00:00  blood 2658 #> 522 106.8      6 1970 1970-06-02 02:00:00   bone 2658 #> 523 112.5      7 1970 1970-07-02 12:00:00  blood 2659 #> 524 112.5      7 1970 1970-07-02 12:00:00   bone 2659 #> 525  93.0      8 1970 1970-08-01 22:00:00  blood 2660 #> 526  93.0      8 1970 1970-08-01 22:00:00   bone 2660 #> 527  99.5      9 1970 1970-09-01 08:00:00  blood 2661 #> 528  99.5      9 1970 1970-09-01 08:00:00   bone 2661 #> 529  86.6     10 1970 1970-10-01 18:00:00  blood 2662 #> 530  86.6     10 1970 1970-10-01 18:00:00   bone 2662 #> 531  95.2     11 1970 1970-11-01 04:00:00  blood 2663 #> 532  95.2     11 1970 1970-11-01 04:00:00   bone 2663 #> 533  83.5     12 1970 1970-12-01 14:00:00  blood 2664 #> 534  83.5     12 1970 1970-12-01 14:00:00   bone 2664 #> 535  91.3      1 1971 1971-01-01 00:00:00  blood 2665 #> 536  91.3      1 1971 1971-01-01 00:00:00   bone 2665 #> 537  79.0      2 1971 1971-01-31 10:00:00  blood 2666 #> 538  79.0      2 1971 1971-01-31 10:00:00   bone 2666 #> 539  60.7      3 1971 1971-03-02 20:00:00  blood 2667 #> 540  60.7      3 1971 1971-03-02 20:00:00   bone 2667 #> 541  71.8      4 1971 1971-04-02 06:00:00  blood 2668 #> 542  71.8      4 1971 1971-04-02 06:00:00   bone 2668 #> 543  57.5      5 1971 1971-05-02 16:00:00  blood 2669 #> 544  57.5      5 1971 1971-05-02 16:00:00   bone 2669 #> 545  49.8      6 1971 1971-06-02 02:00:00  blood 2670 #> 546  49.8      6 1971 1971-06-02 02:00:00   bone 2670 #> 547  81.0      7 1971 1971-07-02 12:00:00  blood 2671 #> 548  81.0      7 1971 1971-07-02 12:00:00   bone 2671 #> 549  61.4      8 1971 1971-08-01 22:00:00  blood 2672 #> 550  61.4      8 1971 1971-08-01 22:00:00   bone 2672 #> 551  50.2      9 1971 1971-09-01 08:00:00  blood 2673 #> 552  50.2      9 1971 1971-09-01 08:00:00   bone 2673 #> 553  51.7     10 1971 1971-10-01 18:00:00  blood 2674 #> 554  51.7     10 1971 1971-10-01 18:00:00   bone 2674 #> 555  63.2     11 1971 1971-11-01 04:00:00  blood 2675 #> 556  63.2     11 1971 1971-11-01 04:00:00   bone 2675 #> 557  82.2     12 1971 1971-12-01 14:00:00  blood 2676 #> 558  82.2     12 1971 1971-12-01 14:00:00   bone 2676 #> 559  61.5      1 1972 1972-01-01 00:00:00  blood 2677 #> 560  61.5      1 1972 1972-01-01 00:00:00   bone 2677 #> 561  88.4      2 1972 1972-01-31 12:00:00  blood 2678 #> 562  88.4      2 1972 1972-01-31 12:00:00   bone 2678 #> 563  80.1      3 1972 1972-03-02 00:00:00  blood 2679 #> 564  80.1      3 1972 1972-03-02 00:00:00   bone 2679 #> 565  63.2      4 1972 1972-04-01 12:00:00  blood 2680 #> 566  63.2      4 1972 1972-04-01 12:00:00   bone 2680 #> 567  80.5      5 1972 1972-05-02 00:00:00  blood 2681 #> 568  80.5      5 1972 1972-05-02 00:00:00   bone 2681 #> 569  88.0      6 1972 1972-06-01 12:00:00  blood 2682 #> 570  88.0      6 1972 1972-06-01 12:00:00   bone 2682 #> 571  76.5      7 1972 1972-07-02 00:00:00  blood 2683 #> 572  76.5      7 1972 1972-07-02 00:00:00   bone 2683 #> 573  76.8      8 1972 1972-08-01 12:00:00  blood 2684 #> 574  76.8      8 1972 1972-08-01 12:00:00   bone 2684 #> 575  64.0      9 1972 1972-09-01 00:00:00  blood 2685 #> 576  64.0      9 1972 1972-09-01 00:00:00   bone 2685 #> 577  61.3     10 1972 1972-10-01 12:00:00  blood 2686 #> 578  61.3     10 1972 1972-10-01 12:00:00   bone 2686 #> 579  41.6     11 1972 1972-11-01 00:00:00  blood 2687 #> 580  41.6     11 1972 1972-11-01 00:00:00   bone 2687 #> 581  45.3     12 1972 1972-12-01 12:00:00  blood 2688 #> 582  45.3     12 1972 1972-12-01 12:00:00   bone 2688 #> 583  43.4      1 1973 1973-01-01 00:00:00  blood 2689 #> 584  43.4      1 1973 1973-01-01 00:00:00   bone 2689 #> 585  42.9      2 1973 1973-01-31 10:00:00  blood 2690 #> 586  42.9      2 1973 1973-01-31 10:00:00   bone 2690 #> 587  46.0      3 1973 1973-03-02 20:00:00  blood 2691 #> 588  46.0      3 1973 1973-03-02 20:00:00   bone 2691 #> 589  57.7      4 1973 1973-04-02 06:00:00  blood 2692 #> 590  57.7      4 1973 1973-04-02 06:00:00   bone 2692 #> 591  42.4      5 1973 1973-05-02 16:00:00  blood 2693 #> 592  42.4      5 1973 1973-05-02 16:00:00   bone 2693 #> 593  39.5      6 1973 1973-06-02 02:00:00  blood 2694 #> 594  39.5      6 1973 1973-06-02 02:00:00   bone 2694 #> 595  23.1      7 1973 1973-07-02 12:00:00  blood 2695 #> 596  23.1      7 1973 1973-07-02 12:00:00   bone 2695 #> 597  25.6      8 1973 1973-08-01 22:00:00  blood 2696 #> 598  25.6      8 1973 1973-08-01 22:00:00   bone 2696 #> 599  59.3      9 1973 1973-09-01 08:00:00  blood 2697 #> 600  59.3      9 1973 1973-09-01 08:00:00   bone 2697 #> 601  30.7     10 1973 1973-10-01 18:00:00  blood 2698 #> 602  30.7     10 1973 1973-10-01 18:00:00   bone 2698 #> 603  23.9     11 1973 1973-11-01 04:00:00  blood 2699 #> 604  23.9     11 1973 1973-11-01 04:00:00   bone 2699 #> 605  23.3     12 1973 1973-12-01 14:00:00  blood 2700 #> 606  23.3     12 1973 1973-12-01 14:00:00   bone 2700 #> 607  27.6      1 1974 1974-01-01 00:00:00  blood 2701 #> 608  27.6      1 1974 1974-01-01 00:00:00   bone 2701 #> 609  26.0      2 1974 1974-01-31 10:00:00  blood 2702 #> 610  26.0      2 1974 1974-01-31 10:00:00   bone 2702 #> 611  21.3      3 1974 1974-03-02 20:00:00  blood 2703 #> 612  21.3      3 1974 1974-03-02 20:00:00   bone 2703 #> 613  40.3      4 1974 1974-04-02 06:00:00  blood 2704 #> 614  40.3      4 1974 1974-04-02 06:00:00   bone 2704 #> 615  39.5      5 1974 1974-05-02 16:00:00  blood 2705 #> 616  39.5      5 1974 1974-05-02 16:00:00   bone 2705 #> 617  36.0      6 1974 1974-06-02 02:00:00  blood 2706 #> 618  36.0      6 1974 1974-06-02 02:00:00   bone 2706 #> 619  55.8      7 1974 1974-07-02 12:00:00  blood 2707 #> 620  55.8      7 1974 1974-07-02 12:00:00   bone 2707 #> 621  33.6      8 1974 1974-08-01 22:00:00  blood 2708 #> 622  33.6      8 1974 1974-08-01 22:00:00   bone 2708 #> 623  40.2      9 1974 1974-09-01 08:00:00  blood 2709 #> 624  40.2      9 1974 1974-09-01 08:00:00   bone 2709 #> 625  47.1     10 1974 1974-10-01 18:00:00  blood 2710 #> 626  47.1     10 1974 1974-10-01 18:00:00   bone 2710 #> 627  25.0     11 1974 1974-11-01 04:00:00  blood 2711 #> 628  25.0     11 1974 1974-11-01 04:00:00   bone 2711 #> 629  20.5     12 1974 1974-12-01 14:00:00  blood 2712 #> 630  20.5     12 1974 1974-12-01 14:00:00   bone 2712 #> 631  18.9      1 1975 1975-01-01 00:00:00  blood 2713 #> 632  18.9      1 1975 1975-01-01 00:00:00   bone 2713 #> 633  11.5      2 1975 1975-01-31 10:00:00  blood 2714 #> 634  11.5      2 1975 1975-01-31 10:00:00   bone 2714 #> 635  11.5      3 1975 1975-03-02 20:00:00  blood 2715 #> 636  11.5      3 1975 1975-03-02 20:00:00   bone 2715 #> 637   5.1      4 1975 1975-04-02 06:00:00  blood 2716 #> 638   5.1      4 1975 1975-04-02 06:00:00   bone 2716 #> 639   9.0      5 1975 1975-05-02 16:00:00  blood 2717 #> 640   9.0      5 1975 1975-05-02 16:00:00   bone 2717 #> 641  11.4      6 1975 1975-06-02 02:00:00  blood 2718 #> 642  11.4      6 1975 1975-06-02 02:00:00   bone 2718 #> 643  28.2      7 1975 1975-07-02 12:00:00  blood 2719 #> 644  28.2      7 1975 1975-07-02 12:00:00   bone 2719 #> 645  39.7      8 1975 1975-08-01 22:00:00  blood 2720 #> 646  39.7      8 1975 1975-08-01 22:00:00   bone 2720 #> 647  13.9      9 1975 1975-09-01 08:00:00  blood 2721 #> 648  13.9      9 1975 1975-09-01 08:00:00   bone 2721 #> 649   9.1     10 1975 1975-10-01 18:00:00  blood 2722 #> 650   9.1     10 1975 1975-10-01 18:00:00   bone 2722 #> 651  19.4     11 1975 1975-11-01 04:00:00  blood 2723 #> 652  19.4     11 1975 1975-11-01 04:00:00   bone 2723 #> 653   7.8     12 1975 1975-12-01 14:00:00  blood 2724 #> 654   7.8     12 1975 1975-12-01 14:00:00   bone 2724 #> 655   8.1      1 1976 1976-01-01 00:00:00  blood 2725 #> 656   8.1      1 1976 1976-01-01 00:00:00   bone 2725 #> 657   4.3      2 1976 1976-01-31 12:00:00  blood 2726 #> 658   4.3      2 1976 1976-01-31 12:00:00   bone 2726 #> 659  21.9      3 1976 1976-03-02 00:00:00  blood 2727 #> 660  21.9      3 1976 1976-03-02 00:00:00   bone 2727 #> 661  18.8      4 1976 1976-04-01 12:00:00  blood 2728 #> 662  18.8      4 1976 1976-04-01 12:00:00   bone 2728 #> 663  12.4      5 1976 1976-05-02 00:00:00  blood 2729 #> 664  12.4      5 1976 1976-05-02 00:00:00   bone 2729 #> 665  12.2      6 1976 1976-06-01 12:00:00  blood 2730 #> 666  12.2      6 1976 1976-06-01 12:00:00   bone 2730 #> 667   1.9      7 1976 1976-07-02 00:00:00  blood 2731 #> 668   1.9      7 1976 1976-07-02 00:00:00   bone 2731 #> 669  16.4      8 1976 1976-08-01 12:00:00  blood 2732 #> 670  16.4      8 1976 1976-08-01 12:00:00   bone 2732 #> 671  13.5      9 1976 1976-09-01 00:00:00  blood 2733 #> 672  13.5      9 1976 1976-09-01 00:00:00   bone 2733 #> 673  20.6     10 1976 1976-10-01 12:00:00  blood 2734 #> 674  20.6     10 1976 1976-10-01 12:00:00   bone 2734 #> 675   5.2     11 1976 1976-11-01 00:00:00  blood 2735 #> 676   5.2     11 1976 1976-11-01 00:00:00   bone 2735 #> 677  15.3     12 1976 1976-12-01 12:00:00  blood 2736 #> 678  15.3     12 1976 1976-12-01 12:00:00   bone 2736 #> 679  16.4      1 1977 1977-01-01 00:00:00  blood 2737 #> 680  16.4      1 1977 1977-01-01 00:00:00   bone 2737 #> 681  23.1      2 1977 1977-01-31 10:00:00  blood 2738 #> 682  23.1      2 1977 1977-01-31 10:00:00   bone 2738 #> 683   8.7      3 1977 1977-03-02 20:00:00  blood 2739 #> 684   8.7      3 1977 1977-03-02 20:00:00   bone 2739 #> 685  12.9      4 1977 1977-04-02 06:00:00  blood 2740 #> 686  12.9      4 1977 1977-04-02 06:00:00   bone 2740 #> 687  18.6      5 1977 1977-05-02 16:00:00  blood 2741 #> 688  18.6      5 1977 1977-05-02 16:00:00   bone 2741 #> 689  38.5      6 1977 1977-06-02 02:00:00  blood 2742 #> 690  38.5      6 1977 1977-06-02 02:00:00   bone 2742 #> 691  21.4      7 1977 1977-07-02 12:00:00  blood 2743 #> 692  21.4      7 1977 1977-07-02 12:00:00   bone 2743 #> 693  30.1      8 1977 1977-08-01 22:00:00  blood 2744 #> 694  30.1      8 1977 1977-08-01 22:00:00   bone 2744 #> 695  44.0      9 1977 1977-09-01 08:00:00  blood 2745 #> 696  44.0      9 1977 1977-09-01 08:00:00   bone 2745 #> 697  43.8     10 1977 1977-10-01 18:00:00  blood 2746 #> 698  43.8     10 1977 1977-10-01 18:00:00   bone 2746 #> 699  29.1     11 1977 1977-11-01 04:00:00  blood 2747 #> 700  29.1     11 1977 1977-11-01 04:00:00   bone 2747 #> 701  43.2     12 1977 1977-12-01 14:00:00  blood 2748 #> 702  43.2     12 1977 1977-12-01 14:00:00   bone 2748 #> 703  51.9      1 1978 1978-01-01 00:00:00  blood 2749 #> 704  51.9      1 1978 1978-01-01 00:00:00   bone 2749 #> 705  93.6      2 1978 1978-01-31 10:00:00  blood 2750 #> 706  93.6      2 1978 1978-01-31 10:00:00   bone 2750 #> 707  76.5      3 1978 1978-03-02 20:00:00  blood 2751 #> 708  76.5      3 1978 1978-03-02 20:00:00   bone 2751 #> 709  99.7      4 1978 1978-04-02 06:00:00  blood 2752 #> 710  99.7      4 1978 1978-04-02 06:00:00   bone 2752 #> 711  82.7      5 1978 1978-05-02 16:00:00  blood 2753 #> 712  82.7      5 1978 1978-05-02 16:00:00   bone 2753 #> 713  95.1      6 1978 1978-06-02 02:00:00  blood 2754 #> 714  95.1      6 1978 1978-06-02 02:00:00   bone 2754 #> 715  70.4      7 1978 1978-07-02 12:00:00  blood 2755 #> 716  70.4      7 1978 1978-07-02 12:00:00   bone 2755 #> 717  58.1      8 1978 1978-08-01 22:00:00  blood 2756 #> 718  58.1      8 1978 1978-08-01 22:00:00   bone 2756 #> 719 138.2      9 1978 1978-09-01 08:00:00  blood 2757 #> 720 138.2      9 1978 1978-09-01 08:00:00   bone 2757 #> 721 125.1     10 1978 1978-10-01 18:00:00  blood 2758 #> 722 125.1     10 1978 1978-10-01 18:00:00   bone 2758 #> 723  97.9     11 1978 1978-11-01 04:00:00  blood 2759 #> 724  97.9     11 1978 1978-11-01 04:00:00   bone 2759 #> 725 122.7     12 1978 1978-12-01 14:00:00  blood 2760 #> 726 122.7     12 1978 1978-12-01 14:00:00   bone 2760 #> 727 166.6      1 1979 1979-01-01 00:00:00  blood 2761 #> 728 166.6      1 1979 1979-01-01 00:00:00   bone 2761 #> 729 137.5      2 1979 1979-01-31 10:00:00  blood 2762 #> 730 137.5      2 1979 1979-01-31 10:00:00   bone 2762 #> 731 138.0      3 1979 1979-03-02 20:00:00  blood 2763 #> 732 138.0      3 1979 1979-03-02 20:00:00   bone 2763 #> 733 101.5      4 1979 1979-04-02 06:00:00  blood 2764 #> 734 101.5      4 1979 1979-04-02 06:00:00   bone 2764 #> 735 134.4      5 1979 1979-05-02 16:00:00  blood 2765 #> 736 134.4      5 1979 1979-05-02 16:00:00   bone 2765 #> 737 149.5      6 1979 1979-06-02 02:00:00  blood 2766 #> 738 149.5      6 1979 1979-06-02 02:00:00   bone 2766 #> 739 159.4      7 1979 1979-07-02 12:00:00  blood 2767 #> 740 159.4      7 1979 1979-07-02 12:00:00   bone 2767 #> 741 142.2      8 1979 1979-08-01 22:00:00  blood 2768 #> 742 142.2      8 1979 1979-08-01 22:00:00   bone 2768 #> 743 188.4      9 1979 1979-09-01 08:00:00  blood 2769 #> 744 188.4      9 1979 1979-09-01 08:00:00   bone 2769 #> 745 186.2     10 1979 1979-10-01 18:00:00  blood 2770 #> 746 186.2     10 1979 1979-10-01 18:00:00   bone 2770 #> 747 183.3     11 1979 1979-11-01 04:00:00  blood 2771 #> 748 183.3     11 1979 1979-11-01 04:00:00   bone 2771 #> 749 176.3     12 1979 1979-12-01 14:00:00  blood 2772 #> 750 176.3     12 1979 1979-12-01 14:00:00   bone 2772 #> 751 159.6      1 1980 1980-01-01 00:00:00  blood 2773 #> 752 159.6      1 1980 1980-01-01 00:00:00   bone 2773 #> 753 155.0      2 1980 1980-01-31 12:00:00  blood 2774 #> 754 155.0      2 1980 1980-01-31 12:00:00   bone 2774 #> 755 126.2      3 1980 1980-03-02 00:00:00  blood 2775 #> 756 126.2      3 1980 1980-03-02 00:00:00   bone 2775 #> 757 164.1      4 1980 1980-04-01 12:00:00  blood 2776 #> 758 164.1      4 1980 1980-04-01 12:00:00   bone 2776 #> 759 179.9      5 1980 1980-05-02 00:00:00  blood 2777 #> 760 179.9      5 1980 1980-05-02 00:00:00   bone 2777 #> 761 157.3      6 1980 1980-06-01 12:00:00  blood 2778 #> 762 157.3      6 1980 1980-06-01 12:00:00   bone 2778 #> 763 136.3      7 1980 1980-07-02 00:00:00  blood 2779 #> 764 136.3      7 1980 1980-07-02 00:00:00   bone 2779 #> 765 135.4      8 1980 1980-08-01 12:00:00  blood 2780 #> 766 135.4      8 1980 1980-08-01 12:00:00   bone 2780 #> 767 155.0      9 1980 1980-09-01 00:00:00  blood 2781 #> 768 155.0      9 1980 1980-09-01 00:00:00   bone 2781 #> 769 164.7     10 1980 1980-10-01 12:00:00  blood 2782 #> 770 164.7     10 1980 1980-10-01 12:00:00   bone 2782 #> 771 147.9     11 1980 1980-11-01 00:00:00  blood 2783 #> 772 147.9     11 1980 1980-11-01 00:00:00   bone 2783 #> 773 174.4     12 1980 1980-12-01 12:00:00  blood 2784 #> 774 174.4     12 1980 1980-12-01 12:00:00   bone 2784 #> 775 114.0      1 1981 1981-01-01 00:00:00  blood 2785 #> 776 114.0      1 1981 1981-01-01 00:00:00   bone 2785 #> 777 141.3      2 1981 1981-01-31 10:00:00  blood 2786 #> 778 141.3      2 1981 1981-01-31 10:00:00   bone 2786 #> 779 135.5      3 1981 1981-03-02 20:00:00  blood 2787 #> 780 135.5      3 1981 1981-03-02 20:00:00   bone 2787 #> 781 156.4      4 1981 1981-04-02 06:00:00  blood 2788 #> 782 156.4      4 1981 1981-04-02 06:00:00   bone 2788 #> 783 127.5      5 1981 1981-05-02 16:00:00  blood 2789 #> 784 127.5      5 1981 1981-05-02 16:00:00   bone 2789 #> 785  90.0      6 1981 1981-06-02 02:00:00  blood 2790 #> 786  90.0      6 1981 1981-06-02 02:00:00   bone 2790 #> 787 143.8      7 1981 1981-07-02 12:00:00  blood 2791 #> 788 143.8      7 1981 1981-07-02 12:00:00   bone 2791 #> 789 158.7      8 1981 1981-08-01 22:00:00  blood 2792 #> 790 158.7      8 1981 1981-08-01 22:00:00   bone 2792 #> 791 167.3      9 1981 1981-09-01 08:00:00  blood 2793 #> 792 167.3      9 1981 1981-09-01 08:00:00   bone 2793 #> 793 162.4     10 1981 1981-10-01 18:00:00  blood 2794 #> 794 162.4     10 1981 1981-10-01 18:00:00   bone 2794 #> 795 137.5     11 1981 1981-11-01 04:00:00  blood 2795 #> 796 137.5     11 1981 1981-11-01 04:00:00   bone 2795 #> 797 150.1     12 1981 1981-12-01 14:00:00  blood 2796 #> 798 150.1     12 1981 1981-12-01 14:00:00   bone 2796 #> 799 111.2      1 1982 1982-01-01 00:00:00  blood 2797 #> 800 111.2      1 1982 1982-01-01 00:00:00   bone 2797 #> 801 163.6      2 1982 1982-01-31 10:00:00  blood 2798 #> 802 163.6      2 1982 1982-01-31 10:00:00   bone 2798 #> 803 153.8      3 1982 1982-03-02 20:00:00  blood 2799 #> 804 153.8      3 1982 1982-03-02 20:00:00   bone 2799 #> 805 122.0      4 1982 1982-04-02 06:00:00  blood 2800 #> 806 122.0      4 1982 1982-04-02 06:00:00   bone 2800 #> 807  82.2      5 1982 1982-05-02 16:00:00  blood 2801 #> 808  82.2      5 1982 1982-05-02 16:00:00   bone 2801 #> 809 110.4      6 1982 1982-06-02 02:00:00  blood 2802 #> 810 110.4      6 1982 1982-06-02 02:00:00   bone 2802 #> 811 106.1      7 1982 1982-07-02 12:00:00  blood 2803 #> 812 106.1      7 1982 1982-07-02 12:00:00   bone 2803 #> 813 107.6      8 1982 1982-08-01 22:00:00  blood 2804 #> 814 107.6      8 1982 1982-08-01 22:00:00   bone 2804 #> 815 118.8      9 1982 1982-09-01 08:00:00  blood 2805 #> 816 118.8      9 1982 1982-09-01 08:00:00   bone 2805 #> 817  94.7     10 1982 1982-10-01 18:00:00  blood 2806 #> 818  94.7     10 1982 1982-10-01 18:00:00   bone 2806 #> 819  98.1     11 1982 1982-11-01 04:00:00  blood 2807 #> 820  98.1     11 1982 1982-11-01 04:00:00   bone 2807 #> 821 127.0     12 1982 1982-12-01 14:00:00  blood 2808 #> 822 127.0     12 1982 1982-12-01 14:00:00   bone 2808 #> 823  84.3      1 1983 1983-01-01 00:00:00  blood 2809 #> 824  84.3      1 1983 1983-01-01 00:00:00   bone 2809 #> 825  51.0      2 1983 1983-01-31 10:00:00  blood 2810 #> 826  51.0      2 1983 1983-01-31 10:00:00   bone 2810 #> 827  66.5      3 1983 1983-03-02 20:00:00  blood 2811 #> 828  66.5      3 1983 1983-03-02 20:00:00   bone 2811 #> 829  80.7      4 1983 1983-04-02 06:00:00  blood 2812 #> 830  80.7      4 1983 1983-04-02 06:00:00   bone 2812 #> 831  99.2      5 1983 1983-05-02 16:00:00  blood 2813 #> 832  99.2      5 1983 1983-05-02 16:00:00   bone 2813 #> 833  91.1      6 1983 1983-06-02 02:00:00  blood 2814 #> 834  91.1      6 1983 1983-06-02 02:00:00   bone 2814 #> 835  82.2      7 1983 1983-07-02 12:00:00  blood 2815 #> 836  82.2      7 1983 1983-07-02 12:00:00   bone 2815 #> 837  71.8      8 1983 1983-08-01 22:00:00  blood 2816 #> 838  71.8      8 1983 1983-08-01 22:00:00   bone 2816 #> 839  50.3      9 1983 1983-09-01 08:00:00  blood 2817 #> 840  50.3      9 1983 1983-09-01 08:00:00   bone 2817 #> 841  55.8     10 1983 1983-10-01 18:00:00  blood 2818 #> 842  55.8     10 1983 1983-10-01 18:00:00   bone 2818 #> 843  33.3     11 1983 1983-11-01 04:00:00  blood 2819 #> 844  33.3     11 1983 1983-11-01 04:00:00   bone 2819 #> 845  33.4     12 1983 1983-12-01 14:00:00  blood 2820 #> 846  33.4     12 1983 1983-12-01 14:00:00   bone 2820 #>   # An xts object example library(xts) #> Loading required package: zoo #>  #> Attaching package: ‘zoo’ #> The following objects are masked from ‘package:base’: #>  #>     as.Date, as.Date.numeric dates <- seq(as.Date(\"2001-05-01\"), length=30, by=\"quarter\") data  <- cbind(c(gas = rpois(30, cumprod(1+rnorm(30, mean = 0.01, sd = 0.001)))), c(oil = rpois(30, cumprod(1+rnorm(30, mean = 0.01, sd = 0.001))))) series <- xts(x = data, order.by = dates) colnames(series) <- c('gas', 'oil') head(series) #>            gas oil #> 2001-05-01   1   1 #> 2001-08-01   0   0 #> 2001-11-01   0   1 #> 2002-02-01   0   2 #> 2002-05-01   1   1 #> 2002-08-01   1   1 series_to_mvgam(series, freq = 4, train_prop = 0.85) #> $data_train #>    y season year       date series time #> 1  1      2 2001 2001-05-01    gas    1 #> 2  1      2 2001 2001-05-01    oil    1 #> 3  0      3 2001 2001-08-01    gas    2 #> 4  0      3 2001 2001-08-01    oil    2 #> 5  0      4 2001 2001-11-01    gas    3 #> 6  1      4 2001 2001-11-01    oil    3 #> 7  0      1 2002 2002-02-01    gas    4 #> 8  2      1 2002 2002-02-01    oil    4 #> 9  1      2 2002 2002-05-01    gas    5 #> 10 1      2 2002 2002-05-01    oil    5 #> 11 1      3 2002 2002-08-01    gas    6 #> 12 1      3 2002 2002-08-01    oil    6 #> 13 0      4 2002 2002-11-01    gas    7 #> 14 1      4 2002 2002-11-01    oil    7 #> 15 0      1 2003 2003-02-01    gas    8 #> 16 0      1 2003 2003-02-01    oil    8 #> 17 0      2 2003 2003-05-01    gas    9 #> 18 0      2 2003 2003-05-01    oil    9 #> 19 3      3 2003 2003-08-01    gas   10 #> 20 2      3 2003 2003-08-01    oil   10 #> 21 1      4 2003 2003-11-01    gas   11 #> 22 1      4 2003 2003-11-01    oil   11 #> 23 1      1 2004 2004-02-01    gas   12 #> 24 0      1 2004 2004-02-01    oil   12 #> 25 0      2 2004 2004-05-01    gas   13 #> 26 2      2 2004 2004-05-01    oil   13 #> 27 2      3 2004 2004-08-01    gas   14 #> 28 1      3 2004 2004-08-01    oil   14 #> 29 2      4 2004 2004-11-01    gas   15 #> 30 3      4 2004 2004-11-01    oil   15 #> 31 0      1 2005 2005-02-01    gas   16 #> 32 0      1 2005 2005-02-01    oil   16 #> 33 1      2 2005 2005-05-01    gas   17 #> 34 1      2 2005 2005-05-01    oil   17 #> 35 0      3 2005 2005-08-01    gas   18 #> 36 3      3 2005 2005-08-01    oil   18 #> 37 3      4 2005 2005-11-01    gas   19 #> 38 0      4 2005 2005-11-01    oil   19 #> 39 0      1 2006 2006-02-01    gas   20 #> 40 1      1 2006 2006-02-01    oil   20 #> 41 2      2 2006 2006-05-01    gas   21 #> 42 5      2 2006 2006-05-01    oil   21 #> 43 3      3 2006 2006-08-01    gas   22 #> 44 2      3 2006 2006-08-01    oil   22 #> 45 3      4 2006 2006-11-01    gas   23 #> 46 2      4 2006 2006-11-01    oil   23 #> 47 0      1 2007 2007-02-01    gas   24 #> 48 2      1 2007 2007-02-01    oil   24 #> 49 2      2 2007 2007-05-01    gas   25 #> 50 1      2 2007 2007-05-01    oil   25 #>  #> $data_test #>    y season year       date series time #> 1  2      3 2007 2007-08-01    gas   26 #> 2  2      3 2007 2007-08-01    oil   26 #> 3  1      4 2007 2007-11-01    gas   27 #> 4  0      4 2007 2007-11-01    oil   27 #> 5  0      1 2008 2008-02-01    gas   28 #> 6  2      1 2008 2008-02-01    oil   28 #> 7  0      2 2008 2008-05-01    gas   29 #> 8  1      2 2008 2008-05-01    oil   29 #> 9  1      3 2008 2008-08-01    gas   30 #> 10 0      3 2008 2008-08-01    oil   30 #>"},{"path":"https://nicholasjclark.github.io/mvgam/reference/sim_mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a set of time series for modelling in mvgam — sim_mvgam","title":"Simulate a set of time series for modelling in mvgam — sim_mvgam","text":"function simulates sets time series data fitting multivariate GAM includes shared seasonality dependence state-space latent dynamic factors. Random dependencies among series, .e. correlations long-term trends, included form correlated loadings latent dynamic factors","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/sim_mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a set of time series for modelling in mvgam — sim_mvgam","text":"","code":"sim_mvgam(   T = 100,   n_series = 3,   seasonality = \"shared\",   use_lv = FALSE,   n_lv = 0,   trend_model = RW(),   drift = FALSE,   prop_trend = 0.2,   trend_rel,   freq = 12,   family = poisson(),   phi,   shape,   sigma,   nu,   mu,   prop_missing = 0,   prop_train = 0.85 )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/sim_mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a set of time series for modelling in mvgam — sim_mvgam","text":"T integer. Number observations (timepoints) n_series integer. Number discrete time series seasonality character. Either shared, meaning series share exact seasonal pattern, hierarchical, meaning global seasonality series' pattern can deviate slightly use_lv logical. TRUE, use dynamic factors estimate series' latent trends reduced dimension format. FALSE, estimate independent latent trends series n_lv integer. Number latent dynamic factors generating series' trends. Defaults 0, meaning dynamics estimated independently series trend_model character specifying time series dynamics latent trend. Options : None (latent trend component; .e. GAM component contributes linear predictor, observation process source error; similarly estimated gam) RW (random walk possible drift) AR1 (possible drift) AR2 (possible drift) AR3 (possible drift) VAR1 (contemporaneously uncorrelated VAR1) VAR1cor (contemporaneously correlated VAR1) GP (Gaussian Process squared exponential kernel) See mvgam_trends details drift logical, simulate drift term trend prop_trend numeric. Relative importance trend series. 0 1 trend_rel Deprecated. Use prop_trend instead freq integer. seasonal frequency series family family specifying exponential observation family series. Currently supported families : nb(), poisson(), bernoulli(), tweedie(), gaussian(), betar(), lognormal(), student() Gamma() phi vector dispersion parameters series (.e. size nb() phi betar()). length(phi) < n_series, first element phi replicated n_series times. Defaults 5 nb() tweedie(); 10 betar() shape vector shape parameters series (.e. shape gamma()) length(shape) < n_series, first element shape replicated n_series times. Defaults 10 sigma vector scale parameters series (.e. sd gaussian() student(), log(sd) lognormal()). length(sigma) < n_series, first element sigma replicated n_series times. Defaults 0.5 gaussian() student(); 0.2 lognormal() nu vector degrees freedom parameters series (.e. nu student()) length(nu) < n_series, first element nu replicated n_series times. Defaults 3 mu vector location parameters series. length(mu) < n_series, first element mu replicated n_series times. Defaults small random values -0.5 0.5 link scale prop_missing numeric stating proportion observations missing. 0 0.8, inclusive prop_train numeric stating proportion data use training. 0.2 1","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/sim_mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a set of time series for modelling in mvgam — sim_mvgam","text":"list object containing outputs needed mvgam, including 'data_train' 'data_test', well additional information simulated seasonality trend dependencies","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/sim_mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a set of time series for modelling in mvgam — sim_mvgam","text":"","code":"# Simulate series with observations bounded at 0 and 1 (Beta responses) sim_data <- sim_mvgam(family = betar(), trend_model = RW(), prop_trend = 0.6) plot_mvgam_series(data = sim_data$data_train, series = 'all')   # Now simulate series with overdispersed discrete observations sim_data <- sim_mvgam(family = nb(), trend_model = RW(), prop_trend = 0.6, phi = 10) plot_mvgam_series(data = sim_data$data_train, series = 'all')"},{"path":"https://nicholasjclark.github.io/mvgam/reference/stability.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate measures of latent VAR community stability — stability.mvgam","title":"Calculate measures of latent VAR community stability — stability.mvgam","text":"Compute reactivity, return rates contributions interactions stationary forecast variance mvgam models Vector Autoregressive dynamics","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/stability.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate measures of latent VAR community stability — stability.mvgam","text":"","code":"stability(object, ...)  # S3 method for mvgam stability(object, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/stability.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate measures of latent VAR community stability — stability.mvgam","text":"object list object class mvgam resulting call mvgam() used Vector Autoregressive latent process model (either VAR(cor = FALSE) VAR(cor = TRUE)) ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/stability.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate measures of latent VAR community stability — stability.mvgam","text":"data.frame containing posterior draws stability metric.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/stability.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate measures of latent VAR community stability — stability.mvgam","text":"measures stability can used assess important inter-series dependencies variability multivariate system ask systems expected respond environmental perturbations. Using formula latent VAR(1) : $$ \\mu_t \\sim \\text{MVNormal}((\\mu_{t - 1}), \\Sigma) \\quad $$ function calculate long-term stationary forecast distribution system, mean \\(\\mu_{\\infty}\\) variance \\(\\Sigma_{\\infty}\\), calculate following quantities: prop_int: Proportion volume stationary forecast distribution attributable lagged interactions (.e. important autoregressive interaction coefficients \\(\\) explaining shape stationary forecast distribution?): $$     det()^2 \\quad     $$ prop_int_adj: prop_int scaled number series \\(p\\) facilitate direct comparisons among systems different numbers interacting variables: $$     det()^{2/p} \\quad     $$ prop_int_offdiag: Sensitivity prop_int inter-series interactions (.e. important -diagonals autoregressive coefficient matrix \\(\\) shaping prop_int?), calculated relative magnitude -diagonals partial derivative matrix: $$     [2~det() (^{-1})^T] \\quad     $$ prop_int_diag: Sensitivity prop_int intra-series interactions (.e. important diagonals autoregressive coefficient matrix \\(\\) shaping prop_int?), calculated relative magnitude diagonals partial derivative matrix: $$     [2~det() (^{-1})^T] \\quad     $$ prop_cov_offdiag: Sensitivity \\(\\Sigma_{\\infty}\\) inter-series error correlations (.e. important -diagonal covariances \\(\\Sigma\\) shaping \\(\\Sigma_{\\infty}\\)?), calculated relative magnitude -diagonals partial derivative matrix: $$     [2~det(\\Sigma_{\\infty}) (\\Sigma_{\\infty}^{-1})^T] \\quad     $$ prop_cov_diag: Sensitivity \\(\\Sigma_{\\infty}\\) error variances (.e. important diagonal variances \\(\\Sigma\\) shaping \\(\\Sigma_{\\infty}\\)?), calculated relative magnitude diagonals partial derivative matrix: $$     [2~det(\\Sigma_{\\infty}) (\\Sigma_{\\infty}^{-1})^T] \\quad     $$ reactivity: measure degree system moves away stable equilibrium following perturbation. Values > 0 suggest system reactive, whereby perturbation system one period can amplified next period. \\(\\sigma_{max}()\\) largest singular value \\(\\), reactivity defined : $$     log\\sigma_{max}() \\quad     $$ mean_return_rate: Asymptotic (long-term) return rate mean transition distribution stationary mean, calculated using largest eigenvalue matrix \\(\\): $$    max(\\lambda_{}) \\quad    $$ Lower values suggest greater stability var_return_rate: Asymptotic (long-term) return rate variance transition distribution stationary variance: $$    max(\\lambda_{\\otimes{}}) \\quad    $$ , lower values suggest greater stability Major advantages using mvgam compute metrics well-calibrated uncertainties available VAR processes forced stationary. properties make simple insightful calculate inspect aspects long-term short-term stability. also possible directly inspect possible interactions among time series latent VAR process. , can calculate Generalized Orthogonalized Impulse Response Functions using irf function, can calculate Forecast Error Variance Decompositions using fevd function.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/stability.mvgam.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate measures of latent VAR community stability — stability.mvgam","text":"AR Ives, B Dennis, KL Cottingham & SR Carpenter (2003). Estimating community stability ecological interactions time-series data. Ecological Monographs. 73, 301-330.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/stability.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate measures of latent VAR community stability — stability.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/stability.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate measures of latent VAR community stability — stability.mvgam","text":"","code":"# \\donttest{ # Simulate some time series that follow a latent VAR(1) process simdat <- sim_mvgam(family = gaussian(),                     n_series = 4,                     trend_model = VAR(cor = TRUE),                     prop_trend = 1) plot_mvgam_series(data = simdat$data_train, series = 'all')   # Fit a model that uses a latent VAR(1) mod <- mvgam(y ~ -1,              trend_formula = ~ 1,              trend_model = VAR(cor = TRUE),              family = gaussian(),              data = simdat$data_train,              chains = 2,              silent = 2)  # Calulate stability metrics for this system metrics <- stability(mod)  # Proportion of stationary forecast distribution # attributable to lagged interactions hist(metrics$prop_int,      xlim = c(0, 1),      xlab = 'Prop_int',      main = '',      col = '#B97C7C',      border = 'white')   # Within this contribution of interactions, how important # are inter-series interactions (offdiagonals of the A matrix) vs # intra-series density dependence (diagonals of the A matrix)? layout(matrix(1:2, nrow = 2)) hist(metrics$prop_int_offdiag,      xlim = c(0, 1),      xlab = '',      main = 'Inter-series interactions',      col = '#B97C7C',      border = 'white')  hist(metrics$prop_int_diag,      xlim = c(0, 1),      xlab = 'Contribution to interaction effect',      main = 'Intra-series interactions (density dependence)',      col = 'darkblue',      border = 'white')  layout(1)  # How important are inter-series error covariances # (offdiagonals of the Sigma matrix) vs # intra-series variances (diagonals of the Sigma matrix) for explaining # the variance of the stationary forecast distribution? layout(matrix(1:2, nrow = 2)) hist(metrics$prop_cov_offdiag,      xlim = c(0, 1),      xlab = '',      main = 'Inter-series covariances',      col = '#B97C7C',      border = 'white')  hist(metrics$prop_cov_diag,      xlim = c(0, 1),      xlab = 'Contribution to forecast variance',      main = 'Intra-series variances',      col = 'darkblue',      border = 'white')  layout(1)  # Reactivity, i.e. degree to which the system moves # away from a stable equilibrium following a perturbation # (values > 1 suggest a more reactive, less stable system) hist(metrics$reactivity,      main = '',      xlab = 'Reactivity',      col = '#B97C7C',      border = 'white',      xlim = c(-1*max(abs(metrics$reactivity)),               max(abs(metrics$reactivity)))) abline(v = 0, lwd = 2.5)  # }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary for a fitted mvgam models — summary.mvgam","title":"Summary for a fitted mvgam models — summary.mvgam","text":"functions take fitted mvgam jsdgam object return various useful summaries","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary for a fitted mvgam models — summary.mvgam","text":"","code":"# S3 method for mvgam summary(object, include_betas = TRUE, smooth_test = TRUE, digits = 2, ...)  # S3 method for mvgam_prefit summary(object, ...)  # S3 method for mvgam coef(object, summarise = TRUE, ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary for a fitted mvgam models — summary.mvgam","text":"object list object returned mvgam include_betas Logical. Print summary includes posterior summaries linear predictor beta coefficients (including spline coefficients)? Defaults TRUE use FALSE concise summary smooth_test Logical. Compute estimated degrees freedom approximate p-values smooth terms? Defaults TRUE, users may wish set FALSE complex models many smooth random effect terms digits number significant digits printing summary; defaults 2. ... Ignored summarise logical. Summaries coefficients returned TRUE. Otherwise full posterior distribution returned","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary for a fitted mvgam models — summary.mvgam","text":"summary.mvgam summary.mvgam_prefit, list printed -screen showing summaries model coef.mvgam, either matrix posterior coefficient distributions (summarise == FALSE data.frame coefficient summaries)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summary for a fitted mvgam models — summary.mvgam","text":"summary.mvgam summary.mvgam_prefit return brief summaries model's call, along posterior intervals key parameters model. Note smooths extra penalties null space, summaries rho parameters may include penalty terms number smooths original model formula. Approximate p-values smooth terms also returned, methods used calculation following used mgcv equivalents (see summary.gam details). Estimated Degrees Freedom (edf) smooth terms computed using either edf.type = 1 models trend component, edf.type = 0 models trend components. described documentation jagam. Experiments suggest p-values tend conservative might returned equivalent model fit summary.gam using method = 'REML' coef.mvgam returns either summaries full posterior estimates GAM component coefficients","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summary for a fitted mvgam models — summary.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_fevd.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior summary of forecast error variance decompositions — summary.mvgam_fevd","title":"Posterior summary of forecast error variance decompositions — summary.mvgam_fevd","text":"function takes mvgam_fevd object calculates posterior summary error variance decompositions series, horizons","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_fevd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior summary of forecast error variance decompositions — summary.mvgam_fevd","text":"","code":"# S3 method for mvgam_fevd summary(object, probs = c(0.025, 0.975), ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_fevd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior summary of forecast error variance decompositions — summary.mvgam_fevd","text":"object object class mvgam_fevd obtained using fevd() function. object contain draws posterior distribution forecast error variance decompositions. probs upper lower percentiles computed quantile function, addition median ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_fevd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior summary of forecast error variance decompositions — summary.mvgam_fevd","text":"long-format tibble / data.frame reporting posterior median, upper lower percentiles error variance decompositions series horizons.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_fevd.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Posterior summary of forecast error variance decompositions — summary.mvgam_fevd","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_forecast.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior summary of hindcast and forecast objects — summary.mvgam_forecast","title":"Posterior summary of hindcast and forecast objects — summary.mvgam_forecast","text":"function takes mvgam_forecast object calculates posterior summary hindcast forecast distributions series, along true values included data newdata type = 'response' used call hindcast() function()","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_forecast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior summary of hindcast and forecast objects — summary.mvgam_forecast","text":"","code":"# S3 method for mvgam_forecast summary(object, probs = c(0.025, 0.975), ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_forecast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior summary of hindcast and forecast objects — summary.mvgam_forecast","text":"object object class mvgam_forecast obtained using either hindcast() function() function. object contain draws posterior distribution hindcasts forecasts. probs upper lower percentiles computed quantile function, addition median ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_forecast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior summary of hindcast and forecast objects — summary.mvgam_forecast","text":"long-format tibble / data.frame reporting posterior median, upper lower percentiles predictions series timepoints originally supplied data , optionally, newdata.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_forecast.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Posterior summary of hindcast and forecast objects — summary.mvgam_forecast","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_irf.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior summary of impulse responses — summary.mvgam_irf","title":"Posterior summary of impulse responses — summary.mvgam_irf","text":"function takes mvgam_irf object calculates posterior summary impulse responses series shocks series, horizons","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_irf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior summary of impulse responses — summary.mvgam_irf","text":"","code":"# S3 method for mvgam_irf summary(object, probs = c(0.025, 0.975), ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_irf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior summary of impulse responses — summary.mvgam_irf","text":"object object class mvgam_irf obtained using irf() function. object contain draws posterior distribution impulse responses. probs upper lower percentiles computed quantile function, addition median ... ignored","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_irf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior summary of impulse responses — summary.mvgam_irf","text":"long-format tibble / data.frame reporting posterior median, upper lower percentiles impulse responses series shocks series horizons.","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/summary.mvgam_irf.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Posterior summary of impulse responses — summary.mvgam_irf","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/tidy.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy an mvgam object's parameter posteriors — tidy.mvgam","title":"Tidy an mvgam object's parameter posteriors — tidy.mvgam","text":"Get parameters' posterior statistics, implementing generic tidy package broom.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/tidy.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidy an mvgam object's parameter posteriors — tidy.mvgam","text":"","code":"# S3 method for mvgam tidy(x, probs = c(0.025, 0.5, 0.975), ...)"},{"path":"https://nicholasjclark.github.io/mvgam/reference/tidy.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidy an mvgam object's parameter posteriors — tidy.mvgam","text":"x object class mvgam. probs desired probability levels parameters' posteriors. Defaults c(0.025, 0.5, 0.975), .e. 2.5%, 50%, 97.5%. ... Unused, included generic consistency .","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/tidy.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tidy an mvgam object's parameter posteriors — tidy.mvgam","text":"tibble containing: \"parameter\": parameter question. \"type\": component model parameter belongs (see details). \"mean\": posterior mean. \"sd\": posterior standard deviation. percentile(s): percentiles interest posteriors.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/tidy.mvgam.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tidy an mvgam object's parameter posteriors — tidy.mvgam","text":"parameters categorized column \"type\". instance, intercept observation model (.e. \"formula\" arg mvgam()) \"type\" \"observation_beta\". possible \"type\"s : observation_family_extra_param: extra parameters observation model, e.g. sigma gaussian observation model. parameters directly derived latent trend components (continuing gaussian example, contrast mu). observation_beta: betas observation model, excluding smooths. formula y ~ x1 + s(x2, bs='cr'), intercept x1's beta categorized . random_effect_group_level: Group-level random effects parameters, .e. mean sd distribution specific random intercepts/slopes considered drawn . random_effect_beta: betas individual random intercepts/slopes. trend_model_param: parameters trend_model. trend_beta: analog \"observation_beta\", trend_formula. trend_random_effect_group_level: analog \"random_effect_group_level\", trend_formula. trend_random_effect_beta: analog \"random_effect_beta\", trend_formula. Additionally, GP terms can incorporated several ways, leading different \"type\"s (absence!): s(bs = \"gp\"): parameters returned. gp() formula: \"type\" \"observation_param\". gp() trend_formula: \"type\" \"trend_formula_param\". GP() trend_model: \"type\" \"trend_model_param\".","code":""},{"path":[]},{"path":"https://nicholasjclark.github.io/mvgam/reference/tidy.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tidy an mvgam object's parameter posteriors — tidy.mvgam","text":"","code":"if (FALSE) { set.seed(0) simdat <- sim_mvgam(T = 100,                     n_series = 3,                     trend_model = AR(),                     prop_trend = 0.75,                     family = gaussian()) simdat$data_train$x = rnorm(nrow(simdat$data_train)) simdat$data_train$year_fac = factor(simdat$data_train$year)  mod <- mvgam(y ~ - 1 + s(time, by = series, bs = 'cr', k = 20) + x,              trend_formula = ~ s(year_fac, bs = 're') - 1,              trend_model = AR(cor = TRUE),              family = gaussian(),              data = simdat$data_train,              silent = 2)  tidy(mod, probs = c(0.2, 0.5, 0.8)) }"},{"path":"https://nicholasjclark.github.io/mvgam/reference/update.mvgam.html","id":null,"dir":"Reference","previous_headings":"","what":"Update an existing mvgam model object — update.mvgam","title":"Update an existing mvgam model object — update.mvgam","text":"function allows previously fitted mvgam model updated","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/update.mvgam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update an existing mvgam model object — update.mvgam","text":"","code":"# S3 method for mvgam update(   object,   formula,   trend_formula,   knots,   trend_knots,   trend_model,   family,   share_obs_params,   data,   newdata,   trend_map,   use_lv,   n_lv,   priors,   chains,   burnin,   samples,   threads,   algorithm,   lfo = FALSE,   ... )  # S3 method for jsdgam update(   object,   formula,   factor_formula,   knots,   factor_knots,   data,   newdata,   n_lv,   family,   share_obs_params,   priors,   chains,   burnin,   samples,   threads,   algorithm,   lfo = FALSE,   ... )"},{"path":"https://nicholasjclark.github.io/mvgam/reference/update.mvgam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update an existing mvgam model object — update.mvgam","text":"object list object returned mvgam. See mvgam() formula Optional new formula object. Note, mvgam currently support dynamic formula updates removal specific terms - term. updating, entire formula needs supplied trend_formula optional formula object specifying GAM process model formula. supplied, linear predictor modelled latent trends capture process model evolution separately observation model. response variable specified left-hand side formula (.e. valid option ~ season + s(year)). Also note use identifier series formula specify effects vary across time series. Instead use trend. ensure models trend_map supplied still work consistently (.e. allowing effects vary across process models, even time series share underlying process model). feature currently available RW(), AR() VAR() trend models. nmix() family models, trend_formula used set linear predictor underlying latent abundance. aware can challenging simultaneously estimate intercept parameters observation mode (captured formula) process model (captured trend_formula). Users recommended drop one using - 1 convention formula right hand side. knots optional list containing user specified knot values used basis construction. bases user simply supplies knots used, must match k value supplied (note number knots always just k). Different terms can use different numbers knots, unless share covariate trend_knots knots , optional list knot values smooth functions within trend_formula trend_model character  function specifying time series dynamics latent trend. Options : None (latent trend component; .e. GAM component contributes linear predictor, observation process source error; similarly estimated gam) ZMVN ZMVN() (Zero-Mean Multivariate Normal; available Stan) 'RW' RW() 'AR1' AR(p = 1) 'AR2' AR(p = 2) 'AR3' AR(p = 3) 'CAR1' CAR(p = 1) (also known Ornstein–Uhlenbeck process) 'VAR1'  VAR()(available Stan) 'PWlogistic, 'PWlinear' PW() (available Stan) 'GP' GP() (Gaussian Process squared exponential kernel; available Stan) trend types apart ZMVN(), GP(), CAR() PW(), moving average /correlated process error terms can also estimated (example, RW(cor = TRUE) set multivariate Random Walk n_series > 1). also possible many multivariate trends estimate hierarchical correlations data structured among levels relevant grouping factor. See mvgam_trends details see ZMVN() example. family family specifying exponential observation family series. Currently supported families : gaussian() real-valued data betar() proportional data (0,1) lognormal() non-negative real-valued data student_t() real-valued data Gamma() non-negative real-valued data bernoulli() binary data poisson() count data nb() overdispersed count data binomial() count data imperfect detection number trials known; note cbind() function must used bind discrete observations discrete number trials beta_binomial() binomial() allows overdispersion nmix() count data imperfect detection number trials unknown modeled via State-Space N-Mixture model. latent states Poisson, capturing 'true' latent abundance, observation process Binomial account imperfect detection. See mvgam_families example use family Default poisson(). See mvgam_families details share_obs_params logical. TRUE family additional family-specific observation parameters (e.g. variance components student_t() gaussian(), dispersion parameters nb() betar()), parameters shared across outcome variables. handy multiple outcomes (time series mvgam models) believe share properties, species different spatial units. Default FALSE. data dataframe list containing model response variable covariates required GAM formula optional trend_formula. models include columns: series (factor index series IDs; number levels identical number unique series labels (.e. n_series = length(levels(data$series)))) time (numeric integer index time point observation). dynamic trend types available mvgam (see argument trend_model), time measured discrete, regularly spaced intervals (.e. c(1, 2, 3, ...)). However can use irregularly spaced intervals using trend_model = CAR(1), though note temporal intervals exactly 0 adjusted small number (1e-12) prevent sampling errors. See example CAR() trends CAR() Note however special cases identifiers needed. example, models hierarchical temporal correlation processes (e.g. AR(gr = region, subgr = species)) include series identifier, constructed internally (see mvgam_trends AR() details). mvgam() can also fit models include time variable temporal dynamic structures included (.e. trend_model = 'None' trend_model = ZMVN()). data also include variables included linear predictor formula newdata Optional dataframe list test data containing variables data. included, observations variable y set NA fitting model posterior simulations can obtained trend_map Optional data.frame specifying series depend latent trends. Useful allowing multiple series depend latent trend process, different observation processes. supplied, latent factor model set setting use_lv = TRUE using mapping set shared trends. Needs column names series trend, integer values trend column state trend series depend . series column single unique entry series data (names perfectly match factor levels series variable data). Note supplied, intercept parameter process model automatically suppressed. yet supported models latent factors evolve continuous time (CAR()). See examples details use_lv logical. TRUE, use dynamic factors estimate series' latent trends reduced dimension format. available RW(), AR() GP() trend models. Defaults FALSE. See lv_correlations worked example n_lv integer number latent dynamic factors use use_lv == TRUE. > n_series. Defaults arbitrarily min(2, floor(n_series / 2)) priors optional data.frame prior definitions , preferentially, vector containing objects class brmsprior (see. prior() details). See get_mvgam_priors() Details' information changing default prior distributions chains integer specifying number parallel chains model. Ignored algorithm %% c('meanfield', 'fullrank', 'pathfinder', 'laplace') burnin integer specifying number warmup iterations Markov chain run tune sampling algorithms. Ignored algorithm %% c('meanfield', 'fullrank', 'pathfinder', 'laplace') samples integer specifying number post-warmup iterations Markov chain run sampling posterior distribution threads integer Experimental option use multithreading within-chain parallelisation Stan. recommend use experienced Stan's reduce_sum function slow running model sped means. Currently works families apart nmix() using Cmdstan backend algorithm Character string naming estimation approach use. Options \"sampling\" MCMC (default), \"meanfield\" variational inference factorized normal distributions, \"fullrank\" variational inference multivariate normal distribution, \"laplace\" Laplace approximation (available using cmdstanr backend) \"pathfinder\" pathfinder algorithm (currently available using cmdstanr backend). Can set globally current R session via \"brms.algorithm\" option (see options). Limited testing suggests \"meanfield\" performs best non-MCMC approximations dynamic GAMs, possibly difficulties estimating covariances among many spline parameters latent trend parameters. rigorous testing carried lfo Logical indicating whether part call lfo_cv.mvgam. Returns lighter version model residuals fewer monitored parameters speed post-processing. downstream functions work properly, users always leave set FALSE ... arguments passed mvgam jsdgam factor_formula Optional new formula object factor linear predictors factor_knots optional list containing user specified knot values used basis construction smooth terms factor_formula. bases user simply supplies knots used, must match k value supplied (note number knots always just k). Different terms can use different numbers knots, unless share covariate","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/update.mvgam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update an existing mvgam model object — update.mvgam","text":"list object class mvgam containing model output, text representation model file, mgcv model output (easily generating simulations unsampled covariate values), Dunn-Smyth residuals outcome variable key information needed functions package. See mvgam-class details. Use methods(class = \"mvgam\") overview available methods. list object class mvgam containing model output, text representation model file, mgcv model output (easily generating simulations unsampled covariate values), Dunn-Smyth residuals series key information needed functions package. See mvgam-class details. Use methods(class = \"mvgam\") overview available methods.","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/update.mvgam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Update an existing mvgam model object — update.mvgam","text":"Nicholas J Clark","code":""},{"path":"https://nicholasjclark.github.io/mvgam/reference/update.mvgam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update an existing mvgam model object — update.mvgam","text":"","code":"# \\donttest{ # Simulate some data and fit a Poisson AR1 model simdat <- sim_mvgam(n_series = 1, trend_model = AR()) mod <- mvgam(y ~ s(season, bs = 'cc'),              trend_model = AR(),              noncentred = TRUE,              data = simdat$data_train,              chains = 2) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 finished in 0.4 seconds. #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 finished in 0.4 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 0.4 seconds. #> Total execution time: 0.5 seconds. #>  summary(mod) #> GAM formula: #> y ~ s(season, bs = \"cc\") #> <environment: 0x5578eaa37468> #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> AR() #>  #>  #> N series: #> 1  #>  #> N timepoints: #> 75  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM coefficient (beta) estimates: #>               2.5%    50%  97.5% Rhat n_eff #> (Intercept) -0.710 -0.350 -0.051    1   512 #> s(season).1 -1.500 -0.810 -0.270    1   507 #> s(season).2 -1.900 -0.930 -0.280    1   385 #> s(season).3 -0.940 -0.220  0.410    1   769 #> s(season).4  0.350  0.900  1.400    1   622 #> s(season).5  0.780  1.300  1.800    1   585 #> s(season).6  0.480  1.000  1.500    1   803 #> s(season).7  0.042  0.600  1.200    1   680 #> s(season).8 -0.460  0.086  0.650    1   633 #>  #> Approximate significance of GAM smooths: #>           edf Ref.df Chi.sq p-value     #> s(season) 3.7      8   45.7 0.00039 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Latent trend parameter AR estimates: #>            2.5%   50% 97.5% Rhat n_eff #> ar1[1]   -0.900 0.046  0.90    1   858 #> sigma[1]  0.073 0.190  0.43    1  1019 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model conditional_effects(mod, type = 'link')   # Update to an AR2 model updated_mod <- update(mod, trend_model = AR(p = 2),                       noncentred = TRUE) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 0.4 seconds. #> Chain 2 finished in 0.4 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 0.4 seconds. #> Total execution time: 0.5 seconds. #>  summary(updated_mod) #> GAM formula: #> y ~ s(season, bs = \"cc\") #> <environment: 0x5578eaa37468> #>  #> Family: #> poisson #>  #> Link function: #> log #>  #> Trend model: #> AR(p = 2) #>  #>  #> N series: #> 1  #>  #> N timepoints: #> 75  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM coefficient (beta) estimates: #>               2.5%    50%  97.5% Rhat n_eff #> (Intercept) -0.690 -0.340 -0.084 1.00   785 #> s(season).1 -1.400 -0.800 -0.240 1.00   981 #> s(season).2 -1.900 -0.960 -0.320 1.00   748 #> s(season).3 -0.970 -0.230  0.350 1.00  1206 #> s(season).4  0.390  0.890  1.400 1.00  1140 #> s(season).5  0.800  1.300  1.800 1.00  1096 #> s(season).6  0.530  1.000  1.500 1.00   918 #> s(season).7  0.026  0.580  1.100 1.01  1252 #> s(season).8 -0.460  0.086  0.620 1.00  1369 #>  #> Approximate significance of GAM smooths: #>            edf Ref.df Chi.sq p-value     #> s(season) 3.42      8   46.6   3e-04 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Latent trend AR parameter estimates: #>            2.5%    50% 97.5% Rhat n_eff #> ar1[1]   -0.900  0.065  0.94    1  1051 #> ar2[1]   -0.850 -0.015  0.88    1  1128 #> sigma[1]  0.076  0.180  0.39    1   898 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model conditional_effects(updated_mod, type = 'link')   # Now update to a Binomial AR1 by adding information on trials # requires that we supply newdata that contains the 'trials' variable simdat$data_train$trials <- max(simdat$data_train$y) + 15 updated_mod <- update(mod,                       formula = cbind(y, trials) ~ s(season, bs = 'cc'),                       noncentred = TRUE,                       data = simdat$data_train,                       family = binomial()) #> Compiling Stan program using cmdstanr #>  #> Start sampling #> Running MCMC with 2 parallel chains... #>  #> Chain 1 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 1 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration:   1 / 1000 [  0%]  (Warmup)  #> Chain 2 Iteration: 100 / 1000 [ 10%]  (Warmup)  #> Chain 1 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 1 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 200 / 1000 [ 20%]  (Warmup)  #> Chain 2 Iteration: 300 / 1000 [ 30%]  (Warmup)  #> Chain 2 Iteration: 400 / 1000 [ 40%]  (Warmup)  #> Chain 2 Iteration: 500 / 1000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 501 / 1000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 600 / 1000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 700 / 1000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 800 / 1000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 2 Iteration: 900 / 1000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1000 / 1000 [100%]  (Sampling)  #> Chain 1 finished in 0.4 seconds. #> Chain 2 finished in 0.4 seconds. #>  #> Both chains finished successfully. #> Mean chain execution time: 0.4 seconds. #> Total execution time: 0.5 seconds. #>  summary(updated_mod) #> GAM formula: #> cbind(y, trials) ~ s(season, bs = \"cc\") #> <environment: 0x5578eaa37468> #>  #> Family: #> binomial #>  #> Link function: #> logit #>  #> Trend model: #> AR() #>  #>  #> N series: #> 1  #>  #> N timepoints: #> 75  #>  #> Status: #> Fitted using Stan  #> 2 chains, each with iter = 1000; warmup = 500; thin = 1  #> Total post-warmup draws = 1000 #>  #>  #> GAM coefficient (beta) estimates: #>               2.5%    50% 97.5% Rhat n_eff #> (Intercept) -3.600 -3.300 -3.00    1   729 #> s(season).1 -1.500 -0.830 -0.27    1   693 #> s(season).2 -1.900 -0.980 -0.33    1   595 #> s(season).3 -0.960 -0.240  0.38    1   873 #> s(season).4  0.370  0.910  1.50    1   870 #> s(season).5  0.900  1.400  1.90    1   681 #> s(season).6  0.510  1.000  1.60    1   658 #> s(season).7  0.071  0.610  1.20    1   759 #> s(season).8 -0.440  0.079  0.63    1   817 #>  #> Approximate significance of GAM smooths: #>            edf Ref.df Chi.sq p-value     #> s(season) 3.23      8   50.3 0.00047 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Latent trend parameter AR estimates: #>            2.5%   50% 97.5% Rhat n_eff #> ar1[1]   -0.880 0.059  0.92    1   913 #> sigma[1]  0.076 0.200  0.48    1   662 #>  #> Stan MCMC diagnostics: #> ✔ No issues with effective samples per iteration #> ✔ Rhat looks good for all parameters #> ✔ No issues with divergences #> ✔ No issues with maximum tree depth #>  #> Samples were drawn using sampling(hmc). For each parameter, n_eff is a #>   crude measure of effective sample size, and Rhat is the potential scale #>   reduction factor on split MCMC chains (at convergence, Rhat = 1) #>  #> Use how_to_cite() to get started describing this model conditional_effects(updated_mod, type = 'link')  # }"},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"mvgam-1151","dir":"Changelog","previous_headings":"","what":"mvgam 1.1.51","title":"mvgam 1.1.51","text":"CRAN release: 2025-03-14","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"new-functionalities-1-1-51","dir":"Changelog","previous_headings":"","what":"New functionalities","title":"mvgam 1.1.51","text":"Changed default priors scale parameters (.e. process errors “sigma” observation errors “sigma_obs”) inverse gammas provide sensible prior regularisation away zero Improved messaging summary() better guidance investigate poor HMC sampler behaviours Converted several plotting functions return ggplot objects place base R plots broader customisation Added four new types pp_check() function allow targeted investigations randomized quantile residual distributions Added plot.mvgam_residcor() function nicer plotting estimated residual correlations jsdgam objects Added summary() functions calculate useful posterior summaries objects class mvgam_irf mvgam_fevd (see ?irf ?fevd examples) Improved efficiency nmix() models slight restructuring model objects (#102)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"bug-fixes-1-1-51","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"mvgam 1.1.51","text":"Bug fix ensure piecewise trends extrapolated correct number timepoints forecasting using forecast() function","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"mvgam-114","dir":"Changelog","previous_headings":"","what":"mvgam 1.1.4","title":"mvgam 1.1.4","text":"CRAN release: 2025-02-19","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"new-functionalities-1-1-4","dir":"Changelog","previous_headings":"","what":"New functionalities","title":"mvgam 1.1.4","text":"Added how_to_cite.mvgam() function generate scaffold methods description fitted models, can hopefully make easier users fully describe programming environment Improved various plotting functions returning ggplot objects place base plots (thanks @mhollanders #38) Added brier score (score = 'brier') option score.mvgam_forecast() scoring forecasts binary variables using family = bernoulli() (#80) Added augment() function add residuals fitted values mvgam object’s observed data (thanks @swpease #83) Added support approximate gp() effects one covariate different kernel functions (#79) Added function jsdgam() estimate Joint Species Distribution Models latent factors observation model components can include mvgam’s complex linear predictor effects. Also added function residual_cor() compute residual correlation, covariance precision matrices jsdgam models. See ?mvgam::jsdgam ?mvgam::residual_cor details Added stability.mvgam() method compute stability metrics models fit Vector Autoregressive dynamics (#21 #76) Added functionality estimate hierarchical error correlations using multivariate latent process models data nested among levels relevant grouping factor (#75); see ?mvgam::AR example Added ZMVN() error models estimating Zero-Mean Multivariate Normal errors; convenient working non time-series data latent residuals expected correlated (fitting Joint Species Distribution Models); see ?mvgam::ZMVN examples Added fevd.mvgam() method compute forecast error variance decompositions models fit Vector Autoregressive dynamics (#21 #76)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"deprecations-1-1-4","dir":"Changelog","previous_headings":"","what":"Deprecations","title":"mvgam 1.1.4","text":"Arguments use_stan, jags_path, data_train, data_test, adapt_delta, max_treedepth drift removed primary functions streamline documentation reflect package’s mission deprecate ‘JAGS’ suitable backend. adapt_delta max_treedepth now supplied named list() new argument control","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"bug-fixes-1-1-4","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"mvgam 1.1.4","text":"Bug fix ensure marginaleffects::comparisons functions appropriately recognise internal rowid variables Updates ensure ensemble provides appropriate weighting forecast draws (#98) necessarily “bug fix”, update removes several dependencies lighten installation improve efficiency workflow (#93) Fixed minor bug way trend_map recognises levels series factor Bug fix ensure lfo_cv recognises actual times time, just case user supplies data doesn’t start t = 1. Also updated documentation better reflect Bug fix ensure update.mvgam captures knots trend_knots arguments passed original model call","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"mvgam-113","dir":"Changelog","previous_headings":"","what":"mvgam 1.1.3","title":"mvgam 1.1.3","text":"CRAN release: 2024-09-04","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"new-functionalities-1-1-3","dir":"Changelog","previous_headings":"","what":"New functionalities","title":"mvgam 1.1.3","text":"Allow intercepts included process models trend_formula supplied. breaks assumption process zero-centred, adding modelling flexibility also potentially inducing nonidentifiabilities respect observation model intercepts. Thoughtful priors must models Added standata.mvgam_prefit, stancode.mvgam stancode.mvgam_prefit methods better alignment ‘brms’ workflows Added ‘gratia’ Enhancements allow popular methods draw() used ‘mvgam’ models ‘gratia’ already installed Added ensemble.mvgam_forecast() method generate evenly weighted combinations probabilistic forecast distributions Added irf.mvgam() method compute Generalized Orthogonalized Impulse Response Functions (IRFs) models fit Vector Autoregressive dynamics","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"deprecations-1-1-3","dir":"Changelog","previous_headings":"","what":"Deprecations","title":"mvgam 1.1.3","text":"drift argument deprecated. now recommended users include parametric fixed effects “time” respective GAM formulae capture expected drift effects","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"bug-fixes-1-1-3","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"mvgam 1.1.3","text":"Added new check ensure exception messages suppressed silent argument user’s version ‘cmdstanr’ adequate Updated dependency ‘brms’ version >= ‘2.21.0’ read_csv_as_stanfit can imported, future-proof conversion ‘cmdstanr’ models stanfit objects (#70)","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"mvgam-112","dir":"Changelog","previous_headings":"","what":"mvgam 1.1.2","title":"mvgam 1.1.2","text":"CRAN release: 2024-07-01","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"new-functionalities-1-1-2","dir":"Changelog","previous_headings":"","what":"New functionalities","title":"mvgam 1.1.2","text":"Added options silencing ‘Stan’ compiler modeling messages using silent argument mvgam() Moved number packages ‘Depends’ ‘Imports’ simpler package loading fewer potential masking conflicts Improved efficiency model initialisation tweaking parameters underlying ‘mgcv’ gam object’s convergence criteria, resulting much faster model setups Added option use trend_model = 'None' State-Space models, increasing flexibility ensuring process error evolves white noise (#51) Added option use non-centred parameterisation autoregressive trend models, speeds mixing time Updated support multithreading observation families (apart nmix()) can now modeled multiple threads Changed default priors autoregressive coefficients (AR1, AR2, AR3) enforce stationarity, much sensible prior majority contexts","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"bug-fixes-1-1-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"mvgam 1.1.2","text":"Fixed small bug prevented conditional_effects.mvgam() handling effects three-way interactions","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"mvgam-111","dir":"Changelog","previous_headings":"","what":"mvgam 1.1.1","title":"mvgam 1.1.1","text":"CRAN release: 2024-05-10","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"new-functionalities-1-1-1","dir":"Changelog","previous_headings":"","what":"New functionalities","title":"mvgam 1.1.1","text":"Changed indexing internal c++ function Prof Brian Ripley’s email: Dear maintainer, Please see problems shown https://cran.r-project.org/web/checks/check_results_mvgam.html. Please correct 2024-05-22 safely retain package CRAN. CRAN Team","code":""},{"path":"https://nicholasjclark.github.io/mvgam/news/index.html","id":"mvgam-110","dir":"Changelog","previous_headings":"","what":"mvgam 1.1.0","title":"mvgam 1.1.0","text":"CRAN release: 2024-05-06 First release mvgam CRAN","code":""}]
