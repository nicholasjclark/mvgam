% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate_mvgams.R
\name{evaluate_mvgams}
\alias{evaluate_mvgams}
\alias{eval_mvgam}
\alias{roll_eval_mvgam}
\alias{compare_mvgams}
\title{Evaluate forecasts from fitted mvgam objects}
\usage{
eval_mvgam(
  object,
  n_samples = 5000,
  eval_timepoint = 3,
  fc_horizon = 3,
  n_cores = 2,
  score = "drps",
  log = TRUE,
  weights
)

roll_eval_mvgam(
  object,
  n_evaluations = 5,
  evaluation_seq,
  n_samples = 5000,
  fc_horizon = 3,
  n_cores = 2,
  score = "drps",
  log = TRUE,
  weights
)

compare_mvgams(
  model1,
  model2,
  n_samples = 1000,
  fc_horizon = 3,
  n_evaluations = 10,
  n_cores = 2,
  score = "drps",
  log = TRUE,
  weights
)
}
\arguments{
\item{object}{\code{list} object returned from \code{mvgam}}

\item{n_samples}{\code{integer} specifying the number of samples to generate from the model's
posterior distribution}

\item{eval_timepoint}{\code{integer} indexing the timepoint that represents our last 'observed'
set of outcome data}

\item{fc_horizon}{\code{integer} specifying the length of the forecast horizon for evaluating forecasts}

\item{n_cores}{\code{integer} specifying number of cores for generating particle forecasts in parallel}

\item{score}{\code{character} specifying the type of ranked probability score to use for evaluation. Options are:
\code{variogram}, \code{drps} or \code{crps}}

\item{log}{\code{logical}. Should the forecasts and truths be logged prior to scoring?
This is often appropriate for comparing
performance of models when series vary in their observation ranges}

\item{weights}{optional \code{vector} of weights (where \code{length(weights) == n_series})
for weighting pairwise correlations when evaluating the variogram score for multivariate
forecasts. Useful for down-weighting series that have larger magnitude observations or that
are of less interest when forecasting}

\item{n_evaluations}{\code{integer} specifying the total number of evaluations to perform}

\item{evaluation_seq}{Optional \code{integer sequence} specifying the exact set of timepoints for
evaluating the model's forecasts. This sequence cannot have values
\code{<3} or \code{> max(training timepoints) - fc_horizon}}

\item{model1}{\code{list} object returned from \code{mvgam} representing the first model to be
evaluated}

\item{model2}{\code{list} object returned from \code{mvgam} representing the second model to be
evaluated}
}
\value{
For \code{eval_mvgam}, a \code{list} object containing information on specific evaluations for each series
(if using \code{drps} or \code{crps} as the score) or a vector of scores when using \code{variogram}.

For \code{roll_eval_mvgam}, a \code{list} object containing information on specific evaluations for each series as well as
a total evaluation summary (taken by summing the forecast score for each series at each evaluation and averaging
the coverages at each evaluation)

For \code{compare_mvgams}, a series of plots comparing forecast Rank Probability Scores for each competing
model. A lower score is preferred. Note however that it is possible to select a model that ultimately
would perform poorly in true out-of-sample forecasting. For example if a wiggly smooth function of 'year'
is included in the model then this function will be learned prior to evaluating rolling window forecasts,
and the model could generate very tight predictions as a result. But when forecasting ahead to timepoints
that the model has not seen (i.e. next year), the smooth function will end up extrapolating, sometimes
in very strange and unexpected ways. It is therefore recommended to only use smooth functions for
covariates that are adequately measured in the data (i.e. 'seasonality', for example) to reduce possible
extrapolation of smooths and let the latent trends in the \code{mvgam} model capture any
temporal dependencies in the data. These trends are time series models and so will provide much more
stable forecasts
}
\description{
Evaluate forecasts from fitted mvgam objects
}
\details{
\code{eval_mvgam} generates a set of samples representing fixed parameters estimated from the full
\code{mvgam} model and latent trend states at a given point in time. The trends are rolled forward
a total of \code{fc_horizon} timesteps according to their estimated state space dynamics to
generate an 'out-of-sample' forecast that is evaluated against the true observations in the horizon window.
This function therefore simulates a situation where the model's parameters had already been estimated but
we have only observed data up to the evaluation timepoint and would like to generate forecasts from the
latent trends that have been observed up to that timepoint. Evaluation involves calculating an
appropriate Rank Probability Score and a binary indicator
for whether or not the true value lies within the forecast's 90\% prediction interval

\code{roll_eval_mvgam} sets up a sequence of evaluation timepoints along a rolling window and iteratively
calls \code{eval_mvgam} to evaluate 'out-of-sample' forecasts.
Evaluation involves calculating the Discrete Rank Probability Score and a binary indicator
for whether or not the true value lies within the forecast's 90\% prediction interval

\code{compare_mvgams} automates the evaluation to compare two fitted models using rolling window forecast evaluation and
provides a series of summary plots to facilitate model selection. It is essentially a wrapper for
\code{roll_eval_mvgam}
}
